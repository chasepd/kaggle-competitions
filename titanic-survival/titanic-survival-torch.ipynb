{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c45ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b15391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b933b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "393591f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47ec7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e938fbb",
   "metadata": {},
   "source": [
    "## Count how many people survived and how many died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dcf59e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived: 342 (38.4%)\n",
      "Died: 549 (61.6%)\n"
     ]
    }
   ],
   "source": [
    "survived = train_df[train_df['Survived'] == 1]\n",
    "died = train_df[train_df['Survived'] == 0]\n",
    "print('Survived: %i (%.1f%%)' % (len(survived), float(len(survived))/len(train_df)*100.0))\n",
    "print('Died: %i (%.1f%%)' % (len(died), float(len(died))/len(train_df)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca86628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extact the title from the name\n",
    "train_df['Title'] = train_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "test_df['Title'] = test_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Extract Last Name\n",
    "train_df['LastName'] = train_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "test_df['LastName'] = test_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "\n",
    "# Temporarily combine both datasets and extract the family size\n",
    "full_df = pd.concat([train_df, test_df])\n",
    "full_df['FamilySize'] = full_df['SibSp'] + full_df['Parch'] + 1\n",
    "\n",
    "# Get the family size from the last name\n",
    "full_df['FamilySizeFromName'] = full_df.groupby('LastName')['LastName'].transform('count')\n",
    "\n",
    "# Get the family size from the ticket\n",
    "full_df['FamilySizeFromTicket'] = full_df.groupby('Ticket')['Ticket'].transform('count')\n",
    "\n",
    "# Get the family size from the cabin\n",
    "full_df['FamilySizeFromCabin'] = full_df.groupby('Cabin')['Cabin'].transform('count')\n",
    "\n",
    "# Split back into train and test\n",
    "train_df = full_df[:len(train_df)]\n",
    "test_df = full_df[len(train_df):]\n",
    "\n",
    "train_df = train_df.drop(['Name'], axis=1)\n",
    "train_df = train_df.drop([\"LastName\"], axis=1)\n",
    "\n",
    "test_df = test_df.drop(['Name'], axis=1)\n",
    "test_df = test_df.drop([\"LastName\"], axis=1)\n",
    "\n",
    "\n",
    "# Combine Miss and Mlle titles\n",
    "train_df['Title'] = train_df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "test_df['Title'] = test_df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "\n",
    "# Combine Mme and Mrs titles\n",
    "train_df['Title'] = train_df['Title'].replace(['Mme'], 'Mrs')\n",
    "test_df['Title'] = test_df['Title'].replace(['Mme'], 'Mrs')\n",
    "\n",
    "# Combine Dr, Rev, Col, Major, Capt titles\n",
    "train_df['Title'] = train_df['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Capt'], 'Officer')\n",
    "test_df['Title'] = test_df['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Capt'], 'Officer')\n",
    "\n",
    "# Combine Don, Sir, the Countess, Lady, Dona titles\n",
    "train_df['Title'] = train_df['Title'].replace(['Don', 'Sir', 'the Countess', 'Lady', 'Dona'], 'Royalty')\n",
    "test_df['Title'] = test_df['Title'].replace(['Don', 'Sir', 'the Countess', 'Lady', 'Dona'], 'Royalty')\n",
    "\n",
    "# Combine Jonkheer and Master titles\n",
    "train_df['Title'] = train_df['Title'].replace(['Jonkheer'], 'Master')\n",
    "test_df['Title'] = test_df['Title'].replace(['Jonkheer'], 'Master')\n",
    "\n",
    "# Fill NA titles with 'NoTitle'\n",
    "train_df['Title'] = train_df['Title'].fillna('NoTitle')\n",
    "\n",
    "title_encoder = LabelEncoder()\n",
    "train_df['Title'] = title_encoder.fit_transform(train_df['Title'])\n",
    "test_df['Title'] = title_encoder.transform(test_df['Title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e71c7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"HasCabin\"] = (train_df[\"Cabin\"].notnull().astype('int'))\n",
    "train_df[\"CabinNumber\"] = train_df[\"Cabin\"].str.extract('([0-9]+)', expand=False)\n",
    "train_df[\"Deck\"] = train_df[\"Cabin\"].str.extract('([A-Z]+)', expand=False)\n",
    "train_df = train_df.drop(['Cabin'], axis=1)\n",
    "\n",
    "test_df[\"HasCabin\"] = (test_df[\"Cabin\"].notnull().astype('int'))\n",
    "test_df[\"CabinNumber\"] = test_df[\"Cabin\"].str.extract('([0-9]+)', expand=False)\n",
    "test_df[\"Deck\"] = test_df[\"Cabin\"].str.extract('([A-Z]+)', expand=False)\n",
    "test_df = test_df.drop(['Cabin'], axis=1)\n",
    "\n",
    "train_df['Deck'] = train_df['Deck'].fillna('Z')\n",
    "test_df['Deck'] = test_df['Deck'].fillna('Z')\n",
    "\n",
    "deck_encoder = LabelEncoder()\n",
    "train_df['Deck'] = deck_encoder.fit_transform(train_df['Deck'])\n",
    "test_df['Deck'] = deck_encoder.transform(test_df['Deck'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f019bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"TicketPrefix\"] = train_df[\"Ticket\"].str.extract('([A-Za-z\\.\\/]+)', expand=False)\n",
    "train_df[\"TicketNumber\"] = train_df[\"Ticket\"].str.extract('([0-9]+)', expand=False)\n",
    "train_df = train_df.drop(['Ticket'], axis=1)\n",
    "\n",
    "test_df[\"TicketPrefix\"] = test_df[\"Ticket\"].str.extract('([A-Za-z\\.\\/]+)', expand=False)\n",
    "test_df[\"TicketNumber\"] = test_df[\"Ticket\"].str.extract('([0-9]+)', expand=False)\n",
    "test_df = test_df.drop(['Ticket'], axis=1)\n",
    "\n",
    "train_df['TicketPrefix'] = train_df['TicketPrefix'].fillna('Z')\n",
    "\n",
    "\n",
    "\n",
    "# Encode the TicketPrefix, ignoring unseen prefixes\n",
    "prefix_encoder = LabelEncoder()\n",
    "train_df['TicketPrefix'] = prefix_encoder.fit_transform(train_df['TicketPrefix'])\n",
    "\n",
    "# For the test data, fill both NA values with null, as well as any values we haven't seen before\n",
    "test_df['TicketPrefix'] = test_df['TicketPrefix'].fillna('Z')\n",
    "test_df.loc[~test_df['TicketPrefix'].isin(prefix_encoder.classes_), 'TicketPrefix'] = 'Z'\n",
    "\n",
    "test_df['TicketPrefix'] = prefix_encoder.transform(test_df['TicketPrefix'])\n",
    "\n",
    "train_df['TicketNumber'] = train_df['TicketNumber'].fillna(0)\n",
    "test_df['TicketNumber'] = test_df['TicketNumber'].fillna(0)\n",
    "\n",
    "# Convert the ticket number to an integer\n",
    "train_df['TicketNumber'] = train_df['TicketNumber'].astype('int')\n",
    "test_df['TicketNumber'] = test_df['TicketNumber'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9dd703e-77db-4da1-bfe5-16e796bc36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())\n",
    "test_df['Age'] = test_df['Age'].fillna(test_df['Age'].median())\n",
    "\n",
    "train_df['Embarked'] = train_df['Embarked'].fillna('S')\n",
    "test_df['Embarked'] = test_df['Embarked'].fillna('S')\n",
    "\n",
    "train_df['Fare'] = train_df['Fare'].fillna(train_df['Fare'].median())\n",
    "test_df['Fare'] = test_df['Fare'].fillna(test_df['Fare'].median())\n",
    "\n",
    "train_df['FamilySizeFromCabin'] = train_df['FamilySizeFromCabin'].fillna(train_df['FamilySizeFromCabin'].median())\n",
    "test_df['FamilySizeFromCabin'] = test_df['FamilySizeFromCabin'].fillna(test_df['FamilySizeFromCabin'].median())\n",
    "\n",
    "train_df['CabinNumber'] = train_df['CabinNumber'].fillna(train_df['CabinNumber'].median())\n",
    "test_df['CabinNumber'] = test_df['CabinNumber'].fillna(test_df['CabinNumber'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce4c24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_encoder = LabelEncoder()\n",
    "train_df['Embarked'] = embarked_encoder.fit_transform(train_df['Embarked'])\n",
    "test_df['Embarked'] = embarked_encoder.transform(test_df['Embarked'])\n",
    "\n",
    "sex_encoder = LabelEncoder()\n",
    "train_df['Sex'] = sex_encoder.fit_transform(train_df['Sex'])\n",
    "test_df['Sex'] = sex_encoder.transform(test_df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8018872f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>FamilySizeFromName</th>\n",
       "      <th>FamilySizeFromTicket</th>\n",
       "      <th>FamilySizeFromCabin</th>\n",
       "      <th>HasCabin</th>\n",
       "      <th>CabinNumber</th>\n",
       "      <th>Deck</th>\n",
       "      <th>TicketPrefix</th>\n",
       "      <th>TicketNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>17599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>113803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>373450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  \\\n",
       "0            1       0.0       3    1  22.0      1      0   7.2500         2   \n",
       "1            2       1.0       1    0  38.0      1      0  71.2833         0   \n",
       "2            3       1.0       3    0  26.0      0      0   7.9250         2   \n",
       "3            4       1.0       1    0  35.0      1      0  53.1000         2   \n",
       "4            5       0.0       3    1  35.0      0      0   8.0500         2   \n",
       "\n",
       "   Title  FamilySize  FamilySizeFromName  FamilySizeFromTicket  \\\n",
       "0      3           2                   2                     1   \n",
       "1      4           2                   2                     2   \n",
       "2      2           1                   1                     1   \n",
       "3      4           2                   2                     2   \n",
       "4      3           1                   2                     1   \n",
       "\n",
       "   FamilySizeFromCabin  HasCabin CabinNumber  Deck  TicketPrefix  TicketNumber  \n",
       "0                  2.0         0        43.0     8             3             5  \n",
       "1                  2.0         1          85     2            15         17599  \n",
       "2                  2.0         0        43.0     8            33             2  \n",
       "3                  2.0         1         123     2            39        113803  \n",
       "4                  2.0         0        43.0     8            39        373450  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b91c45f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId             0\n",
       "Survived                0\n",
       "Pclass                  0\n",
       "Sex                     0\n",
       "Age                     0\n",
       "SibSp                   0\n",
       "Parch                   0\n",
       "Fare                    0\n",
       "Embarked                0\n",
       "Title                   0\n",
       "FamilySize              0\n",
       "FamilySizeFromName      0\n",
       "FamilySizeFromTicket    0\n",
       "FamilySizeFromCabin     0\n",
       "HasCabin                0\n",
       "CabinNumber             0\n",
       "Deck                    0\n",
       "TicketPrefix            0\n",
       "TicketNumber            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc360506",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12763c84-3b4a-4ed9-8f22-9e7acb2c0d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correlation_matrix = train_df.corr()\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(correlation_matrix, vmax=1, square=True, annot=True, cmap='cubehelix')\n",
    "plt.title('Correlation matrix between features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef4fbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>HasCabin</th>\n",
       "      <th>Deck</th>\n",
       "      <th>TicketNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>373450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex     Fare  Embarked  Title  HasCabin  Deck  \\\n",
       "0       0.0       3    1   7.2500         2      3         0     8   \n",
       "1       1.0       1    0  71.2833         0      4         1     2   \n",
       "2       1.0       3    0   7.9250         2      2         0     8   \n",
       "3       1.0       1    0  53.1000         2      4         1     2   \n",
       "4       0.0       3    1   8.0500         2      3         0     8   \n",
       "\n",
       "   TicketNumber  \n",
       "0             5  \n",
       "1         17599  \n",
       "2             2  \n",
       "3        113803  \n",
       "4        373450  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(['SibSp'], axis=1)\n",
    "train_df = train_df.drop(['Parch'], axis=1)\n",
    "#train_df = train_df.drop(['Title'], axis=1)\n",
    "train_df = train_df.drop(['Age'], axis=1)\n",
    "train_df = train_df.drop(['FamilySize'], axis=1)\n",
    "train_df = train_df.drop(['FamilySizeFromTicket'], axis=1)\n",
    "train_df = train_df.drop(['FamilySizeFromName'], axis=1)\n",
    "train_df = train_df.drop(['FamilySizeFromCabin'], axis=1)\n",
    "train_df = train_df.drop(['TicketPrefix'], axis=1)\n",
    "train_df = train_df.drop(['CabinNumber'], axis=1)\n",
    "\n",
    "test_df = test_df.drop(['SibSp'], axis=1)\n",
    "test_df = test_df.drop(['Parch'], axis=1)\n",
    "#test_df = test_df.drop(['Title'], axis=1)\n",
    "test_df = test_df.drop(['Age'], axis=1)\n",
    "test_df = test_df.drop(['FamilySize'], axis=1)\n",
    "test_df = test_df.drop(['FamilySizeFromTicket'], axis=1)\n",
    "test_df = test_df.drop(['FamilySizeFromName'], axis=1)\n",
    "test_df = test_df.drop(['FamilySizeFromCabin'], axis=1)\n",
    "test_df = test_df.drop(['TicketPrefix'], axis=1)\n",
    "test_df = test_df.drop(['CabinNumber'], axis=1)\n",
    "\n",
    "test_df_ids = test_df['PassengerId']\n",
    "test_df = test_df.drop(['PassengerId'], axis=1)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c8c2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correlation_matrix = train_df.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(correlation_matrix, vmax=1, square=True, annot=True, cmap='cubehelix')\n",
    "plt.title('Correlation matrix between features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad20e5af-497c-4117-9307-1407a8392672",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(['Survived'], axis=1)\n",
    "test_df.head()\n",
    "\n",
    "train_y = train_df['Survived']\n",
    "train_X = train_df.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2133d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df = scaler.fit_transform(train_X)\n",
    "test_df = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0707481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TitanicClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout(torch.relu(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000..  Training Loss: 0.704.. \n",
      "Epoch: 2/10000..  Training Loss: 0.501.. \n",
      "Epoch: 3/10000..  Training Loss: 0.467.. \n",
      "Epoch: 4/10000..  Training Loss: 0.468.. \n",
      "Epoch: 5/10000..  Training Loss: 0.453.. \n",
      "Epoch: 6/10000..  Training Loss: 0.429.. \n",
      "Epoch: 7/10000..  Training Loss: 0.417.. \n",
      "Epoch: 8/10000..  Training Loss: 0.415.. \n",
      "Epoch: 9/10000..  Training Loss: 0.415.. \n",
      "Epoch: 10/10000..  Training Loss: 0.412.. \n",
      "Epoch: 11/10000..  Training Loss: 0.407.. \n",
      "Epoch: 12/10000..  Training Loss: 0.401.. \n",
      "Epoch: 13/10000..  Training Loss: 0.396.. \n",
      "Epoch: 14/10000..  Training Loss: 0.394.. \n",
      "Epoch: 15/10000..  Training Loss: 0.392.. \n",
      "Epoch: 16/10000..  Training Loss: 0.391.. \n",
      "Epoch: 17/10000..  Training Loss: 0.388.. \n",
      "Epoch: 18/10000..  Training Loss: 0.384.. \n",
      "Epoch: 19/10000..  Training Loss: 0.381.. \n",
      "Epoch: 20/10000..  Training Loss: 0.379.. \n",
      "Epoch: 21/10000..  Training Loss: 0.378.. \n",
      "Epoch: 22/10000..  Training Loss: 0.377.. \n",
      "Epoch: 23/10000..  Training Loss: 0.375.. \n",
      "Epoch: 24/10000..  Training Loss: 0.373.. \n",
      "Epoch: 25/10000..  Training Loss: 0.371.. \n",
      "Epoch: 26/10000..  Training Loss: 0.369.. \n",
      "Epoch: 27/10000..  Training Loss: 0.368.. \n",
      "Epoch: 28/10000..  Training Loss: 0.367.. \n",
      "Epoch: 29/10000..  Training Loss: 0.365.. \n",
      "Epoch: 30/10000..  Training Loss: 0.364.. \n",
      "Epoch: 31/10000..  Training Loss: 0.362.. \n",
      "Epoch: 32/10000..  Training Loss: 0.360.. \n",
      "Epoch: 33/10000..  Training Loss: 0.359.. \n",
      "Epoch: 34/10000..  Training Loss: 0.357.. \n",
      "Epoch: 35/10000..  Training Loss: 0.356.. \n",
      "Epoch: 36/10000..  Training Loss: 0.354.. \n",
      "Epoch: 37/10000..  Training Loss: 0.353.. \n",
      "Epoch: 38/10000..  Training Loss: 0.351.. \n",
      "Epoch: 39/10000..  Training Loss: 0.350.. \n",
      "Epoch: 40/10000..  Training Loss: 0.348.. \n",
      "Epoch: 41/10000..  Training Loss: 0.347.. \n",
      "Epoch: 42/10000..  Training Loss: 0.345.. \n",
      "Epoch: 43/10000..  Training Loss: 0.344.. \n",
      "Epoch: 44/10000..  Training Loss: 0.343.. \n",
      "Epoch: 45/10000..  Training Loss: 0.341.. \n",
      "Epoch: 46/10000..  Training Loss: 0.340.. \n",
      "Epoch: 47/10000..  Training Loss: 0.338.. \n",
      "Epoch: 48/10000..  Training Loss: 0.337.. \n",
      "Epoch: 49/10000..  Training Loss: 0.336.. \n",
      "Epoch: 50/10000..  Training Loss: 0.334.. \n",
      "Epoch: 51/10000..  Training Loss: 0.334.. \n",
      "Epoch: 52/10000..  Training Loss: 0.333.. \n",
      "Epoch: 53/10000..  Training Loss: 0.332.. \n",
      "Epoch: 54/10000..  Training Loss: 0.330.. \n",
      "Epoch: 55/10000..  Training Loss: 0.328.. \n",
      "Epoch: 56/10000..  Training Loss: 0.328.. \n",
      "Epoch: 57/10000..  Training Loss: 0.327.. \n",
      "Epoch: 58/10000..  Training Loss: 0.325.. \n",
      "Epoch: 59/10000..  Training Loss: 0.324.. \n",
      "Epoch: 60/10000..  Training Loss: 0.324.. \n",
      "Epoch: 61/10000..  Training Loss: 0.322.. \n",
      "Epoch: 62/10000..  Training Loss: 0.321.. \n",
      "Epoch: 63/10000..  Training Loss: 0.320.. \n",
      "Epoch: 64/10000..  Training Loss: 0.320.. \n",
      "Epoch: 65/10000..  Training Loss: 0.319.. \n",
      "Epoch: 66/10000..  Training Loss: 0.317.. \n",
      "Epoch: 67/10000..  Training Loss: 0.316.. \n",
      "Epoch: 68/10000..  Training Loss: 0.316.. \n",
      "Epoch: 69/10000..  Training Loss: 0.315.. \n",
      "Epoch: 70/10000..  Training Loss: 0.314.. \n",
      "Epoch: 71/10000..  Training Loss: 0.313.. \n",
      "Epoch: 72/10000..  Training Loss: 0.312.. \n",
      "Epoch: 73/10000..  Training Loss: 0.311.. \n",
      "Epoch: 74/10000..  Training Loss: 0.311.. \n",
      "Epoch: 75/10000..  Training Loss: 0.310.. \n",
      "Epoch: 76/10000..  Training Loss: 0.310.. \n",
      "Epoch: 77/10000..  Training Loss: 0.310.. \n",
      "Epoch: 78/10000..  Training Loss: 0.309.. \n",
      "Epoch: 79/10000..  Training Loss: 0.308.. \n",
      "Epoch: 80/10000..  Training Loss: 0.306.. \n",
      "Epoch: 81/10000..  Training Loss: 0.305.. \n",
      "Epoch: 82/10000..  Training Loss: 0.305.. \n",
      "Epoch: 83/10000..  Training Loss: 0.304.. \n",
      "Epoch: 84/10000..  Training Loss: 0.304.. \n",
      "Epoch: 85/10000..  Training Loss: 0.305.. \n",
      "Epoch: 86/10000..  Training Loss: 0.304.. \n",
      "Epoch: 87/10000..  Training Loss: 0.303.. \n",
      "Epoch: 88/10000..  Training Loss: 0.301.. \n",
      "Epoch: 89/10000..  Training Loss: 0.300.. \n",
      "Epoch: 90/10000..  Training Loss: 0.300.. \n",
      "Epoch: 91/10000..  Training Loss: 0.300.. \n",
      "Epoch: 92/10000..  Training Loss: 0.300.. \n",
      "Epoch: 93/10000..  Training Loss: 0.300.. \n",
      "Epoch: 94/10000..  Training Loss: 0.300.. \n",
      "Epoch: 95/10000..  Training Loss: 0.298.. \n",
      "Epoch: 96/10000..  Training Loss: 0.297.. \n",
      "Epoch: 97/10000..  Training Loss: 0.296.. \n",
      "Epoch: 98/10000..  Training Loss: 0.295.. \n",
      "Epoch: 99/10000..  Training Loss: 0.296.. \n",
      "Epoch: 100/10000..  Training Loss: 0.296.. \n",
      "Epoch: 101/10000..  Training Loss: 0.296.. \n",
      "Epoch: 102/10000..  Training Loss: 0.296.. \n",
      "Epoch: 103/10000..  Training Loss: 0.295.. \n",
      "Epoch: 104/10000..  Training Loss: 0.293.. \n",
      "Epoch: 105/10000..  Training Loss: 0.292.. \n",
      "Epoch: 106/10000..  Training Loss: 0.292.. \n",
      "Epoch: 107/10000..  Training Loss: 0.292.. \n",
      "Epoch: 108/10000..  Training Loss: 0.293.. \n",
      "Epoch: 109/10000..  Training Loss: 0.292.. \n",
      "Epoch: 110/10000..  Training Loss: 0.292.. \n",
      "Epoch: 111/10000..  Training Loss: 0.290.. \n",
      "Epoch: 112/10000..  Training Loss: 0.289.. \n",
      "Epoch: 113/10000..  Training Loss: 0.288.. \n",
      "Epoch: 114/10000..  Training Loss: 0.289.. \n",
      "Epoch: 115/10000..  Training Loss: 0.289.. \n",
      "Epoch: 116/10000..  Training Loss: 0.289.. \n",
      "Epoch: 117/10000..  Training Loss: 0.289.. \n",
      "Epoch: 118/10000..  Training Loss: 0.288.. \n",
      "Epoch: 119/10000..  Training Loss: 0.287.. \n",
      "Epoch: 120/10000..  Training Loss: 0.286.. \n",
      "Epoch: 121/10000..  Training Loss: 0.285.. \n",
      "Epoch: 122/10000..  Training Loss: 0.285.. \n",
      "Epoch: 123/10000..  Training Loss: 0.285.. \n",
      "Epoch: 124/10000..  Training Loss: 0.285.. \n",
      "Epoch: 125/10000..  Training Loss: 0.285.. \n",
      "Epoch: 126/10000..  Training Loss: 0.286.. \n",
      "Epoch: 127/10000..  Training Loss: 0.287.. \n",
      "Epoch: 128/10000..  Training Loss: 0.288.. \n",
      "Epoch: 129/10000..  Training Loss: 0.287.. \n",
      "Epoch: 130/10000..  Training Loss: 0.285.. \n",
      "Epoch: 131/10000..  Training Loss: 0.282.. \n",
      "Epoch: 132/10000..  Training Loss: 0.281.. \n",
      "Epoch: 133/10000..  Training Loss: 0.282.. \n",
      "Epoch: 134/10000..  Training Loss: 0.284.. \n",
      "Epoch: 135/10000..  Training Loss: 0.284.. \n",
      "Epoch: 136/10000..  Training Loss: 0.283.. \n",
      "Epoch: 137/10000..  Training Loss: 0.281.. \n",
      "Epoch: 138/10000..  Training Loss: 0.280.. \n",
      "Epoch: 139/10000..  Training Loss: 0.280.. \n",
      "Epoch: 140/10000..  Training Loss: 0.280.. \n",
      "Epoch: 141/10000..  Training Loss: 0.281.. \n",
      "Epoch: 142/10000..  Training Loss: 0.281.. \n",
      "Epoch: 143/10000..  Training Loss: 0.280.. \n",
      "Epoch: 144/10000..  Training Loss: 0.279.. \n",
      "Epoch: 145/10000..  Training Loss: 0.278.. \n",
      "Epoch: 146/10000..  Training Loss: 0.277.. \n",
      "Epoch: 147/10000..  Training Loss: 0.278.. \n",
      "Epoch: 148/10000..  Training Loss: 0.278.. \n",
      "Epoch: 149/10000..  Training Loss: 0.279.. \n",
      "Epoch: 150/10000..  Training Loss: 0.278.. \n",
      "Epoch: 151/10000..  Training Loss: 0.278.. \n",
      "Epoch: 152/10000..  Training Loss: 0.277.. \n",
      "Epoch: 153/10000..  Training Loss: 0.276.. \n",
      "Epoch: 154/10000..  Training Loss: 0.275.. \n",
      "Epoch: 155/10000..  Training Loss: 0.275.. \n",
      "Epoch: 156/10000..  Training Loss: 0.275.. \n",
      "Epoch: 157/10000..  Training Loss: 0.275.. \n",
      "Epoch: 158/10000..  Training Loss: 0.275.. \n",
      "Epoch: 159/10000..  Training Loss: 0.276.. \n",
      "Epoch: 160/10000..  Training Loss: 0.277.. \n",
      "Epoch: 161/10000..  Training Loss: 0.277.. \n",
      "Epoch: 162/10000..  Training Loss: 0.277.. \n",
      "Epoch: 163/10000..  Training Loss: 0.275.. \n",
      "Epoch: 164/10000..  Training Loss: 0.273.. \n",
      "Epoch: 165/10000..  Training Loss: 0.272.. \n",
      "Epoch: 166/10000..  Training Loss: 0.272.. \n",
      "Epoch: 167/10000..  Training Loss: 0.272.. \n",
      "Epoch: 168/10000..  Training Loss: 0.273.. \n",
      "Epoch: 169/10000..  Training Loss: 0.274.. \n",
      "Epoch: 170/10000..  Training Loss: 0.275.. \n",
      "Epoch: 171/10000..  Training Loss: 0.274.. \n",
      "Epoch: 172/10000..  Training Loss: 0.273.. \n",
      "Epoch: 173/10000..  Training Loss: 0.271.. \n",
      "Epoch: 174/10000..  Training Loss: 0.270.. \n",
      "Epoch: 175/10000..  Training Loss: 0.270.. \n",
      "Epoch: 176/10000..  Training Loss: 0.270.. \n",
      "Epoch: 177/10000..  Training Loss: 0.271.. \n",
      "Epoch: 178/10000..  Training Loss: 0.272.. \n",
      "Epoch: 179/10000..  Training Loss: 0.272.. \n",
      "Epoch: 180/10000..  Training Loss: 0.271.. \n",
      "Epoch: 181/10000..  Training Loss: 0.270.. \n",
      "Epoch: 182/10000..  Training Loss: 0.269.. \n",
      "Epoch: 183/10000..  Training Loss: 0.268.. \n",
      "Epoch: 184/10000..  Training Loss: 0.268.. \n",
      "Epoch: 185/10000..  Training Loss: 0.268.. \n",
      "Epoch: 186/10000..  Training Loss: 0.268.. \n",
      "Epoch: 187/10000..  Training Loss: 0.269.. \n",
      "Epoch: 188/10000..  Training Loss: 0.269.. \n",
      "Epoch: 189/10000..  Training Loss: 0.269.. \n",
      "Epoch: 190/10000..  Training Loss: 0.269.. \n",
      "Epoch: 191/10000..  Training Loss: 0.268.. \n",
      "Epoch: 192/10000..  Training Loss: 0.267.. \n",
      "Epoch: 193/10000..  Training Loss: 0.266.. \n",
      "Epoch: 194/10000..  Training Loss: 0.265.. \n",
      "Epoch: 195/10000..  Training Loss: 0.265.. \n",
      "Epoch: 196/10000..  Training Loss: 0.265.. \n",
      "Epoch: 197/10000..  Training Loss: 0.266.. \n",
      "Epoch: 198/10000..  Training Loss: 0.266.. \n",
      "Epoch: 199/10000..  Training Loss: 0.267.. \n",
      "Epoch: 200/10000..  Training Loss: 0.269.. \n",
      "Epoch: 201/10000..  Training Loss: 0.270.. \n",
      "Epoch: 202/10000..  Training Loss: 0.270.. \n",
      "Epoch: 203/10000..  Training Loss: 0.268.. \n",
      "Epoch: 204/10000..  Training Loss: 0.266.. \n",
      "Epoch: 205/10000..  Training Loss: 0.264.. \n",
      "Epoch: 206/10000..  Training Loss: 0.263.. \n",
      "Epoch: 207/10000..  Training Loss: 0.264.. \n",
      "Epoch: 208/10000..  Training Loss: 0.265.. \n",
      "Epoch: 209/10000..  Training Loss: 0.267.. \n",
      "Epoch: 210/10000..  Training Loss: 0.266.. \n",
      "Epoch: 211/10000..  Training Loss: 0.265.. \n",
      "Epoch: 212/10000..  Training Loss: 0.263.. \n",
      "Epoch: 213/10000..  Training Loss: 0.262.. \n",
      "Epoch: 214/10000..  Training Loss: 0.262.. \n",
      "Epoch: 215/10000..  Training Loss: 0.262.. \n",
      "Epoch: 216/10000..  Training Loss: 0.263.. \n",
      "Epoch: 217/10000..  Training Loss: 0.264.. \n",
      "Epoch: 218/10000..  Training Loss: 0.264.. \n",
      "Epoch: 219/10000..  Training Loss: 0.263.. \n",
      "Epoch: 220/10000..  Training Loss: 0.261.. \n",
      "Epoch: 221/10000..  Training Loss: 0.260.. \n",
      "Epoch: 222/10000..  Training Loss: 0.260.. \n",
      "Epoch: 223/10000..  Training Loss: 0.260.. \n",
      "Epoch: 224/10000..  Training Loss: 0.260.. \n",
      "Epoch: 225/10000..  Training Loss: 0.261.. \n",
      "Epoch: 226/10000..  Training Loss: 0.261.. \n",
      "Epoch: 227/10000..  Training Loss: 0.262.. \n",
      "Epoch: 228/10000..  Training Loss: 0.261.. \n",
      "Epoch: 229/10000..  Training Loss: 0.261.. \n",
      "Epoch: 230/10000..  Training Loss: 0.260.. \n",
      "Epoch: 231/10000..  Training Loss: 0.259.. \n",
      "Epoch: 232/10000..  Training Loss: 0.258.. \n",
      "Epoch: 233/10000..  Training Loss: 0.258.. \n",
      "Epoch: 234/10000..  Training Loss: 0.258.. \n",
      "Epoch: 235/10000..  Training Loss: 0.258.. \n",
      "Epoch: 236/10000..  Training Loss: 0.258.. \n",
      "Epoch: 237/10000..  Training Loss: 0.258.. \n",
      "Epoch: 238/10000..  Training Loss: 0.259.. \n",
      "Epoch: 239/10000..  Training Loss: 0.260.. \n",
      "Epoch: 240/10000..  Training Loss: 0.262.. \n",
      "Epoch: 241/10000..  Training Loss: 0.263.. \n",
      "Epoch: 242/10000..  Training Loss: 0.265.. \n",
      "Epoch: 243/10000..  Training Loss: 0.262.. \n",
      "Epoch: 244/10000..  Training Loss: 0.258.. \n",
      "Epoch: 245/10000..  Training Loss: 0.256.. \n",
      "Epoch: 246/10000..  Training Loss: 0.256.. \n",
      "Epoch: 247/10000..  Training Loss: 0.258.. \n",
      "Epoch: 248/10000..  Training Loss: 0.260.. \n",
      "Epoch: 249/10000..  Training Loss: 0.261.. \n",
      "Epoch: 250/10000..  Training Loss: 0.259.. \n",
      "Epoch: 251/10000..  Training Loss: 0.257.. \n",
      "Epoch: 252/10000..  Training Loss: 0.255.. \n",
      "Epoch: 253/10000..  Training Loss: 0.255.. \n",
      "Epoch: 254/10000..  Training Loss: 0.255.. \n",
      "Epoch: 255/10000..  Training Loss: 0.256.. \n",
      "Epoch: 256/10000..  Training Loss: 0.257.. \n",
      "Epoch: 257/10000..  Training Loss: 0.257.. \n",
      "Epoch: 258/10000..  Training Loss: 0.257.. \n",
      "Epoch: 259/10000..  Training Loss: 0.255.. \n",
      "Epoch: 260/10000..  Training Loss: 0.254.. \n",
      "Epoch: 261/10000..  Training Loss: 0.253.. \n",
      "Epoch: 262/10000..  Training Loss: 0.253.. \n",
      "Epoch: 263/10000..  Training Loss: 0.254.. \n",
      "Epoch: 264/10000..  Training Loss: 0.254.. \n",
      "Epoch: 265/10000..  Training Loss: 0.255.. \n",
      "Epoch: 266/10000..  Training Loss: 0.255.. \n",
      "Epoch: 267/10000..  Training Loss: 0.255.. \n",
      "Epoch: 268/10000..  Training Loss: 0.254.. \n",
      "Epoch: 269/10000..  Training Loss: 0.254.. \n",
      "Epoch: 270/10000..  Training Loss: 0.252.. \n",
      "Epoch: 271/10000..  Training Loss: 0.252.. \n",
      "Epoch: 272/10000..  Training Loss: 0.251.. \n",
      "Epoch: 273/10000..  Training Loss: 0.251.. \n",
      "Epoch: 274/10000..  Training Loss: 0.252.. \n",
      "Epoch: 275/10000..  Training Loss: 0.252.. \n",
      "Epoch: 276/10000..  Training Loss: 0.254.. \n",
      "Epoch: 277/10000..  Training Loss: 0.255.. \n",
      "Epoch: 278/10000..  Training Loss: 0.256.. \n",
      "Epoch: 279/10000..  Training Loss: 0.256.. \n",
      "Epoch: 280/10000..  Training Loss: 0.255.. \n",
      "Epoch: 281/10000..  Training Loss: 0.252.. \n",
      "Epoch: 282/10000..  Training Loss: 0.250.. \n",
      "Epoch: 283/10000..  Training Loss: 0.250.. \n",
      "Epoch: 284/10000..  Training Loss: 0.251.. \n",
      "Epoch: 285/10000..  Training Loss: 0.253.. \n",
      "Epoch: 286/10000..  Training Loss: 0.254.. \n",
      "Epoch: 287/10000..  Training Loss: 0.255.. \n",
      "Epoch: 288/10000..  Training Loss: 0.254.. \n",
      "Epoch: 289/10000..  Training Loss: 0.252.. \n",
      "Epoch: 290/10000..  Training Loss: 0.249.. \n",
      "Epoch: 291/10000..  Training Loss: 0.248.. \n",
      "Epoch: 292/10000..  Training Loss: 0.249.. \n",
      "Epoch: 293/10000..  Training Loss: 0.250.. \n",
      "Epoch: 294/10000..  Training Loss: 0.252.. \n",
      "Epoch: 295/10000..  Training Loss: 0.252.. \n",
      "Epoch: 296/10000..  Training Loss: 0.251.. \n",
      "Epoch: 297/10000..  Training Loss: 0.249.. \n",
      "Epoch: 298/10000..  Training Loss: 0.248.. \n",
      "Epoch: 299/10000..  Training Loss: 0.247.. \n",
      "Epoch: 300/10000..  Training Loss: 0.247.. \n",
      "Epoch: 301/10000..  Training Loss: 0.248.. \n",
      "Epoch: 302/10000..  Training Loss: 0.249.. \n",
      "Epoch: 303/10000..  Training Loss: 0.250.. \n",
      "Epoch: 304/10000..  Training Loss: 0.250.. \n",
      "Epoch: 305/10000..  Training Loss: 0.249.. \n",
      "Epoch: 306/10000..  Training Loss: 0.248.. \n",
      "Epoch: 307/10000..  Training Loss: 0.247.. \n",
      "Epoch: 308/10000..  Training Loss: 0.246.. \n",
      "Epoch: 309/10000..  Training Loss: 0.246.. \n",
      "Epoch: 310/10000..  Training Loss: 0.246.. \n",
      "Epoch: 311/10000..  Training Loss: 0.246.. \n",
      "Epoch: 312/10000..  Training Loss: 0.246.. \n",
      "Epoch: 313/10000..  Training Loss: 0.247.. \n",
      "Epoch: 314/10000..  Training Loss: 0.248.. \n",
      "Epoch: 315/10000..  Training Loss: 0.249.. \n",
      "Epoch: 316/10000..  Training Loss: 0.250.. \n",
      "Epoch: 317/10000..  Training Loss: 0.250.. \n",
      "Epoch: 318/10000..  Training Loss: 0.250.. \n",
      "Epoch: 319/10000..  Training Loss: 0.247.. \n",
      "Epoch: 320/10000..  Training Loss: 0.245.. \n",
      "Epoch: 321/10000..  Training Loss: 0.244.. \n",
      "Epoch: 322/10000..  Training Loss: 0.244.. \n",
      "Epoch: 323/10000..  Training Loss: 0.245.. \n",
      "Epoch: 324/10000..  Training Loss: 0.246.. \n",
      "Epoch: 325/10000..  Training Loss: 0.248.. \n",
      "Epoch: 326/10000..  Training Loss: 0.249.. \n",
      "Epoch: 327/10000..  Training Loss: 0.250.. \n",
      "Epoch: 328/10000..  Training Loss: 0.248.. \n",
      "Epoch: 329/10000..  Training Loss: 0.246.. \n",
      "Epoch: 330/10000..  Training Loss: 0.244.. \n",
      "Epoch: 331/10000..  Training Loss: 0.243.. \n",
      "Epoch: 332/10000..  Training Loss: 0.243.. \n",
      "Epoch: 333/10000..  Training Loss: 0.244.. \n",
      "Epoch: 334/10000..  Training Loss: 0.246.. \n",
      "Epoch: 335/10000..  Training Loss: 0.247.. \n",
      "Epoch: 336/10000..  Training Loss: 0.247.. \n",
      "Epoch: 337/10000..  Training Loss: 0.246.. \n",
      "Epoch: 338/10000..  Training Loss: 0.244.. \n",
      "Epoch: 339/10000..  Training Loss: 0.242.. \n",
      "Epoch: 340/10000..  Training Loss: 0.242.. \n",
      "Epoch: 341/10000..  Training Loss: 0.242.. \n",
      "Epoch: 342/10000..  Training Loss: 0.242.. \n",
      "Epoch: 343/10000..  Training Loss: 0.243.. \n",
      "Epoch: 344/10000..  Training Loss: 0.244.. \n",
      "Epoch: 345/10000..  Training Loss: 0.245.. \n",
      "Epoch: 346/10000..  Training Loss: 0.245.. \n",
      "Epoch: 347/10000..  Training Loss: 0.244.. \n",
      "Epoch: 348/10000..  Training Loss: 0.243.. \n",
      "Epoch: 349/10000..  Training Loss: 0.241.. \n",
      "Epoch: 350/10000..  Training Loss: 0.241.. \n",
      "Epoch: 351/10000..  Training Loss: 0.240.. \n",
      "Epoch: 352/10000..  Training Loss: 0.240.. \n",
      "Epoch: 353/10000..  Training Loss: 0.241.. \n",
      "Epoch: 354/10000..  Training Loss: 0.242.. \n",
      "Epoch: 355/10000..  Training Loss: 0.243.. \n",
      "Epoch: 356/10000..  Training Loss: 0.244.. \n",
      "Epoch: 357/10000..  Training Loss: 0.244.. \n",
      "Epoch: 358/10000..  Training Loss: 0.244.. \n",
      "Epoch: 359/10000..  Training Loss: 0.243.. \n",
      "Epoch: 360/10000..  Training Loss: 0.241.. \n",
      "Epoch: 361/10000..  Training Loss: 0.240.. \n",
      "Epoch: 362/10000..  Training Loss: 0.239.. \n",
      "Epoch: 363/10000..  Training Loss: 0.239.. \n",
      "Epoch: 364/10000..  Training Loss: 0.239.. \n",
      "Epoch: 365/10000..  Training Loss: 0.240.. \n",
      "Epoch: 366/10000..  Training Loss: 0.241.. \n",
      "Epoch: 367/10000..  Training Loss: 0.242.. \n",
      "Epoch: 368/10000..  Training Loss: 0.243.. \n",
      "Epoch: 369/10000..  Training Loss: 0.244.. \n",
      "Epoch: 370/10000..  Training Loss: 0.243.. \n",
      "Epoch: 371/10000..  Training Loss: 0.242.. \n",
      "Epoch: 372/10000..  Training Loss: 0.239.. \n",
      "Epoch: 373/10000..  Training Loss: 0.238.. \n",
      "Epoch: 374/10000..  Training Loss: 0.238.. \n",
      "Epoch: 375/10000..  Training Loss: 0.238.. \n",
      "Epoch: 376/10000..  Training Loss: 0.239.. \n",
      "Epoch: 377/10000..  Training Loss: 0.240.. \n",
      "Epoch: 378/10000..  Training Loss: 0.242.. \n",
      "Epoch: 379/10000..  Training Loss: 0.242.. \n",
      "Epoch: 380/10000..  Training Loss: 0.243.. \n",
      "Epoch: 381/10000..  Training Loss: 0.241.. \n",
      "Epoch: 382/10000..  Training Loss: 0.239.. \n",
      "Epoch: 383/10000..  Training Loss: 0.237.. \n",
      "Epoch: 384/10000..  Training Loss: 0.236.. \n",
      "Epoch: 385/10000..  Training Loss: 0.237.. \n",
      "Epoch: 386/10000..  Training Loss: 0.238.. \n",
      "Epoch: 387/10000..  Training Loss: 0.240.. \n",
      "Epoch: 388/10000..  Training Loss: 0.240.. \n",
      "Epoch: 389/10000..  Training Loss: 0.241.. \n",
      "Epoch: 390/10000..  Training Loss: 0.239.. \n",
      "Epoch: 391/10000..  Training Loss: 0.238.. \n",
      "Epoch: 392/10000..  Training Loss: 0.236.. \n",
      "Epoch: 393/10000..  Training Loss: 0.235.. \n",
      "Epoch: 394/10000..  Training Loss: 0.235.. \n",
      "Epoch: 395/10000..  Training Loss: 0.236.. \n",
      "Epoch: 396/10000..  Training Loss: 0.237.. \n",
      "Epoch: 397/10000..  Training Loss: 0.238.. \n",
      "Epoch: 398/10000..  Training Loss: 0.239.. \n",
      "Epoch: 399/10000..  Training Loss: 0.240.. \n",
      "Epoch: 400/10000..  Training Loss: 0.239.. \n",
      "Epoch: 401/10000..  Training Loss: 0.238.. \n",
      "Epoch: 402/10000..  Training Loss: 0.236.. \n",
      "Epoch: 403/10000..  Training Loss: 0.235.. \n",
      "Epoch: 404/10000..  Training Loss: 0.234.. \n",
      "Epoch: 405/10000..  Training Loss: 0.234.. \n",
      "Epoch: 406/10000..  Training Loss: 0.235.. \n",
      "Epoch: 407/10000..  Training Loss: 0.236.. \n",
      "Epoch: 408/10000..  Training Loss: 0.237.. \n",
      "Epoch: 409/10000..  Training Loss: 0.238.. \n",
      "Epoch: 410/10000..  Training Loss: 0.238.. \n",
      "Epoch: 411/10000..  Training Loss: 0.238.. \n",
      "Epoch: 412/10000..  Training Loss: 0.237.. \n",
      "Epoch: 413/10000..  Training Loss: 0.236.. \n",
      "Epoch: 414/10000..  Training Loss: 0.234.. \n",
      "Epoch: 415/10000..  Training Loss: 0.233.. \n",
      "Epoch: 416/10000..  Training Loss: 0.233.. \n",
      "Epoch: 417/10000..  Training Loss: 0.233.. \n",
      "Epoch: 418/10000..  Training Loss: 0.234.. \n",
      "Epoch: 419/10000..  Training Loss: 0.235.. \n",
      "Epoch: 420/10000..  Training Loss: 0.237.. \n",
      "Epoch: 421/10000..  Training Loss: 0.238.. \n",
      "Epoch: 422/10000..  Training Loss: 0.239.. \n",
      "Epoch: 423/10000..  Training Loss: 0.237.. \n",
      "Epoch: 424/10000..  Training Loss: 0.235.. \n",
      "Epoch: 425/10000..  Training Loss: 0.233.. \n",
      "Epoch: 426/10000..  Training Loss: 0.232.. \n",
      "Epoch: 427/10000..  Training Loss: 0.232.. \n",
      "Epoch: 428/10000..  Training Loss: 0.233.. \n",
      "Epoch: 429/10000..  Training Loss: 0.235.. \n",
      "Epoch: 430/10000..  Training Loss: 0.236.. \n",
      "Epoch: 431/10000..  Training Loss: 0.238.. \n",
      "Epoch: 432/10000..  Training Loss: 0.237.. \n",
      "Epoch: 433/10000..  Training Loss: 0.236.. \n",
      "Epoch: 434/10000..  Training Loss: 0.234.. \n",
      "Epoch: 435/10000..  Training Loss: 0.232.. \n",
      "Epoch: 436/10000..  Training Loss: 0.231.. \n",
      "Epoch: 437/10000..  Training Loss: 0.232.. \n",
      "Epoch: 438/10000..  Training Loss: 0.233.. \n",
      "Epoch: 439/10000..  Training Loss: 0.234.. \n",
      "Epoch: 440/10000..  Training Loss: 0.236.. \n",
      "Epoch: 441/10000..  Training Loss: 0.235.. \n",
      "Epoch: 442/10000..  Training Loss: 0.235.. \n",
      "Epoch: 443/10000..  Training Loss: 0.233.. \n",
      "Epoch: 444/10000..  Training Loss: 0.231.. \n",
      "Epoch: 445/10000..  Training Loss: 0.230.. \n",
      "Epoch: 446/10000..  Training Loss: 0.230.. \n",
      "Epoch: 447/10000..  Training Loss: 0.231.. \n",
      "Epoch: 448/10000..  Training Loss: 0.231.. \n",
      "Epoch: 449/10000..  Training Loss: 0.233.. \n",
      "Epoch: 450/10000..  Training Loss: 0.233.. \n",
      "Epoch: 451/10000..  Training Loss: 0.234.. \n",
      "Epoch: 452/10000..  Training Loss: 0.234.. \n",
      "Epoch: 453/10000..  Training Loss: 0.234.. \n",
      "Epoch: 454/10000..  Training Loss: 0.232.. \n",
      "Epoch: 455/10000..  Training Loss: 0.230.. \n",
      "Epoch: 456/10000..  Training Loss: 0.229.. \n",
      "Epoch: 457/10000..  Training Loss: 0.229.. \n",
      "Epoch: 458/10000..  Training Loss: 0.229.. \n",
      "Epoch: 459/10000..  Training Loss: 0.230.. \n",
      "Epoch: 460/10000..  Training Loss: 0.232.. \n",
      "Epoch: 461/10000..  Training Loss: 0.232.. \n",
      "Epoch: 462/10000..  Training Loss: 0.234.. \n",
      "Epoch: 463/10000..  Training Loss: 0.234.. \n",
      "Epoch: 464/10000..  Training Loss: 0.235.. \n",
      "Epoch: 465/10000..  Training Loss: 0.232.. \n",
      "Epoch: 466/10000..  Training Loss: 0.231.. \n",
      "Epoch: 467/10000..  Training Loss: 0.229.. \n",
      "Epoch: 468/10000..  Training Loss: 0.228.. \n",
      "Epoch: 469/10000..  Training Loss: 0.229.. \n",
      "Epoch: 470/10000..  Training Loss: 0.230.. \n",
      "Epoch: 471/10000..  Training Loss: 0.232.. \n",
      "Epoch: 472/10000..  Training Loss: 0.234.. \n",
      "Epoch: 473/10000..  Training Loss: 0.235.. \n",
      "Epoch: 474/10000..  Training Loss: 0.233.. \n",
      "Epoch: 475/10000..  Training Loss: 0.231.. \n",
      "Epoch: 476/10000..  Training Loss: 0.229.. \n",
      "Epoch: 477/10000..  Training Loss: 0.228.. \n",
      "Epoch: 478/10000..  Training Loss: 0.228.. \n",
      "Epoch: 479/10000..  Training Loss: 0.229.. \n",
      "Epoch: 480/10000..  Training Loss: 0.231.. \n",
      "Epoch: 481/10000..  Training Loss: 0.232.. \n",
      "Epoch: 482/10000..  Training Loss: 0.232.. \n",
      "Epoch: 483/10000..  Training Loss: 0.230.. \n",
      "Epoch: 484/10000..  Training Loss: 0.229.. \n",
      "Epoch: 485/10000..  Training Loss: 0.227.. \n",
      "Epoch: 486/10000..  Training Loss: 0.227.. \n",
      "Epoch: 487/10000..  Training Loss: 0.227.. \n",
      "Epoch: 488/10000..  Training Loss: 0.228.. \n",
      "Epoch: 489/10000..  Training Loss: 0.229.. \n",
      "Epoch: 490/10000..  Training Loss: 0.230.. \n",
      "Epoch: 491/10000..  Training Loss: 0.230.. \n",
      "Epoch: 492/10000..  Training Loss: 0.229.. \n",
      "Epoch: 493/10000..  Training Loss: 0.228.. \n",
      "Epoch: 494/10000..  Training Loss: 0.227.. \n",
      "Epoch: 495/10000..  Training Loss: 0.226.. \n",
      "Epoch: 496/10000..  Training Loss: 0.226.. \n",
      "Epoch: 497/10000..  Training Loss: 0.226.. \n",
      "Epoch: 498/10000..  Training Loss: 0.227.. \n",
      "Epoch: 499/10000..  Training Loss: 0.228.. \n",
      "Epoch: 500/10000..  Training Loss: 0.229.. \n",
      "Epoch: 501/10000..  Training Loss: 0.230.. \n",
      "Epoch: 502/10000..  Training Loss: 0.231.. \n",
      "Epoch: 503/10000..  Training Loss: 0.229.. \n",
      "Epoch: 504/10000..  Training Loss: 0.228.. \n",
      "Epoch: 505/10000..  Training Loss: 0.226.. \n",
      "Epoch: 506/10000..  Training Loss: 0.225.. \n",
      "Epoch: 507/10000..  Training Loss: 0.225.. \n",
      "Epoch: 508/10000..  Training Loss: 0.225.. \n",
      "Epoch: 509/10000..  Training Loss: 0.226.. \n",
      "Epoch: 510/10000..  Training Loss: 0.227.. \n",
      "Epoch: 511/10000..  Training Loss: 0.229.. \n",
      "Epoch: 512/10000..  Training Loss: 0.229.. \n",
      "Epoch: 513/10000..  Training Loss: 0.231.. \n",
      "Epoch: 514/10000..  Training Loss: 0.230.. \n",
      "Epoch: 515/10000..  Training Loss: 0.229.. \n",
      "Epoch: 516/10000..  Training Loss: 0.226.. \n",
      "Epoch: 517/10000..  Training Loss: 0.225.. \n",
      "Epoch: 518/10000..  Training Loss: 0.224.. \n",
      "Epoch: 519/10000..  Training Loss: 0.224.. \n",
      "Epoch: 520/10000..  Training Loss: 0.225.. \n",
      "Epoch: 521/10000..  Training Loss: 0.227.. \n",
      "Epoch: 522/10000..  Training Loss: 0.228.. \n",
      "Epoch: 523/10000..  Training Loss: 0.229.. \n",
      "Epoch: 524/10000..  Training Loss: 0.230.. \n",
      "Epoch: 525/10000..  Training Loss: 0.228.. \n",
      "Epoch: 526/10000..  Training Loss: 0.226.. \n",
      "Epoch: 527/10000..  Training Loss: 0.224.. \n",
      "Epoch: 528/10000..  Training Loss: 0.223.. \n",
      "Epoch: 529/10000..  Training Loss: 0.224.. \n",
      "Epoch: 530/10000..  Training Loss: 0.224.. \n",
      "Epoch: 531/10000..  Training Loss: 0.226.. \n",
      "Epoch: 532/10000..  Training Loss: 0.228.. \n",
      "Epoch: 533/10000..  Training Loss: 0.229.. \n",
      "Epoch: 534/10000..  Training Loss: 0.229.. \n",
      "Epoch: 535/10000..  Training Loss: 0.228.. \n",
      "Epoch: 536/10000..  Training Loss: 0.225.. \n",
      "Epoch: 537/10000..  Training Loss: 0.223.. \n",
      "Epoch: 538/10000..  Training Loss: 0.223.. \n",
      "Epoch: 539/10000..  Training Loss: 0.223.. \n",
      "Epoch: 540/10000..  Training Loss: 0.225.. \n",
      "Epoch: 541/10000..  Training Loss: 0.226.. \n",
      "Epoch: 542/10000..  Training Loss: 0.228.. \n",
      "Epoch: 543/10000..  Training Loss: 0.228.. \n",
      "Epoch: 544/10000..  Training Loss: 0.227.. \n",
      "Epoch: 545/10000..  Training Loss: 0.225.. \n",
      "Epoch: 546/10000..  Training Loss: 0.223.. \n",
      "Epoch: 547/10000..  Training Loss: 0.222.. \n",
      "Epoch: 548/10000..  Training Loss: 0.223.. \n",
      "Epoch: 549/10000..  Training Loss: 0.224.. \n",
      "Epoch: 550/10000..  Training Loss: 0.225.. \n",
      "Epoch: 551/10000..  Training Loss: 0.227.. \n",
      "Epoch: 552/10000..  Training Loss: 0.227.. \n",
      "Epoch: 553/10000..  Training Loss: 0.227.. \n",
      "Epoch: 554/10000..  Training Loss: 0.224.. \n",
      "Epoch: 555/10000..  Training Loss: 0.223.. \n",
      "Epoch: 556/10000..  Training Loss: 0.221.. \n",
      "Epoch: 557/10000..  Training Loss: 0.222.. \n",
      "Epoch: 558/10000..  Training Loss: 0.222.. \n",
      "Epoch: 559/10000..  Training Loss: 0.223.. \n",
      "Epoch: 560/10000..  Training Loss: 0.225.. \n",
      "Epoch: 561/10000..  Training Loss: 0.225.. \n",
      "Epoch: 562/10000..  Training Loss: 0.226.. \n",
      "Epoch: 563/10000..  Training Loss: 0.225.. \n",
      "Epoch: 564/10000..  Training Loss: 0.224.. \n",
      "Epoch: 565/10000..  Training Loss: 0.222.. \n",
      "Epoch: 566/10000..  Training Loss: 0.221.. \n",
      "Epoch: 567/10000..  Training Loss: 0.221.. \n",
      "Epoch: 568/10000..  Training Loss: 0.221.. \n",
      "Epoch: 569/10000..  Training Loss: 0.222.. \n",
      "Epoch: 570/10000..  Training Loss: 0.223.. \n",
      "Epoch: 571/10000..  Training Loss: 0.225.. \n",
      "Epoch: 572/10000..  Training Loss: 0.224.. \n",
      "Epoch: 573/10000..  Training Loss: 0.224.. \n",
      "Epoch: 574/10000..  Training Loss: 0.223.. \n",
      "Epoch: 575/10000..  Training Loss: 0.222.. \n",
      "Epoch: 576/10000..  Training Loss: 0.221.. \n",
      "Epoch: 577/10000..  Training Loss: 0.220.. \n",
      "Epoch: 578/10000..  Training Loss: 0.220.. \n",
      "Epoch: 579/10000..  Training Loss: 0.220.. \n",
      "Epoch: 580/10000..  Training Loss: 0.221.. \n",
      "Epoch: 581/10000..  Training Loss: 0.221.. \n",
      "Epoch: 582/10000..  Training Loss: 0.223.. \n",
      "Epoch: 583/10000..  Training Loss: 0.223.. \n",
      "Epoch: 584/10000..  Training Loss: 0.225.. \n",
      "Epoch: 585/10000..  Training Loss: 0.224.. \n",
      "Epoch: 586/10000..  Training Loss: 0.225.. \n",
      "Epoch: 587/10000..  Training Loss: 0.223.. \n",
      "Epoch: 588/10000..  Training Loss: 0.221.. \n",
      "Epoch: 589/10000..  Training Loss: 0.220.. \n",
      "Epoch: 590/10000..  Training Loss: 0.219.. \n",
      "Epoch: 591/10000..  Training Loss: 0.219.. \n",
      "Epoch: 592/10000..  Training Loss: 0.219.. \n",
      "Epoch: 593/10000..  Training Loss: 0.220.. \n",
      "Epoch: 594/10000..  Training Loss: 0.221.. \n",
      "Epoch: 595/10000..  Training Loss: 0.223.. \n",
      "Epoch: 596/10000..  Training Loss: 0.224.. \n",
      "Epoch: 597/10000..  Training Loss: 0.225.. \n",
      "Epoch: 598/10000..  Training Loss: 0.224.. \n",
      "Epoch: 599/10000..  Training Loss: 0.223.. \n",
      "Epoch: 600/10000..  Training Loss: 0.221.. \n",
      "Epoch: 601/10000..  Training Loss: 0.219.. \n",
      "Epoch: 602/10000..  Training Loss: 0.218.. \n",
      "Epoch: 603/10000..  Training Loss: 0.219.. \n",
      "Epoch: 604/10000..  Training Loss: 0.220.. \n",
      "Epoch: 605/10000..  Training Loss: 0.221.. \n",
      "Epoch: 606/10000..  Training Loss: 0.224.. \n",
      "Epoch: 607/10000..  Training Loss: 0.224.. \n",
      "Epoch: 608/10000..  Training Loss: 0.226.. \n",
      "Epoch: 609/10000..  Training Loss: 0.224.. \n",
      "Epoch: 610/10000..  Training Loss: 0.222.. \n",
      "Epoch: 611/10000..  Training Loss: 0.219.. \n",
      "Epoch: 612/10000..  Training Loss: 0.218.. \n",
      "Epoch: 613/10000..  Training Loss: 0.218.. \n",
      "Epoch: 614/10000..  Training Loss: 0.219.. \n",
      "Epoch: 615/10000..  Training Loss: 0.222.. \n",
      "Epoch: 616/10000..  Training Loss: 0.223.. \n",
      "Epoch: 617/10000..  Training Loss: 0.225.. \n",
      "Epoch: 618/10000..  Training Loss: 0.222.. \n",
      "Epoch: 619/10000..  Training Loss: 0.221.. \n",
      "Epoch: 620/10000..  Training Loss: 0.218.. \n",
      "Epoch: 621/10000..  Training Loss: 0.217.. \n",
      "Epoch: 622/10000..  Training Loss: 0.218.. \n",
      "Epoch: 623/10000..  Training Loss: 0.219.. \n",
      "Epoch: 624/10000..  Training Loss: 0.220.. \n",
      "Epoch: 625/10000..  Training Loss: 0.221.. \n",
      "Epoch: 626/10000..  Training Loss: 0.222.. \n",
      "Epoch: 627/10000..  Training Loss: 0.221.. \n",
      "Epoch: 628/10000..  Training Loss: 0.220.. \n",
      "Epoch: 629/10000..  Training Loss: 0.218.. \n",
      "Epoch: 630/10000..  Training Loss: 0.217.. \n",
      "Epoch: 631/10000..  Training Loss: 0.217.. \n",
      "Epoch: 632/10000..  Training Loss: 0.217.. \n",
      "Epoch: 633/10000..  Training Loss: 0.218.. \n",
      "Epoch: 634/10000..  Training Loss: 0.219.. \n",
      "Epoch: 635/10000..  Training Loss: 0.221.. \n",
      "Epoch: 636/10000..  Training Loss: 0.221.. \n",
      "Epoch: 637/10000..  Training Loss: 0.221.. \n",
      "Epoch: 638/10000..  Training Loss: 0.219.. \n",
      "Epoch: 639/10000..  Training Loss: 0.218.. \n",
      "Epoch: 640/10000..  Training Loss: 0.217.. \n",
      "Epoch: 641/10000..  Training Loss: 0.216.. \n",
      "Epoch: 642/10000..  Training Loss: 0.216.. \n",
      "Epoch: 643/10000..  Training Loss: 0.216.. \n",
      "Epoch: 644/10000..  Training Loss: 0.217.. \n",
      "Epoch: 645/10000..  Training Loss: 0.218.. \n",
      "Epoch: 646/10000..  Training Loss: 0.220.. \n",
      "Epoch: 647/10000..  Training Loss: 0.220.. \n",
      "Epoch: 648/10000..  Training Loss: 0.221.. \n",
      "Epoch: 649/10000..  Training Loss: 0.220.. \n",
      "Epoch: 650/10000..  Training Loss: 0.219.. \n",
      "Epoch: 651/10000..  Training Loss: 0.217.. \n",
      "Epoch: 652/10000..  Training Loss: 0.216.. \n",
      "Epoch: 653/10000..  Training Loss: 0.215.. \n",
      "Epoch: 654/10000..  Training Loss: 0.215.. \n",
      "Epoch: 655/10000..  Training Loss: 0.216.. \n",
      "Epoch: 656/10000..  Training Loss: 0.217.. \n",
      "Epoch: 657/10000..  Training Loss: 0.218.. \n",
      "Epoch: 658/10000..  Training Loss: 0.219.. \n",
      "Epoch: 659/10000..  Training Loss: 0.221.. \n",
      "Epoch: 660/10000..  Training Loss: 0.221.. \n",
      "Epoch: 661/10000..  Training Loss: 0.221.. \n",
      "Epoch: 662/10000..  Training Loss: 0.218.. \n",
      "Epoch: 663/10000..  Training Loss: 0.216.. \n",
      "Epoch: 664/10000..  Training Loss: 0.215.. \n",
      "Epoch: 665/10000..  Training Loss: 0.215.. \n",
      "Epoch: 666/10000..  Training Loss: 0.215.. \n",
      "Epoch: 667/10000..  Training Loss: 0.217.. \n",
      "Epoch: 668/10000..  Training Loss: 0.218.. \n",
      "Epoch: 669/10000..  Training Loss: 0.220.. \n",
      "Epoch: 670/10000..  Training Loss: 0.222.. \n",
      "Epoch: 671/10000..  Training Loss: 0.220.. \n",
      "Epoch: 672/10000..  Training Loss: 0.219.. \n",
      "Epoch: 673/10000..  Training Loss: 0.216.. \n",
      "Epoch: 674/10000..  Training Loss: 0.215.. \n",
      "Epoch: 675/10000..  Training Loss: 0.214.. \n",
      "Epoch: 676/10000..  Training Loss: 0.215.. \n",
      "Epoch: 677/10000..  Training Loss: 0.216.. \n",
      "Epoch: 678/10000..  Training Loss: 0.217.. \n",
      "Epoch: 679/10000..  Training Loss: 0.219.. \n",
      "Epoch: 680/10000..  Training Loss: 0.220.. \n",
      "Epoch: 681/10000..  Training Loss: 0.221.. \n",
      "Epoch: 682/10000..  Training Loss: 0.218.. \n",
      "Epoch: 683/10000..  Training Loss: 0.216.. \n",
      "Epoch: 684/10000..  Training Loss: 0.214.. \n",
      "Epoch: 685/10000..  Training Loss: 0.214.. \n",
      "Epoch: 686/10000..  Training Loss: 0.214.. \n",
      "Epoch: 687/10000..  Training Loss: 0.215.. \n",
      "Epoch: 688/10000..  Training Loss: 0.216.. \n",
      "Epoch: 689/10000..  Training Loss: 0.217.. \n",
      "Epoch: 690/10000..  Training Loss: 0.219.. \n",
      "Epoch: 691/10000..  Training Loss: 0.218.. \n",
      "Epoch: 692/10000..  Training Loss: 0.218.. \n",
      "Epoch: 693/10000..  Training Loss: 0.216.. \n",
      "Epoch: 694/10000..  Training Loss: 0.215.. \n",
      "Epoch: 695/10000..  Training Loss: 0.214.. \n",
      "Epoch: 696/10000..  Training Loss: 0.213.. \n",
      "Epoch: 697/10000..  Training Loss: 0.213.. \n",
      "Epoch: 698/10000..  Training Loss: 0.214.. \n",
      "Epoch: 699/10000..  Training Loss: 0.214.. \n",
      "Epoch: 700/10000..  Training Loss: 0.215.. \n",
      "Epoch: 701/10000..  Training Loss: 0.217.. \n",
      "Epoch: 702/10000..  Training Loss: 0.218.. \n",
      "Epoch: 703/10000..  Training Loss: 0.219.. \n",
      "Epoch: 704/10000..  Training Loss: 0.218.. \n",
      "Epoch: 705/10000..  Training Loss: 0.217.. \n",
      "Epoch: 706/10000..  Training Loss: 0.215.. \n",
      "Epoch: 707/10000..  Training Loss: 0.213.. \n",
      "Epoch: 708/10000..  Training Loss: 0.213.. \n",
      "Epoch: 709/10000..  Training Loss: 0.212.. \n",
      "Epoch: 710/10000..  Training Loss: 0.213.. \n",
      "Epoch: 711/10000..  Training Loss: 0.214.. \n",
      "Epoch: 712/10000..  Training Loss: 0.215.. \n",
      "Epoch: 713/10000..  Training Loss: 0.217.. \n",
      "Epoch: 714/10000..  Training Loss: 0.219.. \n",
      "Epoch: 715/10000..  Training Loss: 0.219.. \n",
      "Epoch: 716/10000..  Training Loss: 0.218.. \n",
      "Epoch: 717/10000..  Training Loss: 0.215.. \n",
      "Epoch: 718/10000..  Training Loss: 0.213.. \n",
      "Epoch: 719/10000..  Training Loss: 0.212.. \n",
      "Epoch: 720/10000..  Training Loss: 0.212.. \n",
      "Epoch: 721/10000..  Training Loss: 0.213.. \n",
      "Epoch: 722/10000..  Training Loss: 0.214.. \n",
      "Epoch: 723/10000..  Training Loss: 0.216.. \n",
      "Epoch: 724/10000..  Training Loss: 0.217.. \n",
      "Epoch: 725/10000..  Training Loss: 0.218.. \n",
      "Epoch: 726/10000..  Training Loss: 0.217.. \n",
      "Epoch: 727/10000..  Training Loss: 0.216.. \n",
      "Epoch: 728/10000..  Training Loss: 0.213.. \n",
      "Epoch: 729/10000..  Training Loss: 0.212.. \n",
      "Epoch: 730/10000..  Training Loss: 0.211.. \n",
      "Epoch: 731/10000..  Training Loss: 0.212.. \n",
      "Epoch: 732/10000..  Training Loss: 0.213.. \n",
      "Epoch: 733/10000..  Training Loss: 0.214.. \n",
      "Epoch: 734/10000..  Training Loss: 0.216.. \n",
      "Epoch: 735/10000..  Training Loss: 0.216.. \n",
      "Epoch: 736/10000..  Training Loss: 0.217.. \n",
      "Epoch: 737/10000..  Training Loss: 0.215.. \n",
      "Epoch: 738/10000..  Training Loss: 0.214.. \n",
      "Epoch: 739/10000..  Training Loss: 0.212.. \n",
      "Epoch: 740/10000..  Training Loss: 0.211.. \n",
      "Epoch: 741/10000..  Training Loss: 0.211.. \n",
      "Epoch: 742/10000..  Training Loss: 0.212.. \n",
      "Epoch: 743/10000..  Training Loss: 0.213.. \n",
      "Epoch: 744/10000..  Training Loss: 0.214.. \n",
      "Epoch: 745/10000..  Training Loss: 0.216.. \n",
      "Epoch: 746/10000..  Training Loss: 0.216.. \n",
      "Epoch: 747/10000..  Training Loss: 0.216.. \n",
      "Epoch: 748/10000..  Training Loss: 0.214.. \n",
      "Epoch: 749/10000..  Training Loss: 0.213.. \n",
      "Epoch: 750/10000..  Training Loss: 0.211.. \n",
      "Epoch: 751/10000..  Training Loss: 0.211.. \n",
      "Epoch: 752/10000..  Training Loss: 0.210.. \n",
      "Epoch: 753/10000..  Training Loss: 0.211.. \n",
      "Epoch: 754/10000..  Training Loss: 0.211.. \n",
      "Epoch: 755/10000..  Training Loss: 0.212.. \n",
      "Epoch: 756/10000..  Training Loss: 0.214.. \n",
      "Epoch: 757/10000..  Training Loss: 0.215.. \n",
      "Epoch: 758/10000..  Training Loss: 0.217.. \n",
      "Epoch: 759/10000..  Training Loss: 0.216.. \n",
      "Epoch: 760/10000..  Training Loss: 0.216.. \n",
      "Epoch: 761/10000..  Training Loss: 0.213.. \n",
      "Epoch: 762/10000..  Training Loss: 0.212.. \n",
      "Epoch: 763/10000..  Training Loss: 0.210.. \n",
      "Epoch: 764/10000..  Training Loss: 0.210.. \n",
      "Epoch: 765/10000..  Training Loss: 0.210.. \n",
      "Epoch: 766/10000..  Training Loss: 0.211.. \n",
      "Epoch: 767/10000..  Training Loss: 0.213.. \n",
      "Epoch: 768/10000..  Training Loss: 0.214.. \n",
      "Epoch: 769/10000..  Training Loss: 0.216.. \n",
      "Epoch: 770/10000..  Training Loss: 0.216.. \n",
      "Epoch: 771/10000..  Training Loss: 0.215.. \n",
      "Epoch: 772/10000..  Training Loss: 0.213.. \n",
      "Epoch: 773/10000..  Training Loss: 0.211.. \n",
      "Epoch: 774/10000..  Training Loss: 0.210.. \n",
      "Epoch: 775/10000..  Training Loss: 0.209.. \n",
      "Epoch: 776/10000..  Training Loss: 0.210.. \n",
      "Epoch: 777/10000..  Training Loss: 0.211.. \n",
      "Epoch: 778/10000..  Training Loss: 0.212.. \n",
      "Epoch: 779/10000..  Training Loss: 0.213.. \n",
      "Epoch: 780/10000..  Training Loss: 0.215.. \n",
      "Epoch: 781/10000..  Training Loss: 0.215.. \n",
      "Epoch: 782/10000..  Training Loss: 0.215.. \n",
      "Epoch: 783/10000..  Training Loss: 0.212.. \n",
      "Epoch: 784/10000..  Training Loss: 0.211.. \n",
      "Epoch: 785/10000..  Training Loss: 0.209.. \n",
      "Epoch: 786/10000..  Training Loss: 0.209.. \n",
      "Epoch: 787/10000..  Training Loss: 0.209.. \n",
      "Epoch: 788/10000..  Training Loss: 0.210.. \n",
      "Epoch: 789/10000..  Training Loss: 0.211.. \n",
      "Epoch: 790/10000..  Training Loss: 0.212.. \n",
      "Epoch: 791/10000..  Training Loss: 0.214.. \n",
      "Epoch: 792/10000..  Training Loss: 0.214.. \n",
      "Epoch: 793/10000..  Training Loss: 0.214.. \n",
      "Epoch: 794/10000..  Training Loss: 0.213.. \n",
      "Epoch: 795/10000..  Training Loss: 0.211.. \n",
      "Epoch: 796/10000..  Training Loss: 0.210.. \n",
      "Epoch: 797/10000..  Training Loss: 0.209.. \n",
      "Epoch: 798/10000..  Training Loss: 0.208.. \n",
      "Epoch: 799/10000..  Training Loss: 0.208.. \n",
      "Epoch: 800/10000..  Training Loss: 0.209.. \n",
      "Epoch: 801/10000..  Training Loss: 0.210.. \n",
      "Epoch: 802/10000..  Training Loss: 0.211.. \n",
      "Epoch: 803/10000..  Training Loss: 0.212.. \n",
      "Epoch: 804/10000..  Training Loss: 0.214.. \n",
      "Epoch: 805/10000..  Training Loss: 0.214.. \n",
      "Epoch: 806/10000..  Training Loss: 0.215.. \n",
      "Epoch: 807/10000..  Training Loss: 0.213.. \n",
      "Epoch: 808/10000..  Training Loss: 0.211.. \n",
      "Epoch: 809/10000..  Training Loss: 0.209.. \n",
      "Epoch: 810/10000..  Training Loss: 0.208.. \n",
      "Epoch: 811/10000..  Training Loss: 0.208.. \n",
      "Epoch: 812/10000..  Training Loss: 0.209.. \n",
      "Epoch: 813/10000..  Training Loss: 0.211.. \n",
      "Epoch: 814/10000..  Training Loss: 0.213.. \n",
      "Epoch: 815/10000..  Training Loss: 0.215.. \n",
      "Epoch: 816/10000..  Training Loss: 0.214.. \n",
      "Epoch: 817/10000..  Training Loss: 0.213.. \n",
      "Epoch: 818/10000..  Training Loss: 0.210.. \n",
      "Epoch: 819/10000..  Training Loss: 0.208.. \n",
      "Epoch: 820/10000..  Training Loss: 0.207.. \n",
      "Epoch: 821/10000..  Training Loss: 0.208.. \n",
      "Epoch: 822/10000..  Training Loss: 0.209.. \n",
      "Epoch: 823/10000..  Training Loss: 0.211.. \n",
      "Epoch: 824/10000..  Training Loss: 0.213.. \n",
      "Epoch: 825/10000..  Training Loss: 0.213.. \n",
      "Epoch: 826/10000..  Training Loss: 0.213.. \n",
      "Epoch: 827/10000..  Training Loss: 0.211.. \n",
      "Epoch: 828/10000..  Training Loss: 0.209.. \n",
      "Epoch: 829/10000..  Training Loss: 0.208.. \n",
      "Epoch: 830/10000..  Training Loss: 0.207.. \n",
      "Epoch: 831/10000..  Training Loss: 0.207.. \n",
      "Epoch: 832/10000..  Training Loss: 0.208.. \n",
      "Epoch: 833/10000..  Training Loss: 0.209.. \n",
      "Epoch: 834/10000..  Training Loss: 0.210.. \n",
      "Epoch: 835/10000..  Training Loss: 0.212.. \n",
      "Epoch: 836/10000..  Training Loss: 0.212.. \n",
      "Epoch: 837/10000..  Training Loss: 0.212.. \n",
      "Epoch: 838/10000..  Training Loss: 0.210.. \n",
      "Epoch: 839/10000..  Training Loss: 0.209.. \n",
      "Epoch: 840/10000..  Training Loss: 0.207.. \n",
      "Epoch: 841/10000..  Training Loss: 0.207.. \n",
      "Epoch: 842/10000..  Training Loss: 0.207.. \n",
      "Epoch: 843/10000..  Training Loss: 0.207.. \n",
      "Epoch: 844/10000..  Training Loss: 0.208.. \n",
      "Epoch: 845/10000..  Training Loss: 0.209.. \n",
      "Epoch: 846/10000..  Training Loss: 0.210.. \n",
      "Epoch: 847/10000..  Training Loss: 0.211.. \n",
      "Epoch: 848/10000..  Training Loss: 0.212.. \n",
      "Epoch: 849/10000..  Training Loss: 0.211.. \n",
      "Epoch: 850/10000..  Training Loss: 0.211.. \n",
      "Epoch: 851/10000..  Training Loss: 0.209.. \n",
      "Epoch: 852/10000..  Training Loss: 0.207.. \n",
      "Epoch: 853/10000..  Training Loss: 0.206.. \n",
      "Epoch: 854/10000..  Training Loss: 0.206.. \n",
      "Epoch: 855/10000..  Training Loss: 0.206.. \n",
      "Epoch: 856/10000..  Training Loss: 0.207.. \n",
      "Epoch: 857/10000..  Training Loss: 0.208.. \n",
      "Epoch: 858/10000..  Training Loss: 0.209.. \n",
      "Epoch: 859/10000..  Training Loss: 0.211.. \n",
      "Epoch: 860/10000..  Training Loss: 0.211.. \n",
      "Epoch: 861/10000..  Training Loss: 0.212.. \n",
      "Epoch: 862/10000..  Training Loss: 0.211.. \n",
      "Epoch: 863/10000..  Training Loss: 0.209.. \n",
      "Epoch: 864/10000..  Training Loss: 0.207.. \n",
      "Epoch: 865/10000..  Training Loss: 0.206.. \n",
      "Epoch: 866/10000..  Training Loss: 0.206.. \n",
      "Epoch: 867/10000..  Training Loss: 0.206.. \n",
      "Epoch: 868/10000..  Training Loss: 0.206.. \n",
      "Epoch: 869/10000..  Training Loss: 0.207.. \n",
      "Epoch: 870/10000..  Training Loss: 0.209.. \n",
      "Epoch: 871/10000..  Training Loss: 0.210.. \n",
      "Epoch: 872/10000..  Training Loss: 0.211.. \n",
      "Epoch: 873/10000..  Training Loss: 0.210.. \n",
      "Epoch: 874/10000..  Training Loss: 0.210.. \n",
      "Epoch: 875/10000..  Training Loss: 0.208.. \n",
      "Epoch: 876/10000..  Training Loss: 0.207.. \n",
      "Epoch: 877/10000..  Training Loss: 0.205.. \n",
      "Epoch: 878/10000..  Training Loss: 0.205.. \n",
      "Epoch: 879/10000..  Training Loss: 0.205.. \n",
      "Epoch: 880/10000..  Training Loss: 0.206.. \n",
      "Epoch: 881/10000..  Training Loss: 0.207.. \n",
      "Epoch: 882/10000..  Training Loss: 0.208.. \n",
      "Epoch: 883/10000..  Training Loss: 0.211.. \n",
      "Epoch: 884/10000..  Training Loss: 0.212.. \n",
      "Epoch: 885/10000..  Training Loss: 0.213.. \n",
      "Epoch: 886/10000..  Training Loss: 0.211.. \n",
      "Epoch: 887/10000..  Training Loss: 0.209.. \n",
      "Epoch: 888/10000..  Training Loss: 0.206.. \n",
      "Epoch: 889/10000..  Training Loss: 0.205.. \n",
      "Epoch: 890/10000..  Training Loss: 0.205.. \n",
      "Epoch: 891/10000..  Training Loss: 0.206.. \n",
      "Epoch: 892/10000..  Training Loss: 0.207.. \n",
      "Epoch: 893/10000..  Training Loss: 0.208.. \n",
      "Epoch: 894/10000..  Training Loss: 0.211.. \n",
      "Epoch: 895/10000..  Training Loss: 0.211.. \n",
      "Epoch: 896/10000..  Training Loss: 0.211.. \n",
      "Epoch: 897/10000..  Training Loss: 0.209.. \n",
      "Epoch: 898/10000..  Training Loss: 0.207.. \n",
      "Epoch: 899/10000..  Training Loss: 0.205.. \n",
      "Epoch: 900/10000..  Training Loss: 0.204.. \n",
      "Epoch: 901/10000..  Training Loss: 0.204.. \n",
      "Epoch: 902/10000..  Training Loss: 0.205.. \n",
      "Epoch: 903/10000..  Training Loss: 0.207.. \n",
      "Epoch: 904/10000..  Training Loss: 0.208.. \n",
      "Epoch: 905/10000..  Training Loss: 0.210.. \n",
      "Epoch: 906/10000..  Training Loss: 0.210.. \n",
      "Epoch: 907/10000..  Training Loss: 0.210.. \n",
      "Epoch: 908/10000..  Training Loss: 0.208.. \n",
      "Epoch: 909/10000..  Training Loss: 0.206.. \n",
      "Epoch: 910/10000..  Training Loss: 0.204.. \n",
      "Epoch: 911/10000..  Training Loss: 0.204.. \n",
      "Epoch: 912/10000..  Training Loss: 0.204.. \n",
      "Epoch: 913/10000..  Training Loss: 0.205.. \n",
      "Epoch: 914/10000..  Training Loss: 0.207.. \n",
      "Epoch: 915/10000..  Training Loss: 0.208.. \n",
      "Epoch: 916/10000..  Training Loss: 0.211.. \n",
      "Epoch: 917/10000..  Training Loss: 0.210.. \n",
      "Epoch: 918/10000..  Training Loss: 0.209.. \n",
      "Epoch: 919/10000..  Training Loss: 0.207.. \n",
      "Epoch: 920/10000..  Training Loss: 0.205.. \n",
      "Epoch: 921/10000..  Training Loss: 0.204.. \n",
      "Epoch: 922/10000..  Training Loss: 0.204.. \n",
      "Epoch: 923/10000..  Training Loss: 0.204.. \n",
      "Epoch: 924/10000..  Training Loss: 0.206.. \n",
      "Epoch: 925/10000..  Training Loss: 0.207.. \n",
      "Epoch: 926/10000..  Training Loss: 0.208.. \n",
      "Epoch: 927/10000..  Training Loss: 0.208.. \n",
      "Epoch: 928/10000..  Training Loss: 0.207.. \n",
      "Epoch: 929/10000..  Training Loss: 0.206.. \n",
      "Epoch: 930/10000..  Training Loss: 0.205.. \n",
      "Epoch: 931/10000..  Training Loss: 0.204.. \n",
      "Epoch: 932/10000..  Training Loss: 0.203.. \n",
      "Epoch: 933/10000..  Training Loss: 0.203.. \n",
      "Epoch: 934/10000..  Training Loss: 0.203.. \n",
      "Epoch: 935/10000..  Training Loss: 0.203.. \n",
      "Epoch: 936/10000..  Training Loss: 0.204.. \n",
      "Epoch: 937/10000..  Training Loss: 0.204.. \n",
      "Epoch: 938/10000..  Training Loss: 0.206.. \n",
      "Epoch: 939/10000..  Training Loss: 0.207.. \n",
      "Epoch: 940/10000..  Training Loss: 0.208.. \n",
      "Epoch: 941/10000..  Training Loss: 0.208.. \n",
      "Epoch: 942/10000..  Training Loss: 0.208.. \n",
      "Epoch: 943/10000..  Training Loss: 0.207.. \n",
      "Epoch: 944/10000..  Training Loss: 0.205.. \n",
      "Epoch: 945/10000..  Training Loss: 0.204.. \n",
      "Epoch: 946/10000..  Training Loss: 0.203.. \n",
      "Epoch: 947/10000..  Training Loss: 0.203.. \n",
      "Epoch: 948/10000..  Training Loss: 0.203.. \n",
      "Epoch: 949/10000..  Training Loss: 0.203.. \n",
      "Epoch: 950/10000..  Training Loss: 0.204.. \n",
      "Epoch: 951/10000..  Training Loss: 0.206.. \n",
      "Epoch: 952/10000..  Training Loss: 0.207.. \n",
      "Epoch: 953/10000..  Training Loss: 0.209.. \n",
      "Epoch: 954/10000..  Training Loss: 0.208.. \n",
      "Epoch: 955/10000..  Training Loss: 0.208.. \n",
      "Epoch: 956/10000..  Training Loss: 0.206.. \n",
      "Epoch: 957/10000..  Training Loss: 0.204.. \n",
      "Epoch: 958/10000..  Training Loss: 0.203.. \n",
      "Epoch: 959/10000..  Training Loss: 0.202.. \n",
      "Epoch: 960/10000..  Training Loss: 0.202.. \n",
      "Epoch: 961/10000..  Training Loss: 0.203.. \n",
      "Epoch: 962/10000..  Training Loss: 0.203.. \n",
      "Epoch: 963/10000..  Training Loss: 0.204.. \n",
      "Epoch: 964/10000..  Training Loss: 0.205.. \n",
      "Epoch: 965/10000..  Training Loss: 0.206.. \n",
      "Epoch: 966/10000..  Training Loss: 0.209.. \n",
      "Epoch: 967/10000..  Training Loss: 0.208.. \n",
      "Epoch: 968/10000..  Training Loss: 0.208.. \n",
      "Epoch: 969/10000..  Training Loss: 0.206.. \n",
      "Epoch: 970/10000..  Training Loss: 0.204.. \n",
      "Epoch: 971/10000..  Training Loss: 0.202.. \n",
      "Epoch: 972/10000..  Training Loss: 0.202.. \n",
      "Epoch: 973/10000..  Training Loss: 0.202.. \n",
      "Epoch: 974/10000..  Training Loss: 0.203.. \n",
      "Epoch: 975/10000..  Training Loss: 0.203.. \n",
      "Epoch: 976/10000..  Training Loss: 0.204.. \n",
      "Epoch: 977/10000..  Training Loss: 0.206.. \n",
      "Epoch: 978/10000..  Training Loss: 0.206.. \n",
      "Epoch: 979/10000..  Training Loss: 0.207.. \n",
      "Epoch: 980/10000..  Training Loss: 0.205.. \n",
      "Epoch: 981/10000..  Training Loss: 0.205.. \n",
      "Epoch: 982/10000..  Training Loss: 0.203.. \n",
      "Epoch: 983/10000..  Training Loss: 0.202.. \n",
      "Epoch: 984/10000..  Training Loss: 0.201.. \n",
      "Epoch: 985/10000..  Training Loss: 0.202.. \n",
      "Epoch: 986/10000..  Training Loss: 0.202.. \n",
      "Epoch: 987/10000..  Training Loss: 0.202.. \n",
      "Epoch: 988/10000..  Training Loss: 0.202.. \n",
      "Epoch: 989/10000..  Training Loss: 0.203.. \n",
      "Epoch: 990/10000..  Training Loss: 0.205.. \n",
      "Epoch: 991/10000..  Training Loss: 0.206.. \n",
      "Epoch: 992/10000..  Training Loss: 0.207.. \n",
      "Epoch: 993/10000..  Training Loss: 0.207.. \n",
      "Epoch: 994/10000..  Training Loss: 0.207.. \n",
      "Epoch: 995/10000..  Training Loss: 0.205.. \n",
      "Epoch: 996/10000..  Training Loss: 0.203.. \n",
      "Epoch: 997/10000..  Training Loss: 0.202.. \n",
      "Epoch: 998/10000..  Training Loss: 0.202.. \n",
      "Epoch: 999/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1000/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1001/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1002/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1003/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1004/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1005/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1006/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1007/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1008/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1009/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1010/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1011/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1012/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1013/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1014/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1015/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1016/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1017/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1018/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1019/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1020/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1021/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1022/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1023/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1024/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1025/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1026/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1027/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1028/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1029/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1030/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1031/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1032/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1033/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1034/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1035/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1036/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1037/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1038/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1039/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1040/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1041/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1042/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1043/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1044/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1045/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1046/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1047/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1048/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1049/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1050/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1051/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1052/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1053/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1054/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1055/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1056/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1057/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1058/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1059/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1060/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1061/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1062/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1063/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1064/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1065/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1066/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1067/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1068/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1069/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1070/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1071/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1072/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1073/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1074/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1075/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1076/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1077/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1078/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1079/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1080/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1081/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1082/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1083/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1084/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1085/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1086/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1087/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1088/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1089/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1090/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1091/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1092/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1093/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1094/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1095/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1096/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1097/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1098/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1099/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1100/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1101/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1102/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1103/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1104/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1105/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1106/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1107/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1108/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1109/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1110/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1111/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1112/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1113/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1114/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1115/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1116/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1117/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1118/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1119/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1120/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1121/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1122/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1123/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1124/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1125/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1126/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1127/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1128/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1129/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1130/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1131/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1132/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1133/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1134/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1135/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1136/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1137/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1138/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1139/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1140/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1141/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1142/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1143/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1144/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1145/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1146/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1147/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1148/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1149/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1150/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1151/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1152/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1153/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1154/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1155/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1156/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1157/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1158/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1159/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1160/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1161/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1162/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1163/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1164/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1165/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1166/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1167/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1168/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1169/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1170/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1171/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1172/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1173/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1174/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1175/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1176/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1177/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1178/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1179/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1180/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1181/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1182/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1183/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1184/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1185/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1186/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1187/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1188/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1189/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1190/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1191/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1192/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1193/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1194/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1195/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1196/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1197/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1198/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1199/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1200/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1201/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1202/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1203/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1204/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1205/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1206/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1207/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1208/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1209/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1210/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1211/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1212/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1213/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1214/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1215/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1216/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1217/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1218/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1219/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1220/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1221/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1222/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1223/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1224/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1225/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1226/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1227/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1228/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1229/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1230/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1231/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1232/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1233/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1234/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1235/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1236/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1237/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1238/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1239/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1240/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1241/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1242/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1243/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1244/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1245/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1246/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1247/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1248/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1249/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1250/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1251/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1252/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1253/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1254/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1255/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1256/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1257/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1258/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1259/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1260/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1261/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1262/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1263/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1264/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1265/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1266/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1267/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1268/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1269/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1270/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1271/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1272/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1273/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1274/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1275/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1276/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1277/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1278/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1279/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1280/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1281/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1282/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1283/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1284/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1285/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1286/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1287/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1288/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1289/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1290/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1291/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1292/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1293/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1294/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1295/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1296/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1297/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1298/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1299/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1300/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1301/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1302/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1303/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1304/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1305/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1306/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1307/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1308/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1309/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1310/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1311/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1312/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1313/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1314/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1315/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1316/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1317/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1318/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1319/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1320/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1321/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1322/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1323/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1324/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1325/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1326/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1327/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1328/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1329/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1330/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1331/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1332/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1333/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1334/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1335/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1336/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1337/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1338/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1339/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1340/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1341/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1342/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1343/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1344/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1345/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1346/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1347/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1348/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1349/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1350/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1351/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1352/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1353/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1354/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1355/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1356/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1357/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1358/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1359/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1360/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1361/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1362/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1363/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1364/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1365/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1366/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1367/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1368/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1369/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1370/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1371/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1372/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1373/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1374/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1375/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1376/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1377/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1378/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1379/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1380/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1381/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1382/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1383/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1384/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1385/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1386/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1387/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1388/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1389/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1390/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1391/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1392/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1393/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1394/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1395/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1396/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1397/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1398/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1399/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1400/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1401/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1402/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1403/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1404/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1405/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1406/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1407/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1408/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1409/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1410/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1411/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1412/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1413/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1414/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1415/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1416/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1417/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1418/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1419/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1420/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1421/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1422/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1423/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1424/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1425/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1426/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1427/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1428/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1429/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1430/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1431/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1432/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1433/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1434/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1435/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1436/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1437/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1438/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1439/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1440/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1441/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1442/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1443/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1444/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1445/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1446/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1447/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1448/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1449/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1450/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1451/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1452/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1453/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1454/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1455/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1456/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1457/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1458/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1459/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1460/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1461/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1462/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1463/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1464/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1465/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1466/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1467/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1468/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1469/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1470/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1471/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1472/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1473/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1474/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1475/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1476/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1477/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1478/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1479/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1480/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1481/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1482/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1483/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1484/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1485/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1486/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1487/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1488/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1489/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1490/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1491/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1492/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1493/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1494/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1495/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1496/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1497/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1498/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1499/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1500/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1501/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1502/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1503/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1504/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1505/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1506/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1507/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1508/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1509/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1510/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1511/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1512/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1513/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1514/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1515/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1516/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1517/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1518/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1519/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1520/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1521/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1522/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1523/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1524/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1525/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1526/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1527/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1528/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1529/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1530/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1531/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1532/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1533/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1534/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1535/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1536/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1537/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1538/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1539/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1540/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1541/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1542/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1543/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1544/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1545/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1546/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1547/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1548/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1549/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1550/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1551/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1552/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1553/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1554/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1555/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1556/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1557/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1558/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1559/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1560/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1561/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1562/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1563/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1564/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1565/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1566/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1567/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1568/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1569/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1570/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1571/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1572/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1573/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1574/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1575/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1576/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1577/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1578/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1579/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1580/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1581/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1582/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1583/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1584/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1585/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1586/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1587/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1588/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1589/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1590/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1591/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1592/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1593/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1594/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1595/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1596/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1597/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1598/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1599/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1600/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1601/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1602/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1603/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1604/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1605/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1606/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1607/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1608/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1609/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1610/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1611/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1612/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1613/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1614/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1615/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1616/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1617/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1618/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1619/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1620/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1621/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1622/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1623/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1624/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1625/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1626/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1627/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1628/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1629/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1630/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1631/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1632/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1633/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1634/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1635/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1636/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1637/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1638/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1639/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1640/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1641/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1642/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1643/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1644/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1645/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1646/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1647/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1648/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1649/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1650/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1651/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1652/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1653/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1654/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1655/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1656/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1657/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1658/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1659/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1660/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1661/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1662/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1663/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1664/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1665/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1666/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1667/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1668/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1669/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1670/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1671/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1672/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1673/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1674/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1675/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1676/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1677/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1678/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1679/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1680/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1681/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1682/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1683/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1684/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1685/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1686/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1687/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1688/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1689/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1690/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1691/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1692/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1693/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1694/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1695/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1696/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1697/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1698/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1699/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1700/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1701/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1702/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1703/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1704/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1705/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1706/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1707/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1708/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1709/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1710/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1711/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1712/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1713/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1714/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1715/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1716/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1717/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1718/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1719/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1720/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1721/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1722/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1723/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1724/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1725/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1726/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1727/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1728/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1729/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1730/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1731/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1732/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1733/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1734/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1735/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1736/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1737/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1738/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1739/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1740/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1741/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1742/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1743/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1744/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1745/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1746/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1747/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1748/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1749/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1750/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1751/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1752/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1753/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1754/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1755/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1756/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1757/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1758/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1759/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1760/10000..  Training Loss: 0.402.. \n",
      "Epoch: 1761/10000..  Training Loss: 1.259.. \n",
      "Epoch: 1762/10000..  Training Loss: 1.737.. \n",
      "Epoch: 1763/10000..  Training Loss: 1.708.. \n",
      "Epoch: 1764/10000..  Training Loss: 0.967.. \n",
      "Epoch: 1765/10000..  Training Loss: 1.075.. \n",
      "Epoch: 1766/10000..  Training Loss: 0.887.. \n",
      "Epoch: 1767/10000..  Training Loss: 0.705.. \n",
      "Epoch: 1768/10000..  Training Loss: 0.726.. \n",
      "Epoch: 1769/10000..  Training Loss: 0.749.. \n",
      "Epoch: 1770/10000..  Training Loss: 0.829.. \n",
      "Epoch: 1771/10000..  Training Loss: 0.640.. \n",
      "Epoch: 1772/10000..  Training Loss: 0.518.. \n",
      "Epoch: 1773/10000..  Training Loss: 0.465.. \n",
      "Epoch: 1774/10000..  Training Loss: 0.449.. \n",
      "Epoch: 1775/10000..  Training Loss: 0.474.. \n",
      "Epoch: 1776/10000..  Training Loss: 0.363.. \n",
      "Epoch: 1777/10000..  Training Loss: 0.377.. \n",
      "Epoch: 1778/10000..  Training Loss: 0.403.. \n",
      "Epoch: 1779/10000..  Training Loss: 0.372.. \n",
      "Epoch: 1780/10000..  Training Loss: 0.360.. \n",
      "Epoch: 1781/10000..  Training Loss: 0.360.. \n",
      "Epoch: 1782/10000..  Training Loss: 0.327.. \n",
      "Epoch: 1783/10000..  Training Loss: 0.280.. \n",
      "Epoch: 1784/10000..  Training Loss: 0.271.. \n",
      "Epoch: 1785/10000..  Training Loss: 0.290.. \n",
      "Epoch: 1786/10000..  Training Loss: 0.288.. \n",
      "Epoch: 1787/10000..  Training Loss: 0.278.. \n",
      "Epoch: 1788/10000..  Training Loss: 0.275.. \n",
      "Epoch: 1789/10000..  Training Loss: 0.272.. \n",
      "Epoch: 1790/10000..  Training Loss: 0.262.. \n",
      "Epoch: 1791/10000..  Training Loss: 0.251.. \n",
      "Epoch: 1792/10000..  Training Loss: 0.246.. \n",
      "Epoch: 1793/10000..  Training Loss: 0.241.. \n",
      "Epoch: 1794/10000..  Training Loss: 0.240.. \n",
      "Epoch: 1795/10000..  Training Loss: 0.239.. \n",
      "Epoch: 1796/10000..  Training Loss: 0.237.. \n",
      "Epoch: 1797/10000..  Training Loss: 0.244.. \n",
      "Epoch: 1798/10000..  Training Loss: 0.237.. \n",
      "Epoch: 1799/10000..  Training Loss: 0.231.. \n",
      "Epoch: 1800/10000..  Training Loss: 0.227.. \n",
      "Epoch: 1801/10000..  Training Loss: 0.231.. \n",
      "Epoch: 1802/10000..  Training Loss: 0.228.. \n",
      "Epoch: 1803/10000..  Training Loss: 0.224.. \n",
      "Epoch: 1804/10000..  Training Loss: 0.225.. \n",
      "Epoch: 1805/10000..  Training Loss: 0.223.. \n",
      "Epoch: 1806/10000..  Training Loss: 0.220.. \n",
      "Epoch: 1807/10000..  Training Loss: 0.220.. \n",
      "Epoch: 1808/10000..  Training Loss: 0.220.. \n",
      "Epoch: 1809/10000..  Training Loss: 0.219.. \n",
      "Epoch: 1810/10000..  Training Loss: 0.219.. \n",
      "Epoch: 1811/10000..  Training Loss: 0.219.. \n",
      "Epoch: 1812/10000..  Training Loss: 0.219.. \n",
      "Epoch: 1813/10000..  Training Loss: 0.217.. \n",
      "Epoch: 1814/10000..  Training Loss: 0.215.. \n",
      "Epoch: 1815/10000..  Training Loss: 0.214.. \n",
      "Epoch: 1816/10000..  Training Loss: 0.215.. \n",
      "Epoch: 1817/10000..  Training Loss: 0.214.. \n",
      "Epoch: 1818/10000..  Training Loss: 0.213.. \n",
      "Epoch: 1819/10000..  Training Loss: 0.213.. \n",
      "Epoch: 1820/10000..  Training Loss: 0.213.. \n",
      "Epoch: 1821/10000..  Training Loss: 0.212.. \n",
      "Epoch: 1822/10000..  Training Loss: 0.212.. \n",
      "Epoch: 1823/10000..  Training Loss: 0.212.. \n",
      "Epoch: 1824/10000..  Training Loss: 0.211.. \n",
      "Epoch: 1825/10000..  Training Loss: 0.211.. \n",
      "Epoch: 1826/10000..  Training Loss: 0.211.. \n",
      "Epoch: 1827/10000..  Training Loss: 0.211.. \n",
      "Epoch: 1828/10000..  Training Loss: 0.211.. \n",
      "Epoch: 1829/10000..  Training Loss: 0.210.. \n",
      "Epoch: 1830/10000..  Training Loss: 0.210.. \n",
      "Epoch: 1831/10000..  Training Loss: 0.210.. \n",
      "Epoch: 1832/10000..  Training Loss: 0.210.. \n",
      "Epoch: 1833/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1834/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1835/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1836/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1837/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1838/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1839/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1840/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1841/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1842/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1843/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1844/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1845/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1846/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1847/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1848/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1849/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1850/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1851/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1852/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1853/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1854/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1855/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1856/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1857/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1858/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1859/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1860/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1861/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1862/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1863/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1864/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1865/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1866/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1867/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1868/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1869/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1870/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1871/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1872/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1873/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1874/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1875/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1876/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1877/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1878/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1879/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1880/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1881/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1882/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1883/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1884/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1885/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1886/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1887/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1888/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1889/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1890/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1891/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1892/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1893/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1894/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1895/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1896/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1897/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1898/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1899/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1900/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1901/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1902/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1903/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1904/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1905/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1906/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1907/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1908/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1909/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1910/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1911/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1912/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1913/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1914/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1915/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1916/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1917/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1918/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1919/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1920/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1921/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1922/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1923/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1924/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1925/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1926/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1927/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1928/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1929/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1930/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1931/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1932/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1933/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1934/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1935/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1936/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1937/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1938/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1939/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1940/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1941/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1942/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1943/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1944/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1945/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1946/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1947/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1948/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1949/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1950/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1951/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1952/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1953/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1954/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1955/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1956/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1957/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1958/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1959/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1960/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1961/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1962/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1963/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1964/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1965/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1966/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1967/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1968/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1969/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1970/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1971/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1972/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1973/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1974/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1975/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1976/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1977/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1978/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1979/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1980/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1981/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1982/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1983/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1984/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1985/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1986/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1987/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1988/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1989/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1990/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1991/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1992/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1993/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1994/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1995/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1996/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1997/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1998/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1999/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2000/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2001/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2002/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2003/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2004/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2005/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2006/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2007/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2008/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2009/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2010/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2011/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2012/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2013/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2014/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2015/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2016/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2017/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2018/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2019/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2020/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2021/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2022/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2023/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2024/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2025/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2026/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2027/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2028/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2029/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2030/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2031/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2032/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2033/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2034/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2035/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2036/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2037/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2038/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2039/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2040/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2041/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2042/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2043/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2044/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2045/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2046/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2047/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2048/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2049/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2050/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2051/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2052/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2053/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2054/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2055/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2056/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2057/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2058/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2059/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2060/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2061/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2062/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2063/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2064/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2065/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2066/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2067/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2068/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2069/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2070/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2071/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2072/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2073/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2074/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2075/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2076/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2077/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2078/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2079/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2080/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2081/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2082/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2083/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2084/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2085/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2086/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2087/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2088/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2089/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2090/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2091/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2092/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2093/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2094/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2095/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2096/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2097/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2098/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2099/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2100/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2101/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2102/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2103/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2104/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2105/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2106/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2107/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2108/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2109/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2110/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2111/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2112/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2113/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2114/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2115/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2116/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2117/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2118/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2119/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2120/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2121/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2122/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2123/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2124/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2125/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2126/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2127/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2128/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2129/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2130/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2131/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2132/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2133/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2134/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2135/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2136/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2137/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2138/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2139/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2140/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2141/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2142/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2143/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2144/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2145/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2146/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2147/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2148/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2149/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2150/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2151/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2152/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2153/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2154/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2155/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2156/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2157/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2158/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2159/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2160/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2161/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2162/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2163/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2164/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2165/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2166/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2167/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2168/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2169/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2170/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2171/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2172/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2173/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2174/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2175/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2176/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2177/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2178/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2179/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2180/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2181/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2182/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2183/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2184/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2185/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2186/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2187/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2188/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2189/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2190/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2191/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2192/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2193/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2194/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2195/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2196/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2197/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2198/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2199/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2200/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2201/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2202/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2203/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2204/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2205/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2206/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2207/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2208/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2209/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2210/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2211/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2212/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2213/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2214/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2215/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2216/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2217/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2218/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2219/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2220/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2221/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2222/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2223/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2224/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2225/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2226/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2227/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2228/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2229/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2230/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2231/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2232/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2233/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2234/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2235/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2236/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2237/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2238/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2239/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2240/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2241/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2242/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2243/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2244/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2245/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2246/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2247/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2248/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2249/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2250/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2251/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2252/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2253/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2254/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2255/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2256/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2257/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2258/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2259/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2260/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2261/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2262/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2263/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2264/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2265/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2266/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2267/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2268/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2269/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2270/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2271/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2272/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2273/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2274/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2275/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2276/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2277/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2278/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2279/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2280/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2281/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2282/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2283/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2284/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2285/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2286/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2287/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2288/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2289/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2290/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2291/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2292/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2293/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2294/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2295/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2296/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2297/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2298/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2299/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2300/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2301/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2302/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2303/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2304/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2305/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2306/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2307/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2308/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2309/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2310/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2311/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2312/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2313/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2314/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2315/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2316/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2317/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2318/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2319/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2320/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2321/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2322/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2323/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2324/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2325/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2326/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2327/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2328/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2329/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2330/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2331/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2332/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2333/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2334/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2335/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2336/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2337/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2338/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2339/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2340/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2341/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2342/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2343/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2344/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2345/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2346/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2347/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2348/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2349/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2350/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2351/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2352/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2353/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2354/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2355/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2356/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2357/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2358/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2359/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2360/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2361/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2362/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2363/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2364/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2365/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2366/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2367/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2368/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2369/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2370/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2371/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2372/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2373/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2374/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2375/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2376/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2377/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2378/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2379/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2380/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2381/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2382/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2383/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2384/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2385/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2386/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2387/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2388/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2389/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2390/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2391/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2392/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2393/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2394/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2395/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2396/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2397/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2398/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2399/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2400/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2401/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2402/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2403/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2404/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2405/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2406/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2407/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2408/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2409/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2410/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2411/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2412/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2413/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2414/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2415/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2416/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2417/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2418/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2419/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2420/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2421/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2422/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2423/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2424/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2425/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2426/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2427/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2428/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2429/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2430/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2431/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2432/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2433/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2434/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2435/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2436/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2437/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2438/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2439/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2440/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2441/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2442/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2443/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2444/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2445/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2446/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2447/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2448/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2449/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2450/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2451/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2452/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2453/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2454/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2455/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2456/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2457/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2458/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2459/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2460/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2461/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2462/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2463/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2464/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2465/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2466/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2467/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2468/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2469/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2470/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2471/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2472/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2473/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2474/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2475/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2476/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2477/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2478/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2479/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2480/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2481/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2482/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2483/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2484/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2485/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2486/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2487/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2488/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2489/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2490/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2491/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2492/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2493/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2494/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2495/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2496/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2497/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2498/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2499/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2500/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2501/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2502/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2503/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2504/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2505/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2506/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2507/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2508/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2509/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2510/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2511/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2512/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2513/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2514/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2515/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2516/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2517/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2518/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2519/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2520/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2521/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2522/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2523/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2524/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2525/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2526/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2527/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2528/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2529/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2530/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2531/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2532/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2533/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2534/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2535/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2536/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2537/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2538/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2539/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2540/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2541/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2542/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2543/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2544/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2545/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2546/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2547/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2548/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2549/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2550/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2551/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2552/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2553/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2554/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2555/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2556/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2557/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2558/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2559/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2560/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2561/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2562/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2563/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2564/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2565/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2566/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2567/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2568/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2569/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2570/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2571/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2572/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2573/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2574/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2575/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2576/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2577/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2578/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2579/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2580/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2581/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2582/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2583/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2584/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2585/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2586/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2587/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2588/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2589/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2590/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2591/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2592/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2593/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2594/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2595/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2596/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2597/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2598/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2599/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2600/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2601/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2602/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2603/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2604/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2605/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2606/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2607/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2608/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2609/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2610/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2611/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2612/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2613/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2614/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2615/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2616/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2617/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2618/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2619/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2620/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2621/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2622/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2623/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2624/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2625/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2626/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2627/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2628/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2629/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2630/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2631/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2632/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2633/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2634/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2635/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2636/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2637/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2638/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2639/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2640/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2641/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2642/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2643/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2644/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2645/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2646/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2647/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2648/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2649/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2650/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2651/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2652/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2653/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2654/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2655/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2656/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2657/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2658/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2659/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2660/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2661/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2662/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2663/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2664/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2665/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2666/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2667/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2668/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2669/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2670/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2671/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2672/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2673/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2674/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2675/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2676/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2677/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2678/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2679/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2680/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2681/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2682/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2683/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2684/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2685/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2686/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2687/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2688/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2689/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2690/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2691/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2692/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2693/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2694/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2695/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2696/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2697/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2698/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2699/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2700/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2701/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2702/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2703/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2704/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2705/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2706/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2707/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2708/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2709/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2710/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2711/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2712/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2713/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2714/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2715/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2716/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2717/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2718/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2719/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2720/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2721/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2722/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2723/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2724/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2725/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2726/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2727/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2728/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2729/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2730/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2731/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2732/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2733/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2734/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2735/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2736/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2737/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2738/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2739/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2740/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2741/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2742/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2743/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2744/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2745/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2746/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2747/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2748/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2749/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2750/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2751/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2752/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2753/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2754/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2755/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2756/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2757/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2758/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2759/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2760/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2761/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2762/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2763/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2764/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2765/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2766/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2767/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2768/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2769/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2770/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2771/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2772/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2773/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2774/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2775/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2776/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2777/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2778/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2779/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2780/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2781/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2782/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2783/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2784/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2785/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2786/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2787/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2788/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2789/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2790/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2791/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2792/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2793/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2794/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2795/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2796/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2797/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2798/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2799/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2800/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2801/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2802/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2803/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2804/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2805/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2806/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2807/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2808/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2809/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2810/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2811/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2812/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2813/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2814/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2815/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2816/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2817/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2818/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2819/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2820/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2821/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2822/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2823/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2824/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2825/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2826/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2827/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2828/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2829/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2830/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2831/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2832/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2833/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2834/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2835/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2836/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2837/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2838/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2839/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2840/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2841/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2842/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2843/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2844/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2845/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2846/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2847/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2848/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2849/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2850/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2851/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2852/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2853/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2854/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2855/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2856/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2857/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2858/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2859/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2860/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2861/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2862/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2863/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2864/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2865/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2866/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2867/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2868/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2869/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2870/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2871/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2872/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2873/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2874/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2875/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2876/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2877/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2878/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2879/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2880/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2881/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2882/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2883/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2884/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2885/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2886/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2887/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2888/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2889/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2890/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2891/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2892/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2893/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2894/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2895/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2896/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2897/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2898/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2899/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2900/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2901/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2902/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2903/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2904/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2905/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2906/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2907/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2908/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2909/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2910/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2911/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2912/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2913/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2914/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2915/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2916/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2917/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2918/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2919/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2920/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2921/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2922/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2923/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2924/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2925/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2926/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2927/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2928/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2929/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2930/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2931/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2932/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2933/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2934/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2935/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2936/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2937/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2938/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2939/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2940/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2941/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2942/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2943/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2944/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2945/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2946/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2947/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2948/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2949/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2950/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2951/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2952/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2953/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2954/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2955/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2956/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2957/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2958/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2959/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2960/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2961/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2962/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2963/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2964/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2965/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2966/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2967/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2968/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2969/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2970/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2971/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2972/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2973/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2974/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2975/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2976/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2977/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2978/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2979/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2980/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2981/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2982/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2983/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2984/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2985/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2986/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2987/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2988/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2989/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2990/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2991/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2992/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2993/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2994/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2995/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2996/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2997/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2998/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2999/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3000/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3001/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3002/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3003/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3004/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3005/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3006/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3007/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3008/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3009/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3010/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3011/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3012/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3013/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3014/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3015/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3016/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3017/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3018/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3019/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3020/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3021/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3022/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3023/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3024/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3025/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3026/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3027/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3028/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3029/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3030/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3031/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3032/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3033/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3034/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3035/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3036/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3037/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3038/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3039/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3040/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3041/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3042/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3043/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3044/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3045/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3046/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3047/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3048/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3049/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3050/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3051/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3052/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3053/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3054/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3055/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3056/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3057/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3058/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3059/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3060/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3061/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3062/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3063/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3064/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3065/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3066/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3067/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3068/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3069/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3070/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3071/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3072/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3073/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3074/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3075/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3076/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3077/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3078/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3079/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3080/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3081/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3082/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3083/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3084/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3085/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3086/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3087/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3088/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3089/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3090/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3091/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3092/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3093/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3094/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3095/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3096/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3097/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3098/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3099/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3100/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3101/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3102/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3103/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3104/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3105/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3106/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3107/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3108/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3109/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3110/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3111/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3112/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3113/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3114/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3115/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3116/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3117/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3118/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3119/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3120/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3121/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3122/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3123/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3124/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3125/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3126/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3127/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3128/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3129/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3130/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3131/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3132/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3133/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3134/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3135/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3136/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3137/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3138/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3139/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3140/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3141/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3142/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3143/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3144/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3145/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3146/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3147/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3148/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3149/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3150/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3151/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3152/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3153/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3154/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3155/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3156/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3157/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3158/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3159/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3160/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3161/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3162/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3163/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3164/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3165/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3166/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3167/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3168/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3169/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3170/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3171/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3172/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3173/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3174/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3175/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3176/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3177/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3178/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3179/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3180/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3181/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3182/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3183/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3184/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3185/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3186/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3187/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3188/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3189/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3190/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3191/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3192/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3193/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3194/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3195/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3196/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3197/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3198/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3199/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3200/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3201/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3202/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3203/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3204/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3205/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3206/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3207/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3208/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3209/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3210/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3211/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3212/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3213/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3214/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3215/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3216/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3217/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3218/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3219/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3220/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3221/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3222/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3223/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3224/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3225/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3226/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3227/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3228/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3229/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3230/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3231/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3232/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3233/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3234/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3235/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3236/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3237/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3238/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3239/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3240/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3241/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3242/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3243/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3244/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3245/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3246/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3247/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3248/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3249/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3250/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3251/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3252/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3253/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3254/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3255/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3256/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3257/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3258/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3259/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3260/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3261/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3262/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3263/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3264/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3265/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3266/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3267/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3268/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3269/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3270/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3271/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3272/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3273/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3274/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3275/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3276/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3277/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3278/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3279/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3280/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3281/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3282/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3283/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3284/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3285/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3286/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3287/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3288/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3289/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3290/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3291/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3292/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3293/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3294/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3295/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3296/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3297/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3298/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3299/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3300/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3301/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3302/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3303/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3304/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3305/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3306/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3307/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3308/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3309/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3310/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3311/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3312/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3313/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3314/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3315/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3316/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3317/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3318/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3319/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3320/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3321/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3322/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3323/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3324/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3325/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3326/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3327/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3328/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3329/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3330/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3331/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3332/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3333/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3334/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3335/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3336/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3337/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3338/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3339/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3340/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3341/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3342/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3343/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3344/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3345/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3346/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3347/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3348/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3349/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3350/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3351/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3352/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3353/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3354/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3355/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3356/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3357/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3358/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3359/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3360/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3361/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3362/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3363/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3364/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3365/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3366/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3367/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3368/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3369/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3370/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3371/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3372/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3373/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3374/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3375/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3376/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3377/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3378/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3379/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3380/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3381/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3382/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3383/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3384/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3385/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3386/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3387/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3388/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3389/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3390/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3391/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3392/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3393/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3394/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3395/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3396/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3397/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3398/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3399/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3400/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3401/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3402/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3403/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3404/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3405/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3406/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3407/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3408/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3409/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3410/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3411/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3412/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3413/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3414/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3415/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3416/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3417/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3418/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3419/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3420/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3421/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3422/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3423/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3424/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3425/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3426/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3427/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3428/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3429/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3430/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3431/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3432/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3433/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3434/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3435/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3436/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3437/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3438/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3439/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3440/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3441/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3442/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3443/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3444/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3445/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3446/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3447/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3448/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3449/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3450/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3451/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3452/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3453/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3454/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3455/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3456/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3457/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3458/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3459/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3460/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3461/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3462/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3463/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3464/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3465/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3466/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3467/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3468/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3469/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3470/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3471/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3472/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3473/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3474/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3475/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3476/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3477/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3478/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3479/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3480/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3481/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3482/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3483/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3484/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3485/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3486/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3487/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3488/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3489/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3490/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3491/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3492/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3493/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3494/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3495/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3496/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3497/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3498/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3499/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3500/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3501/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3502/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3503/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3504/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3505/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3506/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3507/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3508/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3509/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3510/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3511/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3512/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3513/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3514/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3515/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3516/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3517/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3518/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3519/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3520/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3521/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3522/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3523/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3524/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3525/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3526/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3527/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3528/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3529/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3530/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3531/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3532/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3533/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3534/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3535/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3536/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3537/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3538/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3539/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3540/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3541/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3542/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3543/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3544/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3545/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3546/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3547/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3548/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3549/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3550/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3551/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3552/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3553/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3554/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3555/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3556/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3557/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3558/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3559/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3560/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3561/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3562/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3563/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3564/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3565/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3566/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3567/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3568/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3569/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3570/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3571/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3572/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3573/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3574/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3575/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3576/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3577/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3578/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3579/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3580/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3581/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3582/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3583/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3584/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3585/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3586/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3587/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3588/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3589/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3590/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3591/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3592/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3593/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3594/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3595/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3596/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3597/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3598/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3599/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3600/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3601/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3602/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3603/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3604/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3605/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3606/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3607/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3608/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3609/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3610/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3611/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3612/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3613/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3614/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3615/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3616/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3617/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3618/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3619/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3620/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3621/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3622/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3623/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3624/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3625/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3626/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3627/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3628/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3629/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3630/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3631/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3632/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3633/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3634/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3635/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3636/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3637/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3638/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3639/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3640/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3641/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3642/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3643/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3644/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3645/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3646/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3647/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3648/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3649/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3650/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3651/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3652/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3653/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3654/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3655/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3656/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3657/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3658/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3659/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3660/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3661/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3662/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3663/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3664/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3665/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3666/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3667/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3668/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3669/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3670/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3671/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3672/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3673/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3674/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3675/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3676/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3677/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3678/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3679/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3680/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3681/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3682/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3683/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3684/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3685/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3686/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3687/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3688/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3689/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3690/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3691/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3692/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3693/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3694/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3695/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3696/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3697/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3698/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3699/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3700/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3701/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3702/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3703/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3704/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3705/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3706/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3707/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3708/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3709/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3710/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3711/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3712/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3713/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3714/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3715/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3716/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3717/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3718/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3719/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3720/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3721/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3722/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3723/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3724/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3725/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3726/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3727/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3728/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3729/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3730/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3731/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3732/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3733/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3734/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3735/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3736/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3737/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3738/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3739/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3740/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3741/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3742/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3743/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3744/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3745/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3746/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3747/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3748/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3749/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3750/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3751/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3752/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3753/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3754/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3755/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3756/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3757/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3758/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3759/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3760/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3761/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3762/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3763/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3764/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3765/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3766/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3767/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3768/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3769/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3770/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3771/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3772/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3773/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3774/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3775/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3776/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3777/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3778/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3779/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3780/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3781/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3782/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3783/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3784/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3785/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3786/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3787/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3788/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3789/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3790/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3791/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3792/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3793/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3794/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3795/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3796/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3797/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3798/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3799/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3800/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3801/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3802/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3803/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3804/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3805/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3806/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3807/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3808/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3809/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3810/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3811/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3812/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3813/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3814/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3815/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3816/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3817/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3818/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3819/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3820/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3821/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3822/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3823/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3824/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3825/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3826/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3827/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3828/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3829/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3830/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3831/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3832/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3833/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3834/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3835/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3836/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3837/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3838/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3839/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3840/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3841/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3842/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3843/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3844/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3845/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3846/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3847/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3848/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3849/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3850/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3851/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3852/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3853/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3854/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3855/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3856/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3857/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3858/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3859/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3860/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3861/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3862/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3863/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3864/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3865/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3866/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3867/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3868/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3869/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3870/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3871/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3872/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3873/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3874/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3875/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3876/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3877/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3878/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3879/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3880/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3881/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3882/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3883/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3884/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3885/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3886/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3887/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3888/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3889/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3890/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3891/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3892/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3893/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3894/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3895/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3896/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3897/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3898/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3899/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3900/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3901/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3902/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3903/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3904/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3905/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3906/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3907/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3908/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3909/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3910/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3911/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3912/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3913/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3914/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3915/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3916/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3917/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3918/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3919/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3920/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3921/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3922/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3923/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3924/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3925/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3926/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3927/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3928/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3929/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3930/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3931/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3932/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3933/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3934/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3935/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3936/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3937/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3938/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3939/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3940/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3941/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3942/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3943/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3944/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3945/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3946/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3947/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3948/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3949/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3950/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3951/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3952/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3953/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3954/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3955/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3956/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3957/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3958/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3959/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3960/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3961/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3962/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3963/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3964/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3965/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3966/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3967/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3968/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3969/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3970/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3971/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3972/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3973/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3974/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3975/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3976/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3977/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3978/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3979/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3980/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3981/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3982/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3983/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3984/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3985/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3986/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3987/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3988/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3989/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3990/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3991/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3992/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3993/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3994/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3995/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3996/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3997/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3998/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3999/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4000/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4001/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4002/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4003/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4004/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4005/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4006/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4007/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4008/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4009/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4010/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4011/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4012/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4013/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4014/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4015/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4016/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4017/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4018/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4019/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4020/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4021/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4022/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4023/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4024/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4025/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4026/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4027/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4028/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4029/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4030/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4031/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4032/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4033/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4034/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4035/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4036/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4037/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4038/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4039/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4040/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4041/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4042/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4043/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4044/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4045/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4046/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4047/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4048/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4049/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4050/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4051/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4052/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4053/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4054/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4055/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4056/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4057/10000..  Training Loss: 0.178.. \n",
      "Epoch: 4058/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4059/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4060/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4061/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4062/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4063/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4064/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4065/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4066/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4067/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4068/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4069/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4070/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4071/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4072/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4073/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4074/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4075/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4076/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4077/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4078/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4079/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4080/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4081/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4082/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4083/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4084/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4085/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4086/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4087/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4088/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4089/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4090/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4091/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4092/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4093/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4094/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4095/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4096/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4097/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4098/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4099/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4100/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4101/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4102/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4103/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4104/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4105/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4106/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4107/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4108/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4109/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4110/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4111/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4112/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4113/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4114/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4115/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4116/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4117/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4118/10000..  Training Loss: 0.178.. \n",
      "Epoch: 4119/10000..  Training Loss: 0.178.. \n",
      "Epoch: 4120/10000..  Training Loss: 0.181.. \n",
      "Epoch: 4121/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4122/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4123/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4124/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4125/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4126/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4127/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4128/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4129/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4130/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4131/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4132/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4133/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4134/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4135/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4136/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4137/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4138/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4139/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4140/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4141/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4142/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4143/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4144/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4145/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4146/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4147/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4148/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4149/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4150/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4151/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4152/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4153/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4154/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4155/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4156/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4157/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4158/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4159/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4160/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4161/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4162/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4163/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4164/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4165/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4166/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4167/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4168/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4169/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4170/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4171/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4172/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4173/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4174/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4175/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4176/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4177/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4178/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4179/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4180/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4181/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4182/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4183/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4184/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4185/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4186/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4187/10000..  Training Loss: 0.180.. \n",
      "Epoch: 4188/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4189/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4190/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4191/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4192/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4193/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4194/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4195/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4196/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4197/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4198/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4199/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4200/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4201/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4202/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4203/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4204/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4205/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4206/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4207/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4208/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4209/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4210/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4211/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4212/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4213/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4214/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4215/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4216/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4217/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4218/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4219/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4220/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4221/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4222/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4223/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4224/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4225/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4226/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4227/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4228/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4229/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4230/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4231/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4232/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4233/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4234/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4235/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4236/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4237/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4238/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4239/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4240/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4241/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4242/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4243/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4244/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4245/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4246/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4247/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4248/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4249/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4250/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4251/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4252/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4253/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4254/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4255/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4256/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4257/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4258/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4259/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4260/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4261/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4262/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4263/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4264/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4265/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4266/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4267/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4268/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4269/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4270/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4271/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4272/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4273/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4274/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4275/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4276/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4277/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4278/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4279/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4280/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4281/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4282/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4283/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4284/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4285/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4286/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4287/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4288/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4289/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4290/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4291/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4292/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4293/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4294/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4295/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4296/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4297/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4298/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4299/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4300/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4301/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4302/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4303/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4304/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4305/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4306/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4307/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4308/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4309/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4310/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4311/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4312/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4313/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4314/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4315/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4316/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4317/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4318/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4319/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4320/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4321/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4322/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4323/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4324/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4325/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4326/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4327/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4328/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4329/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4330/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4331/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4332/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4333/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4334/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4335/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4336/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4337/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4338/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4339/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4340/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4341/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4342/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4343/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4344/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4345/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4346/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4347/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4348/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4349/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4350/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4351/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4352/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4353/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4354/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4355/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4356/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4357/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4358/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4359/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4360/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4361/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4362/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4363/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4364/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4365/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4366/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4367/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4368/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4369/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4370/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4371/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4372/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4373/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4374/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4375/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4376/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4377/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4378/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4379/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4380/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4381/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4382/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4383/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4384/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4385/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4386/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4387/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4388/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4389/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4390/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4391/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4392/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4393/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4394/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4395/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4396/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4397/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4398/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4399/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4400/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4401/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4402/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4403/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4404/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4405/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4406/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4407/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4408/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4409/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4410/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4411/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4412/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4413/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4414/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4415/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4416/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4417/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4418/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4419/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4420/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4421/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4422/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4423/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4424/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4425/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4426/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4427/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4428/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4429/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4430/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4431/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4432/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4433/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4434/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4435/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4436/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4437/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4438/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4439/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4440/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4441/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4442/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4443/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4444/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4445/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4446/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4447/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4448/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4449/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4450/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4451/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4452/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4453/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4454/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4455/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4456/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4457/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4458/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4459/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4460/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4461/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4462/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4463/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4464/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4465/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4466/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4467/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4468/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4469/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4470/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4471/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4472/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4473/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4474/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4475/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4476/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4477/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4478/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4479/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4480/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4481/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4482/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4483/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4484/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4485/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4486/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4487/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4488/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4489/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4490/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4491/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4492/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4493/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4494/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4495/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4496/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4497/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4498/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4499/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4500/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4501/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4502/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4503/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4504/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4505/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4506/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4507/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4508/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4509/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4510/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4511/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4512/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4513/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4514/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4515/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4516/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4517/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4518/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4519/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4520/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4521/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4522/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4523/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4524/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4525/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4526/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4527/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4528/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4529/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4530/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4531/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4532/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4533/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4534/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4535/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4536/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4537/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4538/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4539/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4540/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4541/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4542/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4543/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4544/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4545/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4546/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4547/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4548/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4549/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4550/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4551/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4552/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4553/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4554/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4555/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4556/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4557/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4558/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4559/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4560/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4561/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4562/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4563/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4564/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4565/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4566/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4567/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4568/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4569/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4570/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4571/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4572/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4573/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4574/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4575/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4576/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4577/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4578/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4579/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4580/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4581/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4582/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4583/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4584/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4585/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4586/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4587/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4588/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4589/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4590/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4591/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4592/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4593/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4594/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4595/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4596/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4597/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4598/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4599/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4600/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4601/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4602/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4603/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4604/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4605/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4606/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4607/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4608/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4609/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4610/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4611/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4612/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4613/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4614/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4615/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4616/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4617/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4618/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4619/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4620/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4621/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4622/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4623/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4624/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4625/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4626/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4627/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4628/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4629/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4630/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4631/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4632/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4633/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4634/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4635/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4636/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4637/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4638/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4639/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4640/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4641/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4642/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4643/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4644/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4645/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4646/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4647/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4648/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4649/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4650/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4651/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4652/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4653/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4654/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4655/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4656/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4657/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4658/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4659/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4660/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4661/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4662/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4663/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4664/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4665/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4666/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4667/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4668/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4669/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4670/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4671/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4672/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4673/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4674/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4675/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4676/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4677/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4678/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4679/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4680/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4681/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4682/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4683/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4684/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4685/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4686/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4687/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4688/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4689/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4690/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4691/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4692/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4693/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4694/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4695/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4696/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4697/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4698/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4699/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4700/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4701/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4702/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4703/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4704/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4705/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4706/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4707/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4708/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4709/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4710/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4711/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4712/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4713/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4714/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4715/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4716/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4717/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4718/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4719/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4720/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4721/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4722/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4723/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4724/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4725/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4726/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4727/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4728/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4729/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4730/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4731/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4732/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4733/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4734/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4735/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4736/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4737/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4738/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4739/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4740/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4741/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4742/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4743/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4744/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4745/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4746/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4747/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4748/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4749/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4750/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4751/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4752/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4753/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4754/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4755/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4756/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4757/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4758/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4759/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4760/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4761/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4762/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4763/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4764/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4765/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4766/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4767/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4768/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4769/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4770/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4771/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4772/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4773/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4774/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4775/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4776/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4777/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4778/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4779/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4780/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4781/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4782/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4783/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4784/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4785/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4786/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4787/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4788/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4789/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4790/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4791/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4792/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4793/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4794/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4795/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4796/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4797/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4798/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4799/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4800/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4801/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4802/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4803/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4804/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4805/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4806/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4807/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4808/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4809/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4810/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4811/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4812/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4813/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4814/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4815/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4816/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4817/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4818/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4819/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4820/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4821/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4822/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4823/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4824/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4825/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4826/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4827/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4828/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4829/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4830/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4831/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4832/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4833/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4834/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4835/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4836/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4837/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4838/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4839/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4840/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4841/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4842/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4843/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4844/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4845/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4846/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4847/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4848/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4849/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4850/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4851/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4852/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4853/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4854/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4855/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4856/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4857/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4858/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4859/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4860/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4861/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4862/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4863/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4864/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4865/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4866/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4867/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4868/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4869/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4870/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4871/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4872/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4873/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4874/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4875/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4876/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4877/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4878/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4879/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4880/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4881/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4882/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4883/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4884/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4885/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4886/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4887/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4888/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4889/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4890/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4891/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4892/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4893/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4894/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4895/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4896/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4897/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4898/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4899/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4900/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4901/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4902/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4903/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4904/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4905/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4906/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4907/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4908/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4909/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4910/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4911/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4912/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4913/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4914/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4915/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4916/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4917/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4918/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4919/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4920/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4921/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4922/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4923/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4924/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4925/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4926/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4927/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4928/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4929/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4930/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4931/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4932/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4933/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4934/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4935/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4936/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4937/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4938/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4939/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4940/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4941/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4942/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4943/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4944/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4945/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4946/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4947/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4948/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4949/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4950/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4951/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4952/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4953/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4954/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4955/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4956/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4957/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4958/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4959/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4960/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4961/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4962/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4963/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4964/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4965/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4966/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4967/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4968/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4969/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4970/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4971/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4972/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4973/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4974/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4975/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4976/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4977/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4978/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4979/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4980/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4981/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4982/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4983/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4984/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4985/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4986/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4987/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4988/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4989/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4990/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4991/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4992/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4993/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4994/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4995/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4996/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4997/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4998/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4999/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5000/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5001/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5002/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5003/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5004/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5005/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5006/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5007/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5008/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5009/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5010/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5011/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5012/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5013/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5014/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5015/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5016/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5017/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5018/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5019/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5020/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5021/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5022/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5023/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5024/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5025/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5026/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5027/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5028/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5029/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5030/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5031/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5032/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5033/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5034/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5035/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5036/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5037/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5038/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5039/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5040/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5041/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5042/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5043/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5044/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5045/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5046/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5047/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5048/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5049/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5050/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5051/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5052/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5053/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5054/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5055/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5056/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5057/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5058/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5059/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5060/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5061/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5062/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5063/10000..  Training Loss: 0.168.. \n",
      "Epoch: 5064/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5065/10000..  Training Loss: 0.168.. \n",
      "Epoch: 5066/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5067/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5068/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5069/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5070/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5071/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5072/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5073/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5074/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5075/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5076/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5077/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5078/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5079/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5080/10000..  Training Loss: 0.168.. \n",
      "Epoch: 5081/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5082/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5083/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5084/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5085/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5086/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5087/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5088/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5089/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5090/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5091/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5092/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5093/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5094/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5095/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5096/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5097/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5098/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5099/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5100/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5101/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5102/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5103/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5104/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5105/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5106/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5107/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5108/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5109/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5110/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5111/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5112/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5113/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5114/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5115/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5116/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5117/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5118/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5119/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5120/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5121/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5122/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5123/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5124/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5125/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5126/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5127/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5128/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5129/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5130/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5131/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5132/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5133/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5134/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5135/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5136/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5137/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5138/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5139/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5140/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5141/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5142/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5143/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5144/10000..  Training Loss: 0.169.. \n",
      "Epoch: 5145/10000..  Training Loss: 0.168.. \n",
      "Epoch: 5146/10000..  Training Loss: 0.169.. \n",
      "Epoch: 5147/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5148/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5149/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5150/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5151/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5152/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5153/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5154/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5155/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5156/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5157/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5158/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5159/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5160/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5161/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5162/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5163/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5164/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5165/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5166/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5167/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5168/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5169/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5170/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5171/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5172/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5173/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5174/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5175/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5176/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5177/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5178/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5179/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5180/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5181/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5182/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5183/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5184/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5185/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5186/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5187/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5188/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5189/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5190/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5191/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5192/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5193/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5194/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5195/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5196/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5197/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5198/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5199/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5200/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5201/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5202/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5203/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5204/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5205/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5206/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5207/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5208/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5209/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5210/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5211/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5212/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5213/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5214/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5215/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5216/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5217/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5218/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5219/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5220/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5221/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5222/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5223/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5224/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5225/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5226/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5227/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5228/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5229/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5230/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5231/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5232/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5233/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5234/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5235/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5236/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5237/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5238/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5239/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5240/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5241/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5242/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5243/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5244/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5245/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5246/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5247/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5248/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5249/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5250/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5251/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5252/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5253/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5254/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5255/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5256/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5257/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5258/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5259/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5260/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5261/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5262/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5263/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5264/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5265/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5266/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5267/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5268/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5269/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5270/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5271/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5272/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5273/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5274/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5275/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5276/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5277/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5278/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5279/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5280/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5281/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5282/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5283/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5284/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5285/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5286/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5287/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5288/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5289/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5290/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5291/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5292/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5293/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5294/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5295/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5296/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5297/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5298/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5299/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5300/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5301/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5302/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5303/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5304/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5305/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5306/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5307/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5308/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5309/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5310/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5311/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5312/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5313/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5314/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5315/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5316/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5317/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5318/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5319/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5320/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5321/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5322/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5323/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5324/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5325/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5326/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5327/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5328/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5329/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5330/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5331/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5332/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5333/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5334/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5335/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5336/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5337/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5338/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5339/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5340/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5341/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5342/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5343/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5344/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5345/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5346/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5347/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5348/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5349/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5350/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5351/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5352/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5353/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5354/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5355/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5356/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5357/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5358/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5359/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5360/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5361/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5362/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5363/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5364/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5365/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5366/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5367/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5368/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5369/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5370/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5371/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5372/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5373/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5374/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5375/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5376/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5377/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5378/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5379/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5380/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5381/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5382/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5383/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5384/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5385/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5386/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5387/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5388/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5389/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5390/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5391/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5392/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5393/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5394/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5395/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5396/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5397/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5398/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5399/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5400/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5401/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5402/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5403/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5404/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5405/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5406/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5407/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5408/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5409/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5410/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5411/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5412/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5413/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5414/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5415/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5416/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5417/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5418/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5419/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5420/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5421/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5422/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5423/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5424/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5425/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5426/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5427/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5428/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5429/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5430/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5431/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5432/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5433/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5434/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5435/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5436/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5437/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5438/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5439/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5440/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5441/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5442/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5443/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5444/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5445/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5446/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5447/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5448/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5449/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5450/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5451/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5452/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5453/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5454/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5455/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5456/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5457/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5458/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5459/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5460/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5461/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5462/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5463/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5464/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5465/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5466/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5467/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5468/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5469/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5470/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5471/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5472/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5473/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5474/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5475/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5476/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5477/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5478/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5479/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5480/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5481/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5482/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5483/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5484/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5485/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5486/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5487/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5488/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5489/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5490/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5491/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5492/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5493/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5494/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5495/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5496/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5497/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5498/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5499/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5500/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5501/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5502/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5503/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5504/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5505/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5506/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5507/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5508/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5509/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5510/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5511/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5512/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5513/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5514/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5515/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5516/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5517/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5518/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5519/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5520/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5521/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5522/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5523/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5524/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5525/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5526/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5527/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5528/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5529/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5530/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5531/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5532/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5533/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5534/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5535/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5536/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5537/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5538/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5539/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5540/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5541/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5542/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5543/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5544/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5545/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5546/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5547/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5548/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5549/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5550/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5551/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5552/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5553/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5554/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5555/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5556/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5557/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5558/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5559/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5560/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5561/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5562/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5563/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5564/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5565/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5566/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5567/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5568/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5569/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5570/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5571/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5572/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5573/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5574/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5575/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5576/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5577/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5578/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5579/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5580/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5581/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5582/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5583/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5584/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5585/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5586/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5587/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5588/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5589/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5590/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5591/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5592/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5593/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5594/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5595/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5596/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5597/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5598/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5599/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5600/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5601/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5602/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5603/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5604/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5605/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5606/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5607/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5608/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5609/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5610/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5611/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5612/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5613/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5614/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5615/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5616/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5617/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5618/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5619/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5620/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5621/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5622/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5623/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5624/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5625/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5626/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5627/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5628/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5629/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5630/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5631/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5632/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5633/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5634/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5635/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5636/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5637/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5638/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5639/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5640/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5641/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5642/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5643/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5644/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5645/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5646/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5647/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5648/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5649/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5650/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5651/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5652/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5653/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5654/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5655/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5656/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5657/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5658/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5659/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5660/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5661/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5662/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5663/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5664/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5665/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5666/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5667/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5668/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5669/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5670/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5671/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5672/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5673/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5674/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5675/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5676/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5677/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5678/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5679/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5680/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5681/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5682/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5683/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5684/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5685/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5686/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5687/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5688/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5689/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5690/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5691/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5692/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5693/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5694/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5695/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5696/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5697/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5698/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5699/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5700/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5701/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5702/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5703/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5704/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5705/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5706/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5707/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5708/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5709/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5710/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5711/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5712/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5713/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5714/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5715/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5716/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5717/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5718/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5719/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5720/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5721/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5722/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5723/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5724/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5725/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5726/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5727/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5728/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5729/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5730/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5731/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5732/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5733/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5734/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5735/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5736/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5737/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5738/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5739/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5740/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5741/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5742/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5743/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5744/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5745/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5746/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5747/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5748/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5749/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5750/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5751/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5752/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5753/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5754/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5755/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5756/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5757/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5758/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5759/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5760/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5761/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5762/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5763/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5764/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5765/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5766/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5767/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5768/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5769/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5770/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5771/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5772/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5773/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5774/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5775/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5776/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5777/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5778/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5779/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5780/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5781/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5782/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5783/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5784/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5785/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5786/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5787/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5788/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5789/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5790/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5791/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5792/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5793/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5794/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5795/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5796/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5797/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5798/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5799/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5800/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5801/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5802/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5803/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5804/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5805/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5806/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5807/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5808/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5809/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5810/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5811/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5812/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5813/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5814/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5815/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5816/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5817/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5818/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5819/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5820/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5821/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5822/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5823/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5824/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5825/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5826/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5827/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5828/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5829/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5830/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5831/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5832/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5833/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5834/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5835/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5836/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5837/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5838/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5839/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5840/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5841/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5842/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5843/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5844/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5845/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5846/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5847/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5848/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5849/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5850/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5851/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5852/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5853/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5854/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5855/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5856/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5857/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5858/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5859/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5860/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5861/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5862/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5863/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5864/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5865/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5866/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5867/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5868/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5869/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5870/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5871/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5872/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5873/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5874/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5875/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5876/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5877/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5878/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5879/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5880/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5881/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5882/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5883/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5884/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5885/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5886/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5887/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5888/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5889/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5890/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5891/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5892/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5893/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5894/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5895/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5896/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5897/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5898/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5899/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5900/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5901/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5902/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5903/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5904/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5905/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5906/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5907/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5908/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5909/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5910/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5911/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5912/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5913/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5914/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5915/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5916/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5917/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5918/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5919/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5920/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5921/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5922/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5923/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5924/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5925/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5926/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5927/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5928/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5929/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5930/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5931/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5932/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5933/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5934/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5935/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5936/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5937/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5938/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5939/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5940/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5941/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5942/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5943/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5944/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5945/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5946/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5947/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5948/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5949/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5950/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5951/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5952/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5953/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5954/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5955/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5956/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5957/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5958/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5959/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5960/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5961/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5962/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5963/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5964/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5965/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5966/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5967/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5968/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5969/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5970/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5971/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5972/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5973/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5974/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5975/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5976/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5977/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5978/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5979/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5980/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5981/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5982/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5983/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5984/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5985/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5986/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5987/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5988/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5989/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5990/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5991/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5992/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5993/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5994/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5995/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5996/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5997/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5998/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5999/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6000/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6001/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6002/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6003/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6004/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6005/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6006/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6007/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6008/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6009/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6010/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6011/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6012/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6013/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6014/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6015/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6016/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6017/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6018/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6019/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6020/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6021/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6022/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6023/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6024/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6025/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6026/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6027/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6028/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6029/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6030/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6031/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6032/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6033/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6034/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6035/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6036/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6037/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6038/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6039/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6040/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6041/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6042/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6043/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6044/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6045/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6046/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6047/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6048/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6049/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6050/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6051/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6052/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6053/10000..  Training Loss: 0.162.. \n",
      "Epoch: 6054/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6055/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6056/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6057/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6058/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6059/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6060/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6061/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6062/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6063/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6064/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6065/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6066/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6067/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6068/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6069/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6070/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6071/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6072/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6073/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6074/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6075/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6076/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6077/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6078/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6079/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6080/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6081/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6082/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6083/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6084/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6085/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6086/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6087/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6088/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6089/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6090/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6091/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6092/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6093/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6094/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6095/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6096/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6097/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6098/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6099/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6100/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6101/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6102/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6103/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6104/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6105/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6106/10000..  Training Loss: 0.162.. \n",
      "Epoch: 6107/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6108/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6109/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6110/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6111/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6112/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6113/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6114/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6115/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6116/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6117/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6118/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6119/10000..  Training Loss: 0.162.. \n",
      "Epoch: 6120/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6121/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6122/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6123/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6124/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6125/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6126/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6127/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6128/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6129/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6130/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6131/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6132/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6133/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6134/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6135/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6136/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6137/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6138/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6139/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6140/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6141/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6142/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6143/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6144/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6145/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6146/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6147/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6148/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6149/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6150/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6151/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6152/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6153/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6154/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6155/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6156/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6157/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6158/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6159/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6160/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6161/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6162/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6163/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6164/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6165/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6166/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6167/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6168/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6169/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6170/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6171/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6172/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6173/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6174/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6175/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6176/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6177/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6178/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6179/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6180/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6181/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6182/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6183/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6184/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6185/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6186/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6187/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6188/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6189/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6190/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6191/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6192/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6193/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6194/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6195/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6196/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6197/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6198/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6199/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6200/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6201/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6202/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6203/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6204/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6205/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6206/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6207/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6208/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6209/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6210/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6211/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6212/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6213/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6214/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6215/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6216/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6217/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6218/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6219/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6220/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6221/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6222/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6223/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6224/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6225/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6226/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6227/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6228/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6229/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6230/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6231/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6232/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6233/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6234/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6235/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6236/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6237/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6238/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6239/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6240/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6241/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6242/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6243/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6244/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6245/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6246/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6247/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6248/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6249/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6250/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6251/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6252/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6253/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6254/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6255/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6256/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6257/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6258/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6259/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6260/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6261/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6262/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6263/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6264/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6265/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6266/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6267/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6268/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6269/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6270/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6271/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6272/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6273/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6274/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6275/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6276/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6277/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6278/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6279/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6280/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6281/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6282/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6283/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6284/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6285/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6286/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6287/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6288/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6289/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6290/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6291/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6292/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6293/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6294/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6295/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6296/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6297/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6298/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6299/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6300/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6301/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6302/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6303/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6304/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6305/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6306/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6307/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6308/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6309/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6310/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6311/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6312/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6313/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6314/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6315/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6316/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6317/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6318/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6319/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6320/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6321/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6322/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6323/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6324/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6325/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6326/10000..  Training Loss: 0.162.. \n",
      "Epoch: 6327/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6328/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6329/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6330/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6331/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6332/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6333/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6334/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6335/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6336/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6337/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6338/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6339/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6340/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6341/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6342/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6343/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6344/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6345/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6346/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6347/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6348/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6349/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6350/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6351/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6352/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6353/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6354/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6355/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6356/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6357/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6358/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6359/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6360/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6361/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6362/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6363/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6364/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6365/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6366/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6367/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6368/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6369/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6370/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6371/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6372/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6373/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6374/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6375/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6376/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6377/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6378/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6379/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6380/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6381/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6382/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6383/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6384/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6385/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6386/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6387/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6388/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6389/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6390/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6391/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6392/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6393/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6394/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6395/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6396/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6397/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6398/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6399/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6400/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6401/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6402/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6403/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6404/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6405/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6406/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6407/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6408/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6409/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6410/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6411/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6412/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6413/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6414/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6415/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6416/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6417/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6418/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6419/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6420/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6421/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6422/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6423/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6424/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6425/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6426/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6427/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6428/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6429/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6430/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6431/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6432/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6433/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6434/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6435/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6436/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6437/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6438/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6439/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6440/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6441/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6442/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6443/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6444/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6445/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6446/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6447/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6448/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6449/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6450/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6451/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6452/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6453/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6454/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6455/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6456/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6457/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6458/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6459/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6460/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6461/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6462/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6463/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6464/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6465/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6466/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6467/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6468/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6469/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6470/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6471/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6472/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6473/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6474/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6475/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6476/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6477/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6478/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6479/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6480/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6481/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6482/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6483/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6484/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6485/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6486/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6487/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6488/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6489/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6490/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6491/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6492/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6493/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6494/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6495/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6496/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6497/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6498/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6499/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6500/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6501/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6502/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6503/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6504/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6505/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6506/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6507/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6508/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6509/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6510/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6511/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6512/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6513/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6514/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6515/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6516/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6517/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6518/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6519/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6520/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6521/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6522/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6523/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6524/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6525/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6526/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6527/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6528/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6529/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6530/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6531/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6532/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6533/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6534/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6535/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6536/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6537/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6538/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6539/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6540/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6541/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6542/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6543/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6544/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6545/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6546/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6547/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6548/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6549/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6550/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6551/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6552/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6553/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6554/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6555/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6556/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6557/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6558/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6559/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6560/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6561/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6562/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6563/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6564/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6565/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6566/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6567/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6568/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6569/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6570/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6571/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6572/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6573/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6574/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6575/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6576/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6577/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6578/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6579/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6580/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6581/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6582/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6583/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6584/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6585/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6586/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6587/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6588/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6589/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6590/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6591/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6592/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6593/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6594/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6595/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6596/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6597/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6598/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6599/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6600/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6601/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6602/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6603/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6604/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6605/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6606/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6607/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6608/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6609/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6610/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6611/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6612/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6613/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6614/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6615/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6616/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6617/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6618/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6619/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6620/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6621/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6622/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6623/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6624/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6625/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6626/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6627/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6628/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6629/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6630/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6631/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6632/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6633/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6634/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6635/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6636/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6637/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6638/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6639/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6640/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6641/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6642/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6643/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6644/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6645/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6646/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6647/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6648/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6649/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6650/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6651/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6652/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6653/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6654/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6655/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6656/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6657/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6658/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6659/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6660/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6661/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6662/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6663/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6664/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6665/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6666/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6667/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6668/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6669/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6670/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6671/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6672/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6673/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6674/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6675/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6676/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6677/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6678/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6679/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6680/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6681/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6682/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6683/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6684/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6685/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6686/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6687/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6688/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6689/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6690/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6691/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6692/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6693/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6694/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6695/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6696/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6697/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6698/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6699/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6700/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6701/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6702/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6703/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6704/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6705/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6706/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6707/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6708/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6709/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6710/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6711/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6712/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6713/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6714/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6715/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6716/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6717/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6718/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6719/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6720/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6721/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6722/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6723/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6724/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6725/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6726/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6727/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6728/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6729/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6730/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6731/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6732/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6733/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6734/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6735/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6736/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6737/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6738/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6739/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6740/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6741/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6742/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6743/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6744/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6745/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6746/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6747/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6748/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6749/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6750/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6751/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6752/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6753/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6754/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6755/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6756/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6757/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6758/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6759/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6760/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6761/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6762/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6763/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6764/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6765/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6766/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6767/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6768/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6769/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6770/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6771/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6772/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6773/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6774/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6775/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6776/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6777/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6778/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6779/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6780/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6781/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6782/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6783/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6784/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6785/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6786/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6787/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6788/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6789/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6790/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6791/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6792/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6793/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6794/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6795/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6796/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6797/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6798/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6799/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6800/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6801/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6802/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6803/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6804/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6805/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6806/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6807/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6808/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6809/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6810/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6811/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6812/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6813/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6814/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6815/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6816/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6817/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6818/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6819/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6820/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6821/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6822/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6823/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6824/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6825/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6826/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6827/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6828/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6829/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6830/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6831/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6832/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6833/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6834/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6835/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6836/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6837/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6838/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6839/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6840/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6841/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6842/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6843/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6844/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6845/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6846/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6847/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6848/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6849/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6850/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6851/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6852/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6853/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6854/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6855/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6856/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6857/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6858/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6859/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6860/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6861/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6862/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6863/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6864/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6865/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6866/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6867/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6868/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6869/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6870/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6871/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6872/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6873/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6874/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6875/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6876/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6877/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6878/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6879/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6880/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6881/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6882/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6883/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6884/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6885/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6886/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6887/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6888/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6889/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6890/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6891/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6892/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6893/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6894/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6895/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6896/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6897/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6898/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6899/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6900/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6901/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6902/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6903/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6904/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6905/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6906/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6907/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6908/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6909/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6910/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6911/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6912/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6913/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6914/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6915/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6916/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6917/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6918/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6919/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6920/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6921/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6922/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6923/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6924/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6925/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6926/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6927/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6928/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6929/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6930/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6931/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6932/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6933/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6934/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6935/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6936/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6937/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6938/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6939/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6940/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6941/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6942/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6943/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6944/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6945/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6946/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6947/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6948/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6949/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6950/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6951/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6952/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6953/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6954/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6955/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6956/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6957/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6958/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6959/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6960/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6961/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6962/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6963/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6964/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6965/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6966/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6967/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6968/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6969/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6970/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6971/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6972/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6973/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6974/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6975/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6976/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6977/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6978/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6979/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6980/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6981/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6982/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6983/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6984/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6985/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6986/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6987/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6988/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6989/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6990/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6991/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6992/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6993/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6994/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6995/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6996/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6997/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6998/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6999/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7000/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7001/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7002/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7003/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7004/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7005/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7006/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7007/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7008/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7009/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7010/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7011/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7012/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7013/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7014/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7015/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7016/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7017/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7018/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7019/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7020/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7021/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7022/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7023/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7024/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7025/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7026/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7027/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7028/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7029/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7030/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7031/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7032/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7033/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7034/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7035/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7036/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7037/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7038/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7039/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7040/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7041/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7042/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7043/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7044/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7045/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7046/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7047/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7048/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7049/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7050/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7051/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7052/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7053/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7054/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7055/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7056/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7057/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7058/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7059/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7060/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7061/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7062/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7063/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7064/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7065/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7066/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7067/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7068/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7069/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7070/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7071/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7072/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7073/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7074/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7075/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7076/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7077/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7078/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7079/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7080/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7081/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7082/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7083/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7084/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7085/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7086/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7087/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7088/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7089/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7090/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7091/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7092/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7093/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7094/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7095/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7096/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7097/10000..  Training Loss: 0.156.. \n",
      "Epoch: 7098/10000..  Training Loss: 0.155.. \n",
      "Epoch: 7099/10000..  Training Loss: 0.155.. \n",
      "Epoch: 7100/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7101/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7102/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7103/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7104/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7105/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7106/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7107/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7108/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7109/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7110/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7111/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7112/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7113/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7114/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7115/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7116/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7117/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7118/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7119/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7120/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7121/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7122/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7123/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7124/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7125/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7126/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7127/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7128/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7129/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7130/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7131/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7132/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7133/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7134/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7135/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7136/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7137/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7138/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7139/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7140/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7141/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7142/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7143/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7144/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7145/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7146/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7147/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7148/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7149/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7150/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7151/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7152/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7153/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7154/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7155/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7156/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7157/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7158/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7159/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7160/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7161/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7162/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7163/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7164/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7165/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7166/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7167/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7168/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7169/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7170/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7171/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7172/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7173/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7174/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7175/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7176/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7177/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7178/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7179/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7180/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7181/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7182/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7183/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7184/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7185/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7186/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7187/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7188/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7189/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7190/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7191/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7192/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7193/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7194/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7195/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7196/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7197/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7198/10000..  Training Loss: 0.157.. \n",
      "Epoch: 7199/10000..  Training Loss: 0.156.. \n",
      "Epoch: 7200/10000..  Training Loss: 0.157.. \n",
      "Epoch: 7201/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7202/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7203/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7204/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7205/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7206/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7207/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7208/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7209/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7210/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7211/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7212/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7213/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7214/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7215/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7216/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7217/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7218/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7219/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7220/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7221/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7222/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7223/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7224/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7225/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7226/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7227/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7228/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7229/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7230/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7231/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7232/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7233/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7234/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7235/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7236/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7237/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7238/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7239/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7240/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7241/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7242/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7243/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7244/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7245/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7246/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7247/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7248/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7249/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7250/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7251/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7252/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7253/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7254/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7255/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7256/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7257/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7258/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7259/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7260/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7261/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7262/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7263/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7264/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7265/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7266/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7267/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7268/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7269/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7270/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7271/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7272/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7273/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7274/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7275/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7276/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7277/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7278/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7279/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7280/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7281/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7282/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7283/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7284/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7285/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7286/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7287/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7288/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7289/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7290/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7291/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7292/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7293/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7294/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7295/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7296/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7297/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7298/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7299/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7300/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7301/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7302/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7303/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7304/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7305/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7306/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7307/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7308/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7309/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7310/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7311/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7312/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7313/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7314/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7315/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7316/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7317/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7318/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7319/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7320/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7321/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7322/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7323/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7324/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7325/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7326/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7327/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7328/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7329/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7330/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7331/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7332/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7333/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7334/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7335/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7336/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7337/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7338/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7339/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7340/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7341/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7342/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7343/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7344/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7345/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7346/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7347/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7348/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7349/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7350/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7351/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7352/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7353/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7354/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7355/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7356/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7357/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7358/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7359/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7360/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7361/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7362/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7363/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7364/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7365/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7366/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7367/10000..  Training Loss: 0.155.. \n",
      "Epoch: 7368/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7369/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7370/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7371/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7372/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7373/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7374/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7375/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7376/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7377/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7378/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7379/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7380/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7381/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7382/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7383/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7384/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7385/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7386/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7387/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7388/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7389/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7390/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7391/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7392/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7393/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7394/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7395/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7396/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7397/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7398/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7399/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7400/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7401/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7402/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7403/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7404/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7405/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7406/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7407/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7408/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7409/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7410/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7411/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7412/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7413/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7414/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7415/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7416/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7417/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7418/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7419/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7420/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7421/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7422/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7423/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7424/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7425/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7426/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7427/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7428/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7429/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7430/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7431/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7432/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7433/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7434/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7435/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7436/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7437/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7438/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7439/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7440/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7441/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7442/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7443/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7444/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7445/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7446/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7447/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7448/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7449/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7450/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7451/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7452/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7453/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7454/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7455/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7456/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7457/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7458/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7459/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7460/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7461/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7462/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7463/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7464/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7465/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7466/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7467/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7468/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7469/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7470/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7471/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7472/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7473/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7474/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7475/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7476/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7477/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7478/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7479/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7480/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7481/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7482/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7483/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7484/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7485/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7486/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7487/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7488/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7489/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7490/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7491/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7492/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7493/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7494/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7495/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7496/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7497/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7498/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7499/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7500/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7501/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7502/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7503/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7504/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7505/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7506/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7507/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7508/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7509/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7510/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7511/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7512/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7513/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7514/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7515/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7516/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7517/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7518/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7519/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7520/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7521/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7522/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7523/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7524/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7525/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7526/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7527/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7528/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7529/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7530/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7531/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7532/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7533/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7534/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7535/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7536/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7537/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7538/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7539/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7540/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7541/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7542/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7543/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7544/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7545/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7546/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7547/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7548/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7549/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7550/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7551/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7552/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7553/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7554/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7555/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7556/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7557/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7558/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7559/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7560/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7561/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7562/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7563/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7564/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7565/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7566/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7567/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7568/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7569/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7570/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7571/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7572/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7573/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7574/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7575/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7576/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7577/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7578/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7579/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7580/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7581/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7582/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7583/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7584/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7585/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7586/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7587/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7588/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7589/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7590/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7591/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7592/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7593/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7594/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7595/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7596/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7597/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7598/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7599/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7600/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7601/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7602/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7603/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7604/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7605/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7606/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7607/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7608/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7609/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7610/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7611/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7612/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7613/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7614/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7615/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7616/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7617/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7618/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7619/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7620/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7621/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7622/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7623/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7624/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7625/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7626/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7627/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7628/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7629/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7630/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7631/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7632/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7633/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7634/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7635/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7636/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7637/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7638/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7639/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7640/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7641/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7642/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7643/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7644/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7645/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7646/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7647/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7648/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7649/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7650/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7651/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7652/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7653/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7654/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7655/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7656/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7657/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7658/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7659/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7660/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7661/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7662/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7663/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7664/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7665/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7666/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7667/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7668/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7669/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7670/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7671/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7672/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7673/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7674/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7675/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7676/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7677/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7678/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7679/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7680/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7681/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7682/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7683/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7684/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7685/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7686/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7687/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7688/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7689/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7690/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7691/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7692/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7693/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7694/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7695/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7696/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7697/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7698/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7699/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7700/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7701/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7702/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7703/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7704/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7705/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7706/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7707/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7708/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7709/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7710/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7711/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7712/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7713/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7714/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7715/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7716/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7717/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7718/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7719/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7720/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7721/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7722/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7723/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7724/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7725/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7726/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7727/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7728/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7729/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7730/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7731/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7732/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7733/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7734/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7735/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7736/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7737/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7738/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7739/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7740/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7741/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7742/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7743/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7744/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7745/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7746/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7747/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7748/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7749/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7750/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7751/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7752/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7753/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7754/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7755/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7756/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7757/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7758/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7759/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7760/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7761/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7762/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7763/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7764/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7765/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7766/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7767/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7768/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7769/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7770/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7771/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7772/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7773/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7774/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7775/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7776/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7777/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7778/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7779/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7780/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7781/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7782/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7783/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7784/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7785/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7786/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7787/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7788/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7789/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7790/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7791/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7792/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7793/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7794/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7795/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7796/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7797/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7798/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7799/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7800/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7801/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7802/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7803/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7804/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7805/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7806/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7807/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7808/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7809/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7810/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7811/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7812/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7813/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7814/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7815/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7816/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7817/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7818/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7819/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7820/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7821/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7822/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7823/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7824/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7825/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7826/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7827/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7828/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7829/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7830/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7831/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7832/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7833/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7834/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7835/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7836/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7837/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7838/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7839/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7840/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7841/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7842/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7843/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7844/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7845/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7846/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7847/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7848/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7849/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7850/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7851/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7852/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7853/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7854/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7855/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7856/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7857/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7858/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7859/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7860/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7861/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7862/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7863/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7864/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7865/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7866/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7867/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7868/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7869/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7870/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7871/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7872/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7873/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7874/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7875/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7876/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7877/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7878/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7879/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7880/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7881/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7882/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7883/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7884/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7885/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7886/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7887/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7888/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7889/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7890/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7891/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7892/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7893/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7894/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7895/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7896/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7897/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7898/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7899/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7900/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7901/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7902/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7903/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7904/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7905/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7906/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7907/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7908/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7909/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7910/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7911/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7912/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7913/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7914/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7915/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7916/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7917/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7918/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7919/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7920/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7921/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7922/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7923/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7924/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7925/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7926/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7927/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7928/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7929/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7930/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7931/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7932/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7933/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7934/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7935/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7936/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7937/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7938/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7939/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7940/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7941/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7942/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7943/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7944/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7945/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7946/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7947/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7948/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7949/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7950/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7951/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7952/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7953/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7954/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7955/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7956/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7957/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7958/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7959/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7960/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7961/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7962/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7963/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7964/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7965/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7966/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7967/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7968/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7969/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7970/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7971/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7972/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7973/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7974/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7975/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7976/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7977/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7978/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7979/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7980/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7981/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7982/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7983/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7984/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7985/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7986/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7987/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7988/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7989/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7990/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7991/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7992/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7993/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7994/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7995/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7996/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7997/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7998/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7999/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8000/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8001/10000..  Training Loss: 0.150.. \n",
      "Epoch: 8002/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8003/10000..  Training Loss: 0.151.. \n",
      "Epoch: 8004/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8005/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8006/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8007/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8008/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8009/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8010/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8011/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8012/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8013/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8014/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8015/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8016/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8017/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8018/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8019/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8020/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8021/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8022/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8023/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8024/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8025/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8026/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8027/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8028/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8029/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8030/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8031/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8032/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8033/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8034/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8035/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8036/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8037/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8038/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8039/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8040/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8041/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8042/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8043/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8044/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8045/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8046/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8047/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8048/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8049/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8050/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8051/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8052/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8053/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8054/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8055/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8056/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8057/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8058/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8059/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8060/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8061/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8062/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8063/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8064/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8065/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8066/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8067/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8068/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8069/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8070/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8071/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8072/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8073/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8074/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8075/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8076/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8077/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8078/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8079/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8080/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8081/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8082/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8083/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8084/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8085/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8086/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8087/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8088/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8089/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8090/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8091/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8092/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8093/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8094/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8095/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8096/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8097/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8098/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8099/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8100/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8101/10000..  Training Loss: 0.150.. \n",
      "Epoch: 8102/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8103/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8104/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8105/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8106/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8107/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8108/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8109/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8110/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8111/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8112/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8113/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8114/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8115/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8116/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8117/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8118/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8119/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8120/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8121/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8122/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8123/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8124/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8125/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8126/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8127/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8128/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8129/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8130/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8131/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8132/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8133/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8134/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8135/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8136/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8137/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8138/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8139/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8140/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8141/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8142/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8143/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8144/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8145/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8146/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8147/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8148/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8149/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8150/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8151/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8152/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8153/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8154/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8155/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8156/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8157/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8158/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8159/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8160/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8161/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8162/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8163/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8164/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8165/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8166/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8167/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8168/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8169/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8170/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8171/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8172/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8173/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8174/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8175/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8176/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8177/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8178/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8179/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8180/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8181/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8182/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8183/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8184/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8185/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8186/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8187/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8188/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8189/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8190/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8191/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8192/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8193/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8194/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8195/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8196/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8197/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8198/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8199/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8200/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8201/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8202/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8203/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8204/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8205/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8206/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8207/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8208/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8209/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8210/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8211/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8212/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8213/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8214/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8215/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8216/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8217/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8218/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8219/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8220/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8221/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8222/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8223/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8224/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8225/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8226/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8227/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8228/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8229/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8230/10000..  Training Loss: 0.150.. \n",
      "Epoch: 8231/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8232/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8233/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8234/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8235/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8236/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8237/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8238/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8239/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8240/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8241/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8242/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8243/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8244/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8245/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8246/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8247/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8248/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8249/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8250/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8251/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8252/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8253/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8254/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8255/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8256/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8257/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8258/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8259/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8260/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8261/10000..  Training Loss: 0.153.. \n",
      "Epoch: 8262/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8263/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8264/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8265/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8266/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8267/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8268/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8269/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8270/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8271/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8272/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8273/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8274/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8275/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8276/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8277/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8278/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8279/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8280/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8281/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8282/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8283/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8284/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8285/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8286/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8287/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8288/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8289/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8290/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8291/10000..  Training Loss: 0.151.. \n",
      "Epoch: 8292/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8293/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8294/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8295/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8296/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8297/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8298/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8299/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8300/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8301/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8302/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8303/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8304/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8305/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8306/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8307/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8308/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8309/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8310/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8311/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8312/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8313/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8314/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8315/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8316/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8317/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8318/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8319/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8320/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8321/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8322/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8323/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8324/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8325/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8326/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8327/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8328/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8329/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8330/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8331/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8332/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8333/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8334/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8335/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8336/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8337/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8338/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8339/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8340/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8341/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8342/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8343/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8344/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8345/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8346/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8347/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8348/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8349/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8350/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8351/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8352/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8353/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8354/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8355/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8356/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8357/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8358/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8359/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8360/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8361/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8362/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8363/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8364/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8365/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8366/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8367/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8368/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8369/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8370/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8371/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8372/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8373/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8374/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8375/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8376/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8377/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8378/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8379/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8380/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8381/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8382/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8383/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8384/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8385/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8386/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8387/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8388/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8389/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8390/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8391/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8392/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8393/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8394/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8395/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8396/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8397/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8398/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8399/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8400/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8401/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8402/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8403/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8404/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8405/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8406/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8407/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8408/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8409/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8410/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8411/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8412/10000..  Training Loss: 0.155.. \n",
      "Epoch: 8413/10000..  Training Loss: 0.153.. \n",
      "Epoch: 8414/10000..  Training Loss: 0.154.. \n",
      "Epoch: 8415/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8416/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8417/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8418/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8419/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8420/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8421/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8422/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8423/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8424/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8425/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8426/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8427/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8428/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8429/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8430/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8431/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8432/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8433/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8434/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8435/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8436/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8437/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8438/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8439/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8440/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8441/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8442/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8443/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8444/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8445/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8446/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8447/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8448/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8449/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8450/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8451/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8452/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8453/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8454/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8455/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8456/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8457/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8458/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8459/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8460/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8461/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8462/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8463/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8464/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8465/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8466/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8467/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8468/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8469/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8470/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8471/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8472/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8473/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8474/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8475/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8476/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8477/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8478/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8479/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8480/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8481/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8482/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8483/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8484/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8485/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8486/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8487/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8488/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8489/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8490/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8491/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8492/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8493/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8494/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8495/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8496/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8497/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8498/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8499/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8500/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8501/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8502/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8503/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8504/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8505/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8506/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8507/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8508/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8509/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8510/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8511/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8512/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8513/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8514/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8515/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8516/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8517/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8518/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8519/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8520/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8521/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8522/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8523/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8524/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8525/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8526/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8527/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8528/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8529/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8530/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8531/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8532/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8533/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8534/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8535/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8536/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8537/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8538/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8539/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8540/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8541/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8542/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8543/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8544/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8545/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8546/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8547/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8548/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8549/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8550/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8551/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8552/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8553/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8554/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8555/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8556/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8557/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8558/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8559/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8560/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8561/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8562/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8563/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8564/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8565/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8566/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8567/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8568/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8569/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8570/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8571/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8572/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8573/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8574/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8575/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8576/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8577/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8578/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8579/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8580/10000..  Training Loss: 0.150.. \n",
      "Epoch: 8581/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8582/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8583/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8584/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8585/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8586/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8587/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8588/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8589/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8590/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8591/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8592/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8593/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8594/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8595/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8596/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8597/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8598/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8599/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8600/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8601/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8602/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8603/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8604/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8605/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8606/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8607/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8608/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8609/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8610/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8611/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8612/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8613/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8614/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8615/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8616/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8617/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8618/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8619/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8620/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8621/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8622/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8623/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8624/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8625/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8626/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8627/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8628/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8629/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8630/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8631/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8632/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8633/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8634/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8635/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8636/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8637/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8638/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8639/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8640/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8641/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8642/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8643/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8644/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8645/10000..  Training Loss: 0.152.. \n",
      "Epoch: 8646/10000..  Training Loss: 0.152.. \n",
      "Epoch: 8647/10000..  Training Loss: 0.154.. \n",
      "Epoch: 8648/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8649/10000..  Training Loss: 0.152.. \n",
      "Epoch: 8650/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8651/10000..  Training Loss: 0.154.. \n",
      "Epoch: 8652/10000..  Training Loss: 0.150.. \n",
      "Epoch: 8653/10000..  Training Loss: 0.161.. \n",
      "Epoch: 8654/10000..  Training Loss: 0.168.. \n",
      "Epoch: 8655/10000..  Training Loss: 0.164.. \n",
      "Epoch: 8656/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8657/10000..  Training Loss: 0.155.. \n",
      "Epoch: 8658/10000..  Training Loss: 0.151.. \n",
      "Epoch: 8659/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8660/10000..  Training Loss: 0.153.. \n",
      "Epoch: 8661/10000..  Training Loss: 0.153.. \n",
      "Epoch: 8662/10000..  Training Loss: 0.153.. \n",
      "Epoch: 8663/10000..  Training Loss: 0.151.. \n",
      "Epoch: 8664/10000..  Training Loss: 0.151.. \n",
      "Epoch: 8665/10000..  Training Loss: 0.152.. \n",
      "Epoch: 8666/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8667/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8668/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8669/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8670/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8671/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8672/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8673/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8674/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8675/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8676/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8677/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8678/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8679/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8680/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8681/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8682/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8683/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8684/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8685/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8686/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8687/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8688/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8689/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8690/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8691/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8692/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8693/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8694/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8695/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8696/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8697/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8698/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8699/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8700/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8701/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8702/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8703/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8704/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8705/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8706/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8707/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8708/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8709/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8710/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8711/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8712/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8713/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8714/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8715/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8716/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8717/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8718/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8719/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8720/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8721/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8722/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8723/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8724/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8725/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8726/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8727/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8728/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8729/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8730/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8731/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8732/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8733/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8734/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8735/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8736/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8737/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8738/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8739/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8740/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8741/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8742/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8743/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8744/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8745/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8746/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8747/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8748/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8749/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8750/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8751/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8752/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8753/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8754/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8755/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8756/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8757/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8758/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8759/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8760/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8761/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8762/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8763/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8764/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8765/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8766/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8767/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8768/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8769/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8770/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8771/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8772/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8773/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8774/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8775/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8776/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8777/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8778/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8779/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8780/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8781/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8782/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8783/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8784/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8785/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8786/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8787/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8788/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8789/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8790/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8791/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8792/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8793/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8794/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8795/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8796/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8797/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8798/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8799/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8800/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8801/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8802/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8803/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8804/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8805/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8806/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8807/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8808/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8809/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8810/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8811/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8812/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8813/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8814/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8815/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8816/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8817/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8818/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8819/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8820/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8821/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8822/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8823/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8824/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8825/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8826/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8827/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8828/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8829/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8830/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8831/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8832/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8833/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8834/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8835/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8836/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8837/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8838/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8839/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8840/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8841/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8842/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8843/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8844/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8845/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8846/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8847/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8848/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8849/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8850/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8851/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8852/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8853/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8854/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8855/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8856/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8857/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8858/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8859/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8860/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8861/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8862/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8863/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8864/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8865/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8866/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8867/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8868/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8869/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8870/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8871/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8872/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8873/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8874/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8875/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8876/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8877/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8878/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8879/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8880/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8881/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8882/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8883/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8884/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8885/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8886/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8887/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8888/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8889/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8890/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8891/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8892/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8893/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8894/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8895/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8896/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8897/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8898/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8899/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8900/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8901/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8902/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8903/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8904/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8905/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8906/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8907/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8908/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8909/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8910/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8911/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8912/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8913/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8914/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8915/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8916/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8917/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8918/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8919/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8920/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8921/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8922/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8923/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8924/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8925/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8926/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8927/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8928/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8929/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8930/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8931/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8932/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8933/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8934/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8935/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8936/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8937/10000..  Training Loss: 0.155.. \n",
      "Epoch: 8938/10000..  Training Loss: 0.155.. \n",
      "Epoch: 8939/10000..  Training Loss: 0.167.. \n",
      "Epoch: 8940/10000..  Training Loss: 0.161.. \n",
      "Epoch: 8941/10000..  Training Loss: 0.162.. \n",
      "Epoch: 8942/10000..  Training Loss: 0.157.. \n",
      "Epoch: 8943/10000..  Training Loss: 0.158.. \n",
      "Epoch: 8944/10000..  Training Loss: 0.154.. \n",
      "Epoch: 8945/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8946/10000..  Training Loss: 0.154.. \n",
      "Epoch: 8947/10000..  Training Loss: 0.159.. \n",
      "Epoch: 8948/10000..  Training Loss: 1.244.. \n",
      "Epoch: 8949/10000..  Training Loss: 0.576.. \n",
      "Epoch: 8950/10000..  Training Loss: 1.138.. \n",
      "Epoch: 8951/10000..  Training Loss: 0.518.. \n",
      "Epoch: 8952/10000..  Training Loss: 0.838.. \n",
      "Epoch: 8953/10000..  Training Loss: 0.417.. \n",
      "Epoch: 8954/10000..  Training Loss: 0.792.. \n",
      "Epoch: 8955/10000..  Training Loss: 0.462.. \n",
      "Epoch: 8956/10000..  Training Loss: 0.549.. \n",
      "Epoch: 8957/10000..  Training Loss: 0.504.. \n",
      "Epoch: 8958/10000..  Training Loss: 0.467.. \n",
      "Epoch: 8959/10000..  Training Loss: 0.440.. \n",
      "Epoch: 8960/10000..  Training Loss: 0.413.. \n",
      "Epoch: 8961/10000..  Training Loss: 0.357.. \n",
      "Epoch: 8962/10000..  Training Loss: 0.381.. \n",
      "Epoch: 8963/10000..  Training Loss: 0.450.. \n",
      "Epoch: 8964/10000..  Training Loss: 0.340.. \n",
      "Epoch: 8965/10000..  Training Loss: 0.317.. \n",
      "Epoch: 8966/10000..  Training Loss: 0.380.. \n",
      "Epoch: 8967/10000..  Training Loss: 0.444.. \n",
      "Epoch: 8968/10000..  Training Loss: 0.283.. \n",
      "Epoch: 8969/10000..  Training Loss: 0.303.. \n",
      "Epoch: 8970/10000..  Training Loss: 0.460.. \n",
      "Epoch: 8971/10000..  Training Loss: 0.287.. \n",
      "Epoch: 8972/10000..  Training Loss: 0.350.. \n",
      "Epoch: 8973/10000..  Training Loss: 0.365.. \n",
      "Epoch: 8974/10000..  Training Loss: 0.321.. \n",
      "Epoch: 8975/10000..  Training Loss: 0.276.. \n",
      "Epoch: 8976/10000..  Training Loss: 0.226.. \n",
      "Epoch: 8977/10000..  Training Loss: 0.272.. \n",
      "Epoch: 8978/10000..  Training Loss: 0.312.. \n",
      "Epoch: 8979/10000..  Training Loss: 0.259.. \n",
      "Epoch: 8980/10000..  Training Loss: 0.228.. \n",
      "Epoch: 8981/10000..  Training Loss: 0.213.. \n",
      "Epoch: 8982/10000..  Training Loss: 0.227.. \n",
      "Epoch: 8983/10000..  Training Loss: 0.258.. \n",
      "Epoch: 8984/10000..  Training Loss: 0.222.. \n",
      "Epoch: 8985/10000..  Training Loss: 0.209.. \n",
      "Epoch: 8986/10000..  Training Loss: 0.203.. \n",
      "Epoch: 8987/10000..  Training Loss: 0.202.. \n",
      "Epoch: 8988/10000..  Training Loss: 0.192.. \n",
      "Epoch: 8989/10000..  Training Loss: 0.213.. \n",
      "Epoch: 8990/10000..  Training Loss: 0.194.. \n",
      "Epoch: 8991/10000..  Training Loss: 0.183.. \n",
      "Epoch: 8992/10000..  Training Loss: 0.184.. \n",
      "Epoch: 8993/10000..  Training Loss: 0.176.. \n",
      "Epoch: 8994/10000..  Training Loss: 0.180.. \n",
      "Epoch: 8995/10000..  Training Loss: 0.173.. \n",
      "Epoch: 8996/10000..  Training Loss: 0.174.. \n",
      "Epoch: 8997/10000..  Training Loss: 0.177.. \n",
      "Epoch: 8998/10000..  Training Loss: 0.174.. \n",
      "Epoch: 8999/10000..  Training Loss: 0.182.. \n",
      "Epoch: 9000/10000..  Training Loss: 0.171.. \n",
      "Epoch: 9001/10000..  Training Loss: 0.169.. \n",
      "Epoch: 9002/10000..  Training Loss: 0.170.. \n",
      "Epoch: 9003/10000..  Training Loss: 0.168.. \n",
      "Epoch: 9004/10000..  Training Loss: 0.166.. \n",
      "Epoch: 9005/10000..  Training Loss: 0.167.. \n",
      "Epoch: 9006/10000..  Training Loss: 0.165.. \n",
      "Epoch: 9007/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9008/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9009/10000..  Training Loss: 0.177.. \n",
      "Epoch: 9010/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9011/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9012/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9013/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9014/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9015/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9016/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9017/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9018/10000..  Training Loss: 0.166.. \n",
      "Epoch: 9019/10000..  Training Loss: 0.162.. \n",
      "Epoch: 9020/10000..  Training Loss: 0.162.. \n",
      "Epoch: 9021/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9022/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9023/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9024/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9025/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9026/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9027/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9028/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9029/10000..  Training Loss: 0.162.. \n",
      "Epoch: 9030/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9031/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9032/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9033/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9034/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9035/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9036/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9037/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9038/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9039/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9040/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9041/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9042/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9043/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9044/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9045/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9046/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9047/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9048/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9049/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9050/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9051/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9052/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9053/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9054/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9055/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9056/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9057/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9058/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9059/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9060/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9061/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9062/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9063/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9064/10000..  Training Loss: 0.164.. \n",
      "Epoch: 9065/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9066/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9067/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9068/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9069/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9070/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9071/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9072/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9073/10000..  Training Loss: 0.171.. \n",
      "Epoch: 9074/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9075/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9076/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9077/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9078/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9079/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9080/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9081/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9082/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9083/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9084/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9085/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9086/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9087/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9088/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9089/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9090/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9091/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9092/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9093/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9094/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9095/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9096/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9097/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9098/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9099/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9100/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9101/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9102/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9103/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9104/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9105/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9106/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9107/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9108/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9109/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9110/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9111/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9112/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9113/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9114/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9115/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9116/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9117/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9118/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9119/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9120/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9121/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9122/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9123/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9124/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9125/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9126/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9127/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9128/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9129/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9130/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9131/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9132/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9133/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9134/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9135/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9136/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9137/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9138/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9139/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9140/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9141/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9142/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9143/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9144/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9145/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9146/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9147/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9148/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9149/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9150/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9151/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9152/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9153/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9154/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9155/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9156/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9157/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9158/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9159/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9160/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9161/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9162/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9163/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9164/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9165/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9166/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9167/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9168/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9169/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9170/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9171/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9172/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9173/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9174/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9175/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9176/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9177/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9178/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9179/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9180/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9181/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9182/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9183/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9184/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9185/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9186/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9187/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9188/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9189/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9190/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9191/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9192/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9193/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9194/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9195/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9196/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9197/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9198/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9199/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9200/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9201/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9202/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9203/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9204/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9205/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9206/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9207/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9208/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9209/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9210/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9211/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9212/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9213/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9214/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9215/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9216/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9217/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9218/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9219/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9220/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9221/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9222/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9223/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9224/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9225/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9226/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9227/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9228/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9229/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9230/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9231/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9232/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9233/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9234/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9235/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9236/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9237/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9238/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9239/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9240/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9241/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9242/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9243/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9244/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9245/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9246/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9247/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9248/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9249/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9250/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9251/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9252/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9253/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9254/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9255/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9256/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9257/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9258/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9259/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9260/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9261/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9262/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9263/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9264/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9265/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9266/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9267/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9268/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9269/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9270/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9271/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9272/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9273/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9274/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9275/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9276/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9277/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9278/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9279/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9280/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9281/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9282/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9283/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9284/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9285/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9286/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9287/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9288/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9289/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9290/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9291/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9292/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9293/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9294/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9295/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9296/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9297/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9298/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9299/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9300/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9301/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9302/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9303/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9304/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9305/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9306/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9307/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9308/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9309/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9310/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9311/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9312/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9313/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9314/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9315/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9316/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9317/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9318/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9319/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9320/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9321/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9322/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9323/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9324/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9325/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9326/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9327/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9328/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9329/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9330/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9331/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9332/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9333/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9334/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9335/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9336/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9337/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9338/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9339/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9340/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9341/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9342/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9343/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9344/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9345/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9346/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9347/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9348/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9349/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9350/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9351/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9352/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9353/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9354/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9355/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9356/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9357/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9358/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9359/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9360/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9361/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9362/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9363/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9364/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9365/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9366/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9367/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9368/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9369/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9370/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9371/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9372/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9373/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9374/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9375/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9376/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9377/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9378/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9379/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9380/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9381/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9382/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9383/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9384/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9385/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9386/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9387/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9388/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9389/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9390/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9391/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9392/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9393/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9394/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9395/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9396/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9397/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9398/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9399/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9400/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9401/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9402/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9403/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9404/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9405/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9406/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9407/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9408/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9409/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9410/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9411/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9412/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9413/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9414/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9415/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9416/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9417/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9418/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9419/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9420/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9421/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9422/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9423/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9424/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9425/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9426/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9427/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9428/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9429/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9430/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9431/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9432/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9433/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9434/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9435/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9436/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9437/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9438/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9439/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9440/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9441/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9442/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9443/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9444/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9445/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9446/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9447/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9448/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9449/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9450/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9451/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9452/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9453/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9454/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9455/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9456/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9457/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9458/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9459/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9460/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9461/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9462/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9463/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9464/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9465/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9466/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9467/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9468/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9469/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9470/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9471/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9472/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9473/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9474/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9475/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9476/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9477/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9478/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9479/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9480/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9481/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9482/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9483/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9484/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9485/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9486/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9487/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9488/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9489/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9490/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9491/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9492/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9493/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9494/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9495/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9496/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9497/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9498/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9499/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9500/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9501/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9502/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9503/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9504/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9505/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9506/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9507/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9508/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9509/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9510/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9511/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9512/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9513/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9514/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9515/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9516/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9517/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9518/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9519/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9520/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9521/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9522/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9523/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9524/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9525/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9526/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9527/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9528/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9529/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9530/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9531/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9532/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9533/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9534/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9535/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9536/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9537/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9538/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9539/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9540/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9541/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9542/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9543/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9544/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9545/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9546/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9547/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9548/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9549/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9550/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9551/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9552/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9553/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9554/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9555/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9556/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9557/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9558/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9559/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9560/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9561/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9562/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9563/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9564/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9565/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9566/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9567/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9568/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9569/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9570/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9571/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9572/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9573/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9574/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9575/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9576/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9577/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9578/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9579/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9580/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9581/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9582/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9583/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9584/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9585/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9586/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9587/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9588/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9589/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9590/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9591/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9592/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9593/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9594/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9595/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9596/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9597/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9598/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9599/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9600/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9601/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9602/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9603/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9604/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9605/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9606/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9607/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9608/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9609/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9610/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9611/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9612/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9613/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9614/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9615/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9616/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9617/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9618/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9619/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9620/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9621/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9622/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9623/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9624/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9625/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9626/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9627/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9628/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9629/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9630/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9631/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9632/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9633/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9634/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9635/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9636/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9637/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9638/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9639/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9640/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9641/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9642/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9643/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9644/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9645/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9646/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9647/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9648/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9649/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9650/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9651/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9652/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9653/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9654/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9655/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9656/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9657/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9658/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9659/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9660/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9661/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9662/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9663/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9664/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9665/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9666/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9667/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9668/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9669/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9670/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9671/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9672/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9673/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9674/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9675/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9676/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9677/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9678/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9679/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9680/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9681/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9682/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9683/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9684/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9685/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9686/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9687/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9688/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9689/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9690/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9691/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9692/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9693/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9694/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9695/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9696/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9697/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9698/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9699/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9700/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9701/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9702/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9703/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9704/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9705/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9706/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9707/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9708/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9709/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9710/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9711/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9712/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9713/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9714/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9715/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9716/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9717/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9718/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9719/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9720/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9721/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9722/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9723/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9724/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9725/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9726/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9727/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9728/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9729/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9730/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9731/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9732/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9733/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9734/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9735/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9736/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9737/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9738/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9739/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9740/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9741/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9742/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9743/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9744/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9745/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9746/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9747/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9748/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9749/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9750/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9751/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9752/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9753/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9754/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9755/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9756/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9757/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9758/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9759/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9760/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9761/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9762/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9763/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9764/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9765/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9766/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9767/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9768/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9769/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9770/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9771/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9772/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9773/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9774/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9775/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9776/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9777/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9778/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9779/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9780/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9781/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9782/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9783/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9784/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9785/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9786/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9787/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9788/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9789/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9790/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9791/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9792/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9793/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9794/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9795/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9796/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9797/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9798/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9799/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9800/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9801/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9802/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9803/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9804/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9805/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9806/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9807/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9808/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9809/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9810/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9811/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9812/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9813/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9814/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9815/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9816/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9817/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9818/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9819/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9820/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9821/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9822/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9823/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9824/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9825/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9826/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9827/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9828/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9829/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9830/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9831/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9832/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9833/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9834/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9835/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9836/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9837/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9838/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9839/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9840/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9841/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9842/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9843/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9844/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9845/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9846/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9847/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9848/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9849/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9850/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9851/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9852/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9853/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9854/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9855/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9856/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9857/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9858/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9859/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9860/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9861/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9862/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9863/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9864/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9865/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9866/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9867/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9868/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9869/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9870/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9871/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9872/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9873/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9874/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9875/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9876/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9877/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9878/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9879/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9880/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9881/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9882/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9883/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9884/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9885/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9886/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9887/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9888/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9889/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9890/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9891/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9892/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9893/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9894/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9895/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9896/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9897/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9898/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9899/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9900/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9901/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9902/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9903/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9904/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9905/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9906/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9907/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9908/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9909/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9910/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9911/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9912/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9913/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9914/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9915/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9916/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9917/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9918/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9919/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9920/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9921/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9922/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9923/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9924/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9925/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9926/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9927/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9928/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9929/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9930/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9931/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9932/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9933/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9934/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9935/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9936/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9937/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9938/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9939/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9940/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9941/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9942/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9943/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9944/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9945/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9946/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9947/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9948/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9949/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9950/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9951/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9952/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9953/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9954/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9955/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9956/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9957/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9958/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9959/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9960/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9961/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9962/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9963/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9964/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9965/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9966/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9967/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9968/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9969/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9970/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9971/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9972/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9973/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9974/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9975/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9976/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9977/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9978/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9979/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9980/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9981/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9982/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9983/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9984/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9985/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9986/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9987/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9988/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9989/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9990/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9991/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9992/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9993/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9994/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9995/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9996/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9997/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9998/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9999/10000..  Training Loss: 0.150.. \n",
      "Epoch: 10000/10000..  Training Loss: 0.150.. \n",
      "Epoch: 1/10000..  Training Loss: 0.675.. \n",
      "Epoch: 2/10000..  Training Loss: 0.509.. \n",
      "Epoch: 3/10000..  Training Loss: 0.470.. \n",
      "Epoch: 4/10000..  Training Loss: 0.459.. \n",
      "Epoch: 5/10000..  Training Loss: 0.452.. \n",
      "Epoch: 6/10000..  Training Loss: 0.433.. \n",
      "Epoch: 7/10000..  Training Loss: 0.415.. \n",
      "Epoch: 8/10000..  Training Loss: 0.409.. \n",
      "Epoch: 9/10000..  Training Loss: 0.411.. \n",
      "Epoch: 10/10000..  Training Loss: 0.412.. \n",
      "Epoch: 11/10000..  Training Loss: 0.408.. \n",
      "Epoch: 12/10000..  Training Loss: 0.403.. \n",
      "Epoch: 13/10000..  Training Loss: 0.398.. \n",
      "Epoch: 14/10000..  Training Loss: 0.396.. \n",
      "Epoch: 15/10000..  Training Loss: 0.394.. \n",
      "Epoch: 16/10000..  Training Loss: 0.392.. \n",
      "Epoch: 17/10000..  Training Loss: 0.391.. \n",
      "Epoch: 18/10000..  Training Loss: 0.390.. \n",
      "Epoch: 19/10000..  Training Loss: 0.388.. \n",
      "Epoch: 20/10000..  Training Loss: 0.386.. \n",
      "Epoch: 21/10000..  Training Loss: 0.383.. \n",
      "Epoch: 22/10000..  Training Loss: 0.382.. \n",
      "Epoch: 23/10000..  Training Loss: 0.382.. \n",
      "Epoch: 24/10000..  Training Loss: 0.381.. \n",
      "Epoch: 25/10000..  Training Loss: 0.379.. \n",
      "Epoch: 26/10000..  Training Loss: 0.378.. \n",
      "Epoch: 27/10000..  Training Loss: 0.376.. \n",
      "Epoch: 28/10000..  Training Loss: 0.375.. \n",
      "Epoch: 29/10000..  Training Loss: 0.374.. \n",
      "Epoch: 30/10000..  Training Loss: 0.373.. \n",
      "Epoch: 31/10000..  Training Loss: 0.372.. \n",
      "Epoch: 32/10000..  Training Loss: 0.371.. \n",
      "Epoch: 33/10000..  Training Loss: 0.369.. \n",
      "Epoch: 34/10000..  Training Loss: 0.368.. \n",
      "Epoch: 35/10000..  Training Loss: 0.367.. \n",
      "Epoch: 36/10000..  Training Loss: 0.366.. \n",
      "Epoch: 37/10000..  Training Loss: 0.365.. \n",
      "Epoch: 38/10000..  Training Loss: 0.363.. \n",
      "Epoch: 39/10000..  Training Loss: 0.362.. \n",
      "Epoch: 40/10000..  Training Loss: 0.361.. \n",
      "Epoch: 41/10000..  Training Loss: 0.360.. \n",
      "Epoch: 42/10000..  Training Loss: 0.359.. \n",
      "Epoch: 43/10000..  Training Loss: 0.357.. \n",
      "Epoch: 44/10000..  Training Loss: 0.356.. \n",
      "Epoch: 45/10000..  Training Loss: 0.355.. \n",
      "Epoch: 46/10000..  Training Loss: 0.354.. \n",
      "Epoch: 47/10000..  Training Loss: 0.352.. \n",
      "Epoch: 48/10000..  Training Loss: 0.351.. \n",
      "Epoch: 49/10000..  Training Loss: 0.350.. \n",
      "Epoch: 50/10000..  Training Loss: 0.349.. \n",
      "Epoch: 51/10000..  Training Loss: 0.347.. \n",
      "Epoch: 52/10000..  Training Loss: 0.346.. \n",
      "Epoch: 53/10000..  Training Loss: 0.345.. \n",
      "Epoch: 54/10000..  Training Loss: 0.343.. \n",
      "Epoch: 55/10000..  Training Loss: 0.342.. \n",
      "Epoch: 56/10000..  Training Loss: 0.341.. \n",
      "Epoch: 57/10000..  Training Loss: 0.340.. \n",
      "Epoch: 58/10000..  Training Loss: 0.338.. \n",
      "Epoch: 59/10000..  Training Loss: 0.337.. \n",
      "Epoch: 60/10000..  Training Loss: 0.336.. \n",
      "Epoch: 61/10000..  Training Loss: 0.334.. \n",
      "Epoch: 62/10000..  Training Loss: 0.333.. \n",
      "Epoch: 63/10000..  Training Loss: 0.332.. \n",
      "Epoch: 64/10000..  Training Loss: 0.330.. \n",
      "Epoch: 65/10000..  Training Loss: 0.329.. \n",
      "Epoch: 66/10000..  Training Loss: 0.328.. \n",
      "Epoch: 67/10000..  Training Loss: 0.326.. \n",
      "Epoch: 68/10000..  Training Loss: 0.325.. \n",
      "Epoch: 69/10000..  Training Loss: 0.324.. \n",
      "Epoch: 70/10000..  Training Loss: 0.322.. \n",
      "Epoch: 71/10000..  Training Loss: 0.321.. \n",
      "Epoch: 72/10000..  Training Loss: 0.320.. \n",
      "Epoch: 73/10000..  Training Loss: 0.319.. \n",
      "Epoch: 74/10000..  Training Loss: 0.318.. \n",
      "Epoch: 75/10000..  Training Loss: 0.318.. \n",
      "Epoch: 76/10000..  Training Loss: 0.317.. \n",
      "Epoch: 77/10000..  Training Loss: 0.315.. \n",
      "Epoch: 78/10000..  Training Loss: 0.313.. \n",
      "Epoch: 79/10000..  Training Loss: 0.312.. \n",
      "Epoch: 80/10000..  Training Loss: 0.312.. \n",
      "Epoch: 81/10000..  Training Loss: 0.312.. \n",
      "Epoch: 82/10000..  Training Loss: 0.310.. \n",
      "Epoch: 83/10000..  Training Loss: 0.309.. \n",
      "Epoch: 84/10000..  Training Loss: 0.307.. \n",
      "Epoch: 85/10000..  Training Loss: 0.307.. \n",
      "Epoch: 86/10000..  Training Loss: 0.306.. \n",
      "Epoch: 87/10000..  Training Loss: 0.306.. \n",
      "Epoch: 88/10000..  Training Loss: 0.305.. \n",
      "Epoch: 89/10000..  Training Loss: 0.304.. \n",
      "Epoch: 90/10000..  Training Loss: 0.302.. \n",
      "Epoch: 91/10000..  Training Loss: 0.301.. \n",
      "Epoch: 92/10000..  Training Loss: 0.301.. \n",
      "Epoch: 93/10000..  Training Loss: 0.300.. \n",
      "Epoch: 94/10000..  Training Loss: 0.300.. \n",
      "Epoch: 95/10000..  Training Loss: 0.300.. \n",
      "Epoch: 96/10000..  Training Loss: 0.300.. \n",
      "Epoch: 97/10000..  Training Loss: 0.299.. \n",
      "Epoch: 98/10000..  Training Loss: 0.297.. \n",
      "Epoch: 99/10000..  Training Loss: 0.296.. \n",
      "Epoch: 100/10000..  Training Loss: 0.295.. \n",
      "Epoch: 101/10000..  Training Loss: 0.295.. \n",
      "Epoch: 102/10000..  Training Loss: 0.295.. \n",
      "Epoch: 103/10000..  Training Loss: 0.295.. \n",
      "Epoch: 104/10000..  Training Loss: 0.294.. \n",
      "Epoch: 105/10000..  Training Loss: 0.293.. \n",
      "Epoch: 106/10000..  Training Loss: 0.292.. \n",
      "Epoch: 107/10000..  Training Loss: 0.291.. \n",
      "Epoch: 108/10000..  Training Loss: 0.291.. \n",
      "Epoch: 109/10000..  Training Loss: 0.291.. \n",
      "Epoch: 110/10000..  Training Loss: 0.291.. \n",
      "Epoch: 111/10000..  Training Loss: 0.291.. \n",
      "Epoch: 112/10000..  Training Loss: 0.290.. \n",
      "Epoch: 113/10000..  Training Loss: 0.288.. \n",
      "Epoch: 114/10000..  Training Loss: 0.287.. \n",
      "Epoch: 115/10000..  Training Loss: 0.287.. \n",
      "Epoch: 116/10000..  Training Loss: 0.287.. \n",
      "Epoch: 117/10000..  Training Loss: 0.287.. \n",
      "Epoch: 118/10000..  Training Loss: 0.287.. \n",
      "Epoch: 119/10000..  Training Loss: 0.287.. \n",
      "Epoch: 120/10000..  Training Loss: 0.286.. \n",
      "Epoch: 121/10000..  Training Loss: 0.284.. \n",
      "Epoch: 122/10000..  Training Loss: 0.284.. \n",
      "Epoch: 123/10000..  Training Loss: 0.283.. \n",
      "Epoch: 124/10000..  Training Loss: 0.283.. \n",
      "Epoch: 125/10000..  Training Loss: 0.284.. \n",
      "Epoch: 126/10000..  Training Loss: 0.284.. \n",
      "Epoch: 127/10000..  Training Loss: 0.284.. \n",
      "Epoch: 128/10000..  Training Loss: 0.284.. \n",
      "Epoch: 129/10000..  Training Loss: 0.282.. \n",
      "Epoch: 130/10000..  Training Loss: 0.281.. \n",
      "Epoch: 131/10000..  Training Loss: 0.280.. \n",
      "Epoch: 132/10000..  Training Loss: 0.280.. \n",
      "Epoch: 133/10000..  Training Loss: 0.280.. \n",
      "Epoch: 134/10000..  Training Loss: 0.281.. \n",
      "Epoch: 135/10000..  Training Loss: 0.281.. \n",
      "Epoch: 136/10000..  Training Loss: 0.282.. \n",
      "Epoch: 137/10000..  Training Loss: 0.281.. \n",
      "Epoch: 138/10000..  Training Loss: 0.280.. \n",
      "Epoch: 139/10000..  Training Loss: 0.278.. \n",
      "Epoch: 140/10000..  Training Loss: 0.277.. \n",
      "Epoch: 141/10000..  Training Loss: 0.277.. \n",
      "Epoch: 142/10000..  Training Loss: 0.278.. \n",
      "Epoch: 143/10000..  Training Loss: 0.278.. \n",
      "Epoch: 144/10000..  Training Loss: 0.278.. \n",
      "Epoch: 145/10000..  Training Loss: 0.277.. \n",
      "Epoch: 146/10000..  Training Loss: 0.276.. \n",
      "Epoch: 147/10000..  Training Loss: 0.275.. \n",
      "Epoch: 148/10000..  Training Loss: 0.274.. \n",
      "Epoch: 149/10000..  Training Loss: 0.275.. \n",
      "Epoch: 150/10000..  Training Loss: 0.275.. \n",
      "Epoch: 151/10000..  Training Loss: 0.275.. \n",
      "Epoch: 152/10000..  Training Loss: 0.275.. \n",
      "Epoch: 153/10000..  Training Loss: 0.274.. \n",
      "Epoch: 154/10000..  Training Loss: 0.274.. \n",
      "Epoch: 155/10000..  Training Loss: 0.273.. \n",
      "Epoch: 156/10000..  Training Loss: 0.272.. \n",
      "Epoch: 157/10000..  Training Loss: 0.272.. \n",
      "Epoch: 158/10000..  Training Loss: 0.271.. \n",
      "Epoch: 159/10000..  Training Loss: 0.271.. \n",
      "Epoch: 160/10000..  Training Loss: 0.271.. \n",
      "Epoch: 161/10000..  Training Loss: 0.271.. \n",
      "Epoch: 162/10000..  Training Loss: 0.272.. \n",
      "Epoch: 163/10000..  Training Loss: 0.273.. \n",
      "Epoch: 164/10000..  Training Loss: 0.274.. \n",
      "Epoch: 165/10000..  Training Loss: 0.277.. \n",
      "Epoch: 166/10000..  Training Loss: 0.277.. \n",
      "Epoch: 167/10000..  Training Loss: 0.274.. \n",
      "Epoch: 168/10000..  Training Loss: 0.270.. \n",
      "Epoch: 169/10000..  Training Loss: 0.269.. \n",
      "Epoch: 170/10000..  Training Loss: 0.271.. \n",
      "Epoch: 171/10000..  Training Loss: 0.273.. \n",
      "Epoch: 172/10000..  Training Loss: 0.272.. \n",
      "Epoch: 173/10000..  Training Loss: 0.268.. \n",
      "Epoch: 174/10000..  Training Loss: 0.268.. \n",
      "Epoch: 175/10000..  Training Loss: 0.269.. \n",
      "Epoch: 176/10000..  Training Loss: 0.270.. \n",
      "Epoch: 177/10000..  Training Loss: 0.269.. \n",
      "Epoch: 178/10000..  Training Loss: 0.267.. \n",
      "Epoch: 179/10000..  Training Loss: 0.267.. \n",
      "Epoch: 180/10000..  Training Loss: 0.268.. \n",
      "Epoch: 181/10000..  Training Loss: 0.268.. \n",
      "Epoch: 182/10000..  Training Loss: 0.268.. \n",
      "Epoch: 183/10000..  Training Loss: 0.266.. \n",
      "Epoch: 184/10000..  Training Loss: 0.265.. \n",
      "Epoch: 185/10000..  Training Loss: 0.265.. \n",
      "Epoch: 186/10000..  Training Loss: 0.266.. \n",
      "Epoch: 187/10000..  Training Loss: 0.266.. \n",
      "Epoch: 188/10000..  Training Loss: 0.266.. \n",
      "Epoch: 189/10000..  Training Loss: 0.265.. \n",
      "Epoch: 190/10000..  Training Loss: 0.264.. \n",
      "Epoch: 191/10000..  Training Loss: 0.264.. \n",
      "Epoch: 192/10000..  Training Loss: 0.264.. \n",
      "Epoch: 193/10000..  Training Loss: 0.264.. \n",
      "Epoch: 194/10000..  Training Loss: 0.264.. \n",
      "Epoch: 195/10000..  Training Loss: 0.264.. \n",
      "Epoch: 196/10000..  Training Loss: 0.264.. \n",
      "Epoch: 197/10000..  Training Loss: 0.263.. \n",
      "Epoch: 198/10000..  Training Loss: 0.263.. \n",
      "Epoch: 199/10000..  Training Loss: 0.262.. \n",
      "Epoch: 200/10000..  Training Loss: 0.262.. \n",
      "Epoch: 201/10000..  Training Loss: 0.262.. \n",
      "Epoch: 202/10000..  Training Loss: 0.261.. \n",
      "Epoch: 203/10000..  Training Loss: 0.261.. \n",
      "Epoch: 204/10000..  Training Loss: 0.261.. \n",
      "Epoch: 205/10000..  Training Loss: 0.261.. \n",
      "Epoch: 206/10000..  Training Loss: 0.261.. \n",
      "Epoch: 207/10000..  Training Loss: 0.261.. \n",
      "Epoch: 208/10000..  Training Loss: 0.262.. \n",
      "Epoch: 209/10000..  Training Loss: 0.263.. \n",
      "Epoch: 210/10000..  Training Loss: 0.267.. \n",
      "Epoch: 211/10000..  Training Loss: 0.272.. \n",
      "Epoch: 212/10000..  Training Loss: 0.273.. \n",
      "Epoch: 213/10000..  Training Loss: 0.268.. \n",
      "Epoch: 214/10000..  Training Loss: 0.260.. \n",
      "Epoch: 215/10000..  Training Loss: 0.261.. \n",
      "Epoch: 216/10000..  Training Loss: 0.267.. \n",
      "Epoch: 217/10000..  Training Loss: 0.267.. \n",
      "Epoch: 218/10000..  Training Loss: 0.261.. \n",
      "Epoch: 219/10000..  Training Loss: 0.259.. \n",
      "Epoch: 220/10000..  Training Loss: 0.263.. \n",
      "Epoch: 221/10000..  Training Loss: 0.265.. \n",
      "Epoch: 222/10000..  Training Loss: 0.261.. \n",
      "Epoch: 223/10000..  Training Loss: 0.258.. \n",
      "Epoch: 224/10000..  Training Loss: 0.260.. \n",
      "Epoch: 225/10000..  Training Loss: 0.262.. \n",
      "Epoch: 226/10000..  Training Loss: 0.260.. \n",
      "Epoch: 227/10000..  Training Loss: 0.257.. \n",
      "Epoch: 228/10000..  Training Loss: 0.259.. \n",
      "Epoch: 229/10000..  Training Loss: 0.260.. \n",
      "Epoch: 230/10000..  Training Loss: 0.259.. \n",
      "Epoch: 231/10000..  Training Loss: 0.257.. \n",
      "Epoch: 232/10000..  Training Loss: 0.257.. \n",
      "Epoch: 233/10000..  Training Loss: 0.258.. \n",
      "Epoch: 234/10000..  Training Loss: 0.258.. \n",
      "Epoch: 235/10000..  Training Loss: 0.257.. \n",
      "Epoch: 236/10000..  Training Loss: 0.256.. \n",
      "Epoch: 237/10000..  Training Loss: 0.257.. \n",
      "Epoch: 238/10000..  Training Loss: 0.257.. \n",
      "Epoch: 239/10000..  Training Loss: 0.256.. \n",
      "Epoch: 240/10000..  Training Loss: 0.255.. \n",
      "Epoch: 241/10000..  Training Loss: 0.255.. \n",
      "Epoch: 242/10000..  Training Loss: 0.255.. \n",
      "Epoch: 243/10000..  Training Loss: 0.256.. \n",
      "Epoch: 244/10000..  Training Loss: 0.255.. \n",
      "Epoch: 245/10000..  Training Loss: 0.255.. \n",
      "Epoch: 246/10000..  Training Loss: 0.254.. \n",
      "Epoch: 247/10000..  Training Loss: 0.254.. \n",
      "Epoch: 248/10000..  Training Loss: 0.254.. \n",
      "Epoch: 249/10000..  Training Loss: 0.254.. \n",
      "Epoch: 250/10000..  Training Loss: 0.254.. \n",
      "Epoch: 251/10000..  Training Loss: 0.254.. \n",
      "Epoch: 252/10000..  Training Loss: 0.253.. \n",
      "Epoch: 253/10000..  Training Loss: 0.253.. \n",
      "Epoch: 254/10000..  Training Loss: 0.253.. \n",
      "Epoch: 255/10000..  Training Loss: 0.253.. \n",
      "Epoch: 256/10000..  Training Loss: 0.253.. \n",
      "Epoch: 257/10000..  Training Loss: 0.253.. \n",
      "Epoch: 258/10000..  Training Loss: 0.253.. \n",
      "Epoch: 259/10000..  Training Loss: 0.253.. \n",
      "Epoch: 260/10000..  Training Loss: 0.252.. \n",
      "Epoch: 261/10000..  Training Loss: 0.252.. \n",
      "Epoch: 262/10000..  Training Loss: 0.251.. \n",
      "Epoch: 263/10000..  Training Loss: 0.251.. \n",
      "Epoch: 264/10000..  Training Loss: 0.251.. \n",
      "Epoch: 265/10000..  Training Loss: 0.251.. \n",
      "Epoch: 266/10000..  Training Loss: 0.251.. \n",
      "Epoch: 267/10000..  Training Loss: 0.251.. \n",
      "Epoch: 268/10000..  Training Loss: 0.251.. \n",
      "Epoch: 269/10000..  Training Loss: 0.251.. \n",
      "Epoch: 270/10000..  Training Loss: 0.251.. \n",
      "Epoch: 271/10000..  Training Loss: 0.252.. \n",
      "Epoch: 272/10000..  Training Loss: 0.254.. \n",
      "Epoch: 273/10000..  Training Loss: 0.257.. \n",
      "Epoch: 274/10000..  Training Loss: 0.259.. \n",
      "Epoch: 275/10000..  Training Loss: 0.259.. \n",
      "Epoch: 276/10000..  Training Loss: 0.255.. \n",
      "Epoch: 277/10000..  Training Loss: 0.251.. \n",
      "Epoch: 278/10000..  Training Loss: 0.250.. \n",
      "Epoch: 279/10000..  Training Loss: 0.252.. \n",
      "Epoch: 280/10000..  Training Loss: 0.256.. \n",
      "Epoch: 281/10000..  Training Loss: 0.256.. \n",
      "Epoch: 282/10000..  Training Loss: 0.252.. \n",
      "Epoch: 283/10000..  Training Loss: 0.249.. \n",
      "Epoch: 284/10000..  Training Loss: 0.249.. \n",
      "Epoch: 285/10000..  Training Loss: 0.251.. \n",
      "Epoch: 286/10000..  Training Loss: 0.252.. \n",
      "Epoch: 287/10000..  Training Loss: 0.251.. \n",
      "Epoch: 288/10000..  Training Loss: 0.249.. \n",
      "Epoch: 289/10000..  Training Loss: 0.248.. \n",
      "Epoch: 290/10000..  Training Loss: 0.248.. \n",
      "Epoch: 291/10000..  Training Loss: 0.249.. \n",
      "Epoch: 292/10000..  Training Loss: 0.250.. \n",
      "Epoch: 293/10000..  Training Loss: 0.250.. \n",
      "Epoch: 294/10000..  Training Loss: 0.248.. \n",
      "Epoch: 295/10000..  Training Loss: 0.247.. \n",
      "Epoch: 296/10000..  Training Loss: 0.247.. \n",
      "Epoch: 297/10000..  Training Loss: 0.247.. \n",
      "Epoch: 298/10000..  Training Loss: 0.248.. \n",
      "Epoch: 299/10000..  Training Loss: 0.248.. \n",
      "Epoch: 300/10000..  Training Loss: 0.248.. \n",
      "Epoch: 301/10000..  Training Loss: 0.247.. \n",
      "Epoch: 302/10000..  Training Loss: 0.246.. \n",
      "Epoch: 303/10000..  Training Loss: 0.246.. \n",
      "Epoch: 304/10000..  Training Loss: 0.246.. \n",
      "Epoch: 305/10000..  Training Loss: 0.246.. \n",
      "Epoch: 306/10000..  Training Loss: 0.246.. \n",
      "Epoch: 307/10000..  Training Loss: 0.246.. \n",
      "Epoch: 308/10000..  Training Loss: 0.246.. \n",
      "Epoch: 309/10000..  Training Loss: 0.246.. \n",
      "Epoch: 310/10000..  Training Loss: 0.246.. \n",
      "Epoch: 311/10000..  Training Loss: 0.246.. \n",
      "Epoch: 312/10000..  Training Loss: 0.246.. \n",
      "Epoch: 313/10000..  Training Loss: 0.245.. \n",
      "Epoch: 314/10000..  Training Loss: 0.245.. \n",
      "Epoch: 315/10000..  Training Loss: 0.244.. \n",
      "Epoch: 316/10000..  Training Loss: 0.244.. \n",
      "Epoch: 317/10000..  Training Loss: 0.244.. \n",
      "Epoch: 318/10000..  Training Loss: 0.243.. \n",
      "Epoch: 319/10000..  Training Loss: 0.243.. \n",
      "Epoch: 320/10000..  Training Loss: 0.243.. \n",
      "Epoch: 321/10000..  Training Loss: 0.243.. \n",
      "Epoch: 322/10000..  Training Loss: 0.243.. \n",
      "Epoch: 323/10000..  Training Loss: 0.243.. \n",
      "Epoch: 324/10000..  Training Loss: 0.244.. \n",
      "Epoch: 325/10000..  Training Loss: 0.247.. \n",
      "Epoch: 326/10000..  Training Loss: 0.252.. \n",
      "Epoch: 327/10000..  Training Loss: 0.261.. \n",
      "Epoch: 328/10000..  Training Loss: 0.263.. \n",
      "Epoch: 329/10000..  Training Loss: 0.257.. \n",
      "Epoch: 330/10000..  Training Loss: 0.244.. \n",
      "Epoch: 331/10000..  Training Loss: 0.246.. \n",
      "Epoch: 332/10000..  Training Loss: 0.256.. \n",
      "Epoch: 333/10000..  Training Loss: 0.255.. \n",
      "Epoch: 334/10000..  Training Loss: 0.246.. \n",
      "Epoch: 335/10000..  Training Loss: 0.243.. \n",
      "Epoch: 336/10000..  Training Loss: 0.251.. \n",
      "Epoch: 337/10000..  Training Loss: 0.254.. \n",
      "Epoch: 338/10000..  Training Loss: 0.245.. \n",
      "Epoch: 339/10000..  Training Loss: 0.243.. \n",
      "Epoch: 340/10000..  Training Loss: 0.249.. \n",
      "Epoch: 341/10000..  Training Loss: 0.249.. \n",
      "Epoch: 342/10000..  Training Loss: 0.243.. \n",
      "Epoch: 343/10000..  Training Loss: 0.242.. \n",
      "Epoch: 344/10000..  Training Loss: 0.247.. \n",
      "Epoch: 345/10000..  Training Loss: 0.246.. \n",
      "Epoch: 346/10000..  Training Loss: 0.242.. \n",
      "Epoch: 347/10000..  Training Loss: 0.242.. \n",
      "Epoch: 348/10000..  Training Loss: 0.244.. \n",
      "Epoch: 349/10000..  Training Loss: 0.243.. \n",
      "Epoch: 350/10000..  Training Loss: 0.241.. \n",
      "Epoch: 351/10000..  Training Loss: 0.241.. \n",
      "Epoch: 352/10000..  Training Loss: 0.243.. \n",
      "Epoch: 353/10000..  Training Loss: 0.241.. \n",
      "Epoch: 354/10000..  Training Loss: 0.240.. \n",
      "Epoch: 355/10000..  Training Loss: 0.241.. \n",
      "Epoch: 356/10000..  Training Loss: 0.241.. \n",
      "Epoch: 357/10000..  Training Loss: 0.240.. \n",
      "Epoch: 358/10000..  Training Loss: 0.240.. \n",
      "Epoch: 359/10000..  Training Loss: 0.239.. \n",
      "Epoch: 360/10000..  Training Loss: 0.240.. \n",
      "Epoch: 361/10000..  Training Loss: 0.240.. \n",
      "Epoch: 362/10000..  Training Loss: 0.239.. \n",
      "Epoch: 363/10000..  Training Loss: 0.239.. \n",
      "Epoch: 364/10000..  Training Loss: 0.239.. \n",
      "Epoch: 365/10000..  Training Loss: 0.239.. \n",
      "Epoch: 366/10000..  Training Loss: 0.239.. \n",
      "Epoch: 367/10000..  Training Loss: 0.238.. \n",
      "Epoch: 368/10000..  Training Loss: 0.238.. \n",
      "Epoch: 369/10000..  Training Loss: 0.238.. \n",
      "Epoch: 370/10000..  Training Loss: 0.238.. \n",
      "Epoch: 371/10000..  Training Loss: 0.238.. \n",
      "Epoch: 372/10000..  Training Loss: 0.237.. \n",
      "Epoch: 373/10000..  Training Loss: 0.237.. \n",
      "Epoch: 374/10000..  Training Loss: 0.237.. \n",
      "Epoch: 375/10000..  Training Loss: 0.237.. \n",
      "Epoch: 376/10000..  Training Loss: 0.237.. \n",
      "Epoch: 377/10000..  Training Loss: 0.237.. \n",
      "Epoch: 378/10000..  Training Loss: 0.237.. \n",
      "Epoch: 379/10000..  Training Loss: 0.237.. \n",
      "Epoch: 380/10000..  Training Loss: 0.236.. \n",
      "Epoch: 381/10000..  Training Loss: 0.236.. \n",
      "Epoch: 382/10000..  Training Loss: 0.236.. \n",
      "Epoch: 383/10000..  Training Loss: 0.236.. \n",
      "Epoch: 384/10000..  Training Loss: 0.236.. \n",
      "Epoch: 385/10000..  Training Loss: 0.236.. \n",
      "Epoch: 386/10000..  Training Loss: 0.236.. \n",
      "Epoch: 387/10000..  Training Loss: 0.236.. \n",
      "Epoch: 388/10000..  Training Loss: 0.235.. \n",
      "Epoch: 389/10000..  Training Loss: 0.235.. \n",
      "Epoch: 390/10000..  Training Loss: 0.235.. \n",
      "Epoch: 391/10000..  Training Loss: 0.235.. \n",
      "Epoch: 392/10000..  Training Loss: 0.235.. \n",
      "Epoch: 393/10000..  Training Loss: 0.235.. \n",
      "Epoch: 394/10000..  Training Loss: 0.235.. \n",
      "Epoch: 395/10000..  Training Loss: 0.235.. \n",
      "Epoch: 396/10000..  Training Loss: 0.235.. \n",
      "Epoch: 397/10000..  Training Loss: 0.235.. \n",
      "Epoch: 398/10000..  Training Loss: 0.235.. \n",
      "Epoch: 399/10000..  Training Loss: 0.235.. \n",
      "Epoch: 400/10000..  Training Loss: 0.235.. \n",
      "Epoch: 401/10000..  Training Loss: 0.235.. \n",
      "Epoch: 402/10000..  Training Loss: 0.236.. \n",
      "Epoch: 403/10000..  Training Loss: 0.236.. \n",
      "Epoch: 404/10000..  Training Loss: 0.237.. \n",
      "Epoch: 405/10000..  Training Loss: 0.237.. \n",
      "Epoch: 406/10000..  Training Loss: 0.237.. \n",
      "Epoch: 407/10000..  Training Loss: 0.236.. \n",
      "Epoch: 408/10000..  Training Loss: 0.236.. \n",
      "Epoch: 409/10000..  Training Loss: 0.235.. \n",
      "Epoch: 410/10000..  Training Loss: 0.234.. \n",
      "Epoch: 411/10000..  Training Loss: 0.233.. \n",
      "Epoch: 412/10000..  Training Loss: 0.233.. \n",
      "Epoch: 413/10000..  Training Loss: 0.233.. \n",
      "Epoch: 414/10000..  Training Loss: 0.233.. \n",
      "Epoch: 415/10000..  Training Loss: 0.233.. \n",
      "Epoch: 416/10000..  Training Loss: 0.233.. \n",
      "Epoch: 417/10000..  Training Loss: 0.233.. \n",
      "Epoch: 418/10000..  Training Loss: 0.233.. \n",
      "Epoch: 419/10000..  Training Loss: 0.234.. \n",
      "Epoch: 420/10000..  Training Loss: 0.235.. \n",
      "Epoch: 421/10000..  Training Loss: 0.237.. \n",
      "Epoch: 422/10000..  Training Loss: 0.240.. \n",
      "Epoch: 423/10000..  Training Loss: 0.244.. \n",
      "Epoch: 424/10000..  Training Loss: 0.242.. \n",
      "Epoch: 425/10000..  Training Loss: 0.239.. \n",
      "Epoch: 426/10000..  Training Loss: 0.234.. \n",
      "Epoch: 427/10000..  Training Loss: 0.232.. \n",
      "Epoch: 428/10000..  Training Loss: 0.233.. \n",
      "Epoch: 429/10000..  Training Loss: 0.237.. \n",
      "Epoch: 430/10000..  Training Loss: 0.239.. \n",
      "Epoch: 431/10000..  Training Loss: 0.238.. \n",
      "Epoch: 432/10000..  Training Loss: 0.235.. \n",
      "Epoch: 433/10000..  Training Loss: 0.232.. \n",
      "Epoch: 434/10000..  Training Loss: 0.231.. \n",
      "Epoch: 435/10000..  Training Loss: 0.232.. \n",
      "Epoch: 436/10000..  Training Loss: 0.234.. \n",
      "Epoch: 437/10000..  Training Loss: 0.236.. \n",
      "Epoch: 438/10000..  Training Loss: 0.235.. \n",
      "Epoch: 439/10000..  Training Loss: 0.233.. \n",
      "Epoch: 440/10000..  Training Loss: 0.231.. \n",
      "Epoch: 441/10000..  Training Loss: 0.230.. \n",
      "Epoch: 442/10000..  Training Loss: 0.231.. \n",
      "Epoch: 443/10000..  Training Loss: 0.233.. \n",
      "Epoch: 444/10000..  Training Loss: 0.234.. \n",
      "Epoch: 445/10000..  Training Loss: 0.233.. \n",
      "Epoch: 446/10000..  Training Loss: 0.232.. \n",
      "Epoch: 447/10000..  Training Loss: 0.230.. \n",
      "Epoch: 448/10000..  Training Loss: 0.230.. \n",
      "Epoch: 449/10000..  Training Loss: 0.230.. \n",
      "Epoch: 450/10000..  Training Loss: 0.230.. \n",
      "Epoch: 451/10000..  Training Loss: 0.231.. \n",
      "Epoch: 452/10000..  Training Loss: 0.232.. \n",
      "Epoch: 453/10000..  Training Loss: 0.232.. \n",
      "Epoch: 454/10000..  Training Loss: 0.231.. \n",
      "Epoch: 455/10000..  Training Loss: 0.230.. \n",
      "Epoch: 456/10000..  Training Loss: 0.229.. \n",
      "Epoch: 457/10000..  Training Loss: 0.229.. \n",
      "Epoch: 458/10000..  Training Loss: 0.229.. \n",
      "Epoch: 459/10000..  Training Loss: 0.229.. \n",
      "Epoch: 460/10000..  Training Loss: 0.230.. \n",
      "Epoch: 461/10000..  Training Loss: 0.230.. \n",
      "Epoch: 462/10000..  Training Loss: 0.231.. \n",
      "Epoch: 463/10000..  Training Loss: 0.231.. \n",
      "Epoch: 464/10000..  Training Loss: 0.231.. \n",
      "Epoch: 465/10000..  Training Loss: 0.231.. \n",
      "Epoch: 466/10000..  Training Loss: 0.230.. \n",
      "Epoch: 467/10000..  Training Loss: 0.229.. \n",
      "Epoch: 468/10000..  Training Loss: 0.228.. \n",
      "Epoch: 469/10000..  Training Loss: 0.228.. \n",
      "Epoch: 470/10000..  Training Loss: 0.227.. \n",
      "Epoch: 471/10000..  Training Loss: 0.228.. \n",
      "Epoch: 472/10000..  Training Loss: 0.228.. \n",
      "Epoch: 473/10000..  Training Loss: 0.229.. \n",
      "Epoch: 474/10000..  Training Loss: 0.230.. \n",
      "Epoch: 475/10000..  Training Loss: 0.231.. \n",
      "Epoch: 476/10000..  Training Loss: 0.232.. \n",
      "Epoch: 477/10000..  Training Loss: 0.234.. \n",
      "Epoch: 478/10000..  Training Loss: 0.233.. \n",
      "Epoch: 479/10000..  Training Loss: 0.231.. \n",
      "Epoch: 480/10000..  Training Loss: 0.228.. \n",
      "Epoch: 481/10000..  Training Loss: 0.227.. \n",
      "Epoch: 482/10000..  Training Loss: 0.227.. \n",
      "Epoch: 483/10000..  Training Loss: 0.228.. \n",
      "Epoch: 484/10000..  Training Loss: 0.229.. \n",
      "Epoch: 485/10000..  Training Loss: 0.231.. \n",
      "Epoch: 486/10000..  Training Loss: 0.233.. \n",
      "Epoch: 487/10000..  Training Loss: 0.232.. \n",
      "Epoch: 488/10000..  Training Loss: 0.231.. \n",
      "Epoch: 489/10000..  Training Loss: 0.228.. \n",
      "Epoch: 490/10000..  Training Loss: 0.226.. \n",
      "Epoch: 491/10000..  Training Loss: 0.226.. \n",
      "Epoch: 492/10000..  Training Loss: 0.227.. \n",
      "Epoch: 493/10000..  Training Loss: 0.229.. \n",
      "Epoch: 494/10000..  Training Loss: 0.230.. \n",
      "Epoch: 495/10000..  Training Loss: 0.232.. \n",
      "Epoch: 496/10000..  Training Loss: 0.231.. \n",
      "Epoch: 497/10000..  Training Loss: 0.229.. \n",
      "Epoch: 498/10000..  Training Loss: 0.227.. \n",
      "Epoch: 499/10000..  Training Loss: 0.225.. \n",
      "Epoch: 500/10000..  Training Loss: 0.225.. \n",
      "Epoch: 501/10000..  Training Loss: 0.226.. \n",
      "Epoch: 502/10000..  Training Loss: 0.228.. \n",
      "Epoch: 503/10000..  Training Loss: 0.229.. \n",
      "Epoch: 504/10000..  Training Loss: 0.229.. \n",
      "Epoch: 505/10000..  Training Loss: 0.228.. \n",
      "Epoch: 506/10000..  Training Loss: 0.227.. \n",
      "Epoch: 507/10000..  Training Loss: 0.225.. \n",
      "Epoch: 508/10000..  Training Loss: 0.224.. \n",
      "Epoch: 509/10000..  Training Loss: 0.224.. \n",
      "Epoch: 510/10000..  Training Loss: 0.225.. \n",
      "Epoch: 511/10000..  Training Loss: 0.226.. \n",
      "Epoch: 512/10000..  Training Loss: 0.227.. \n",
      "Epoch: 513/10000..  Training Loss: 0.228.. \n",
      "Epoch: 514/10000..  Training Loss: 0.228.. \n",
      "Epoch: 515/10000..  Training Loss: 0.228.. \n",
      "Epoch: 516/10000..  Training Loss: 0.227.. \n",
      "Epoch: 517/10000..  Training Loss: 0.225.. \n",
      "Epoch: 518/10000..  Training Loss: 0.224.. \n",
      "Epoch: 519/10000..  Training Loss: 0.224.. \n",
      "Epoch: 520/10000..  Training Loss: 0.224.. \n",
      "Epoch: 521/10000..  Training Loss: 0.225.. \n",
      "Epoch: 522/10000..  Training Loss: 0.226.. \n",
      "Epoch: 523/10000..  Training Loss: 0.226.. \n",
      "Epoch: 524/10000..  Training Loss: 0.227.. \n",
      "Epoch: 525/10000..  Training Loss: 0.226.. \n",
      "Epoch: 526/10000..  Training Loss: 0.226.. \n",
      "Epoch: 527/10000..  Training Loss: 0.224.. \n",
      "Epoch: 528/10000..  Training Loss: 0.224.. \n",
      "Epoch: 529/10000..  Training Loss: 0.223.. \n",
      "Epoch: 530/10000..  Training Loss: 0.223.. \n",
      "Epoch: 531/10000..  Training Loss: 0.223.. \n",
      "Epoch: 532/10000..  Training Loss: 0.223.. \n",
      "Epoch: 533/10000..  Training Loss: 0.223.. \n",
      "Epoch: 534/10000..  Training Loss: 0.224.. \n",
      "Epoch: 535/10000..  Training Loss: 0.225.. \n",
      "Epoch: 536/10000..  Training Loss: 0.225.. \n",
      "Epoch: 537/10000..  Training Loss: 0.226.. \n",
      "Epoch: 538/10000..  Training Loss: 0.226.. \n",
      "Epoch: 539/10000..  Training Loss: 0.226.. \n",
      "Epoch: 540/10000..  Training Loss: 0.225.. \n",
      "Epoch: 541/10000..  Training Loss: 0.224.. \n",
      "Epoch: 542/10000..  Training Loss: 0.223.. \n",
      "Epoch: 543/10000..  Training Loss: 0.222.. \n",
      "Epoch: 544/10000..  Training Loss: 0.222.. \n",
      "Epoch: 545/10000..  Training Loss: 0.221.. \n",
      "Epoch: 546/10000..  Training Loss: 0.221.. \n",
      "Epoch: 547/10000..  Training Loss: 0.222.. \n",
      "Epoch: 548/10000..  Training Loss: 0.222.. \n",
      "Epoch: 549/10000..  Training Loss: 0.223.. \n",
      "Epoch: 550/10000..  Training Loss: 0.224.. \n",
      "Epoch: 551/10000..  Training Loss: 0.226.. \n",
      "Epoch: 552/10000..  Training Loss: 0.228.. \n",
      "Epoch: 553/10000..  Training Loss: 0.228.. \n",
      "Epoch: 554/10000..  Training Loss: 0.229.. \n",
      "Epoch: 555/10000..  Training Loss: 0.225.. \n",
      "Epoch: 556/10000..  Training Loss: 0.223.. \n",
      "Epoch: 557/10000..  Training Loss: 0.221.. \n",
      "Epoch: 558/10000..  Training Loss: 0.221.. \n",
      "Epoch: 559/10000..  Training Loss: 0.222.. \n",
      "Epoch: 560/10000..  Training Loss: 0.224.. \n",
      "Epoch: 561/10000..  Training Loss: 0.226.. \n",
      "Epoch: 562/10000..  Training Loss: 0.227.. \n",
      "Epoch: 563/10000..  Training Loss: 0.227.. \n",
      "Epoch: 564/10000..  Training Loss: 0.224.. \n",
      "Epoch: 565/10000..  Training Loss: 0.221.. \n",
      "Epoch: 566/10000..  Training Loss: 0.220.. \n",
      "Epoch: 567/10000..  Training Loss: 0.221.. \n",
      "Epoch: 568/10000..  Training Loss: 0.223.. \n",
      "Epoch: 569/10000..  Training Loss: 0.225.. \n",
      "Epoch: 570/10000..  Training Loss: 0.226.. \n",
      "Epoch: 571/10000..  Training Loss: 0.225.. \n",
      "Epoch: 572/10000..  Training Loss: 0.223.. \n",
      "Epoch: 573/10000..  Training Loss: 0.221.. \n",
      "Epoch: 574/10000..  Training Loss: 0.220.. \n",
      "Epoch: 575/10000..  Training Loss: 0.220.. \n",
      "Epoch: 576/10000..  Training Loss: 0.222.. \n",
      "Epoch: 577/10000..  Training Loss: 0.224.. \n",
      "Epoch: 578/10000..  Training Loss: 0.224.. \n",
      "Epoch: 579/10000..  Training Loss: 0.224.. \n",
      "Epoch: 580/10000..  Training Loss: 0.222.. \n",
      "Epoch: 581/10000..  Training Loss: 0.220.. \n",
      "Epoch: 582/10000..  Training Loss: 0.219.. \n",
      "Epoch: 583/10000..  Training Loss: 0.219.. \n",
      "Epoch: 584/10000..  Training Loss: 0.220.. \n",
      "Epoch: 585/10000..  Training Loss: 0.221.. \n",
      "Epoch: 586/10000..  Training Loss: 0.223.. \n",
      "Epoch: 587/10000..  Training Loss: 0.222.. \n",
      "Epoch: 588/10000..  Training Loss: 0.222.. \n",
      "Epoch: 589/10000..  Training Loss: 0.220.. \n",
      "Epoch: 590/10000..  Training Loss: 0.219.. \n",
      "Epoch: 591/10000..  Training Loss: 0.218.. \n",
      "Epoch: 592/10000..  Training Loss: 0.219.. \n",
      "Epoch: 593/10000..  Training Loss: 0.219.. \n",
      "Epoch: 594/10000..  Training Loss: 0.219.. \n",
      "Epoch: 595/10000..  Training Loss: 0.220.. \n",
      "Epoch: 596/10000..  Training Loss: 0.221.. \n",
      "Epoch: 597/10000..  Training Loss: 0.221.. \n",
      "Epoch: 598/10000..  Training Loss: 0.221.. \n",
      "Epoch: 599/10000..  Training Loss: 0.220.. \n",
      "Epoch: 600/10000..  Training Loss: 0.219.. \n",
      "Epoch: 601/10000..  Training Loss: 0.218.. \n",
      "Epoch: 602/10000..  Training Loss: 0.218.. \n",
      "Epoch: 603/10000..  Training Loss: 0.218.. \n",
      "Epoch: 604/10000..  Training Loss: 0.218.. \n",
      "Epoch: 605/10000..  Training Loss: 0.218.. \n",
      "Epoch: 606/10000..  Training Loss: 0.218.. \n",
      "Epoch: 607/10000..  Training Loss: 0.218.. \n",
      "Epoch: 608/10000..  Training Loss: 0.219.. \n",
      "Epoch: 609/10000..  Training Loss: 0.220.. \n",
      "Epoch: 610/10000..  Training Loss: 0.221.. \n",
      "Epoch: 611/10000..  Training Loss: 0.222.. \n",
      "Epoch: 612/10000..  Training Loss: 0.223.. \n",
      "Epoch: 613/10000..  Training Loss: 0.222.. \n",
      "Epoch: 614/10000..  Training Loss: 0.222.. \n",
      "Epoch: 615/10000..  Training Loss: 0.220.. \n",
      "Epoch: 616/10000..  Training Loss: 0.219.. \n",
      "Epoch: 617/10000..  Training Loss: 0.217.. \n",
      "Epoch: 618/10000..  Training Loss: 0.217.. \n",
      "Epoch: 619/10000..  Training Loss: 0.217.. \n",
      "Epoch: 620/10000..  Training Loss: 0.218.. \n",
      "Epoch: 621/10000..  Training Loss: 0.219.. \n",
      "Epoch: 622/10000..  Training Loss: 0.220.. \n",
      "Epoch: 623/10000..  Training Loss: 0.223.. \n",
      "Epoch: 624/10000..  Training Loss: 0.224.. \n",
      "Epoch: 625/10000..  Training Loss: 0.224.. \n",
      "Epoch: 626/10000..  Training Loss: 0.220.. \n",
      "Epoch: 627/10000..  Training Loss: 0.218.. \n",
      "Epoch: 628/10000..  Training Loss: 0.216.. \n",
      "Epoch: 629/10000..  Training Loss: 0.216.. \n",
      "Epoch: 630/10000..  Training Loss: 0.218.. \n",
      "Epoch: 631/10000..  Training Loss: 0.219.. \n",
      "Epoch: 632/10000..  Training Loss: 0.222.. \n",
      "Epoch: 633/10000..  Training Loss: 0.221.. \n",
      "Epoch: 634/10000..  Training Loss: 0.221.. \n",
      "Epoch: 635/10000..  Training Loss: 0.219.. \n",
      "Epoch: 636/10000..  Training Loss: 0.217.. \n",
      "Epoch: 637/10000..  Training Loss: 0.216.. \n",
      "Epoch: 638/10000..  Training Loss: 0.216.. \n",
      "Epoch: 639/10000..  Training Loss: 0.216.. \n",
      "Epoch: 640/10000..  Training Loss: 0.217.. \n",
      "Epoch: 641/10000..  Training Loss: 0.219.. \n",
      "Epoch: 642/10000..  Training Loss: 0.219.. \n",
      "Epoch: 643/10000..  Training Loss: 0.219.. \n",
      "Epoch: 644/10000..  Training Loss: 0.218.. \n",
      "Epoch: 645/10000..  Training Loss: 0.218.. \n",
      "Epoch: 646/10000..  Training Loss: 0.216.. \n",
      "Epoch: 647/10000..  Training Loss: 0.216.. \n",
      "Epoch: 648/10000..  Training Loss: 0.215.. \n",
      "Epoch: 649/10000..  Training Loss: 0.215.. \n",
      "Epoch: 650/10000..  Training Loss: 0.216.. \n",
      "Epoch: 651/10000..  Training Loss: 0.216.. \n",
      "Epoch: 652/10000..  Training Loss: 0.217.. \n",
      "Epoch: 653/10000..  Training Loss: 0.217.. \n",
      "Epoch: 654/10000..  Training Loss: 0.218.. \n",
      "Epoch: 655/10000..  Training Loss: 0.218.. \n",
      "Epoch: 656/10000..  Training Loss: 0.217.. \n",
      "Epoch: 657/10000..  Training Loss: 0.217.. \n",
      "Epoch: 658/10000..  Training Loss: 0.216.. \n",
      "Epoch: 659/10000..  Training Loss: 0.215.. \n",
      "Epoch: 660/10000..  Training Loss: 0.215.. \n",
      "Epoch: 661/10000..  Training Loss: 0.214.. \n",
      "Epoch: 662/10000..  Training Loss: 0.214.. \n",
      "Epoch: 663/10000..  Training Loss: 0.214.. \n",
      "Epoch: 664/10000..  Training Loss: 0.214.. \n",
      "Epoch: 665/10000..  Training Loss: 0.215.. \n",
      "Epoch: 666/10000..  Training Loss: 0.216.. \n",
      "Epoch: 667/10000..  Training Loss: 0.217.. \n",
      "Epoch: 668/10000..  Training Loss: 0.218.. \n",
      "Epoch: 669/10000..  Training Loss: 0.221.. \n",
      "Epoch: 670/10000..  Training Loss: 0.221.. \n",
      "Epoch: 671/10000..  Training Loss: 0.221.. \n",
      "Epoch: 672/10000..  Training Loss: 0.218.. \n",
      "Epoch: 673/10000..  Training Loss: 0.215.. \n",
      "Epoch: 674/10000..  Training Loss: 0.214.. \n",
      "Epoch: 675/10000..  Training Loss: 0.214.. \n",
      "Epoch: 676/10000..  Training Loss: 0.215.. \n",
      "Epoch: 677/10000..  Training Loss: 0.217.. \n",
      "Epoch: 678/10000..  Training Loss: 0.220.. \n",
      "Epoch: 679/10000..  Training Loss: 0.220.. \n",
      "Epoch: 680/10000..  Training Loss: 0.221.. \n",
      "Epoch: 681/10000..  Training Loss: 0.217.. \n",
      "Epoch: 682/10000..  Training Loss: 0.215.. \n",
      "Epoch: 683/10000..  Training Loss: 0.213.. \n",
      "Epoch: 684/10000..  Training Loss: 0.214.. \n",
      "Epoch: 685/10000..  Training Loss: 0.216.. \n",
      "Epoch: 686/10000..  Training Loss: 0.217.. \n",
      "Epoch: 687/10000..  Training Loss: 0.220.. \n",
      "Epoch: 688/10000..  Training Loss: 0.218.. \n",
      "Epoch: 689/10000..  Training Loss: 0.217.. \n",
      "Epoch: 690/10000..  Training Loss: 0.214.. \n",
      "Epoch: 691/10000..  Training Loss: 0.213.. \n",
      "Epoch: 692/10000..  Training Loss: 0.213.. \n",
      "Epoch: 693/10000..  Training Loss: 0.214.. \n",
      "Epoch: 694/10000..  Training Loss: 0.216.. \n",
      "Epoch: 695/10000..  Training Loss: 0.217.. \n",
      "Epoch: 696/10000..  Training Loss: 0.218.. \n",
      "Epoch: 697/10000..  Training Loss: 0.216.. \n",
      "Epoch: 698/10000..  Training Loss: 0.215.. \n",
      "Epoch: 699/10000..  Training Loss: 0.213.. \n",
      "Epoch: 700/10000..  Training Loss: 0.213.. \n",
      "Epoch: 701/10000..  Training Loss: 0.213.. \n",
      "Epoch: 702/10000..  Training Loss: 0.214.. \n",
      "Epoch: 703/10000..  Training Loss: 0.215.. \n",
      "Epoch: 704/10000..  Training Loss: 0.215.. \n",
      "Epoch: 705/10000..  Training Loss: 0.217.. \n",
      "Epoch: 706/10000..  Training Loss: 0.215.. \n",
      "Epoch: 707/10000..  Training Loss: 0.214.. \n",
      "Epoch: 708/10000..  Training Loss: 0.213.. \n",
      "Epoch: 709/10000..  Training Loss: 0.212.. \n",
      "Epoch: 710/10000..  Training Loss: 0.212.. \n",
      "Epoch: 711/10000..  Training Loss: 0.212.. \n",
      "Epoch: 712/10000..  Training Loss: 0.212.. \n",
      "Epoch: 713/10000..  Training Loss: 0.213.. \n",
      "Epoch: 714/10000..  Training Loss: 0.213.. \n",
      "Epoch: 715/10000..  Training Loss: 0.214.. \n",
      "Epoch: 716/10000..  Training Loss: 0.215.. \n",
      "Epoch: 717/10000..  Training Loss: 0.215.. \n",
      "Epoch: 718/10000..  Training Loss: 0.215.. \n",
      "Epoch: 719/10000..  Training Loss: 0.214.. \n",
      "Epoch: 720/10000..  Training Loss: 0.213.. \n",
      "Epoch: 721/10000..  Training Loss: 0.212.. \n",
      "Epoch: 722/10000..  Training Loss: 0.212.. \n",
      "Epoch: 723/10000..  Training Loss: 0.211.. \n",
      "Epoch: 724/10000..  Training Loss: 0.211.. \n",
      "Epoch: 725/10000..  Training Loss: 0.211.. \n",
      "Epoch: 726/10000..  Training Loss: 0.211.. \n",
      "Epoch: 727/10000..  Training Loss: 0.211.. \n",
      "Epoch: 728/10000..  Training Loss: 0.211.. \n",
      "Epoch: 729/10000..  Training Loss: 0.211.. \n",
      "Epoch: 730/10000..  Training Loss: 0.212.. \n",
      "Epoch: 731/10000..  Training Loss: 0.213.. \n",
      "Epoch: 732/10000..  Training Loss: 0.215.. \n",
      "Epoch: 733/10000..  Training Loss: 0.218.. \n",
      "Epoch: 734/10000..  Training Loss: 0.218.. \n",
      "Epoch: 735/10000..  Training Loss: 0.219.. \n",
      "Epoch: 736/10000..  Training Loss: 0.215.. \n",
      "Epoch: 737/10000..  Training Loss: 0.213.. \n",
      "Epoch: 738/10000..  Training Loss: 0.211.. \n",
      "Epoch: 739/10000..  Training Loss: 0.210.. \n",
      "Epoch: 740/10000..  Training Loss: 0.211.. \n",
      "Epoch: 741/10000..  Training Loss: 0.212.. \n",
      "Epoch: 742/10000..  Training Loss: 0.214.. \n",
      "Epoch: 743/10000..  Training Loss: 0.216.. \n",
      "Epoch: 744/10000..  Training Loss: 0.219.. \n",
      "Epoch: 745/10000..  Training Loss: 0.218.. \n",
      "Epoch: 746/10000..  Training Loss: 0.216.. \n",
      "Epoch: 747/10000..  Training Loss: 0.213.. \n",
      "Epoch: 748/10000..  Training Loss: 0.210.. \n",
      "Epoch: 749/10000..  Training Loss: 0.210.. \n",
      "Epoch: 750/10000..  Training Loss: 0.211.. \n",
      "Epoch: 751/10000..  Training Loss: 0.214.. \n",
      "Epoch: 752/10000..  Training Loss: 0.215.. \n",
      "Epoch: 753/10000..  Training Loss: 0.216.. \n",
      "Epoch: 754/10000..  Training Loss: 0.214.. \n",
      "Epoch: 755/10000..  Training Loss: 0.212.. \n",
      "Epoch: 756/10000..  Training Loss: 0.210.. \n",
      "Epoch: 757/10000..  Training Loss: 0.209.. \n",
      "Epoch: 758/10000..  Training Loss: 0.210.. \n",
      "Epoch: 759/10000..  Training Loss: 0.210.. \n",
      "Epoch: 760/10000..  Training Loss: 0.212.. \n",
      "Epoch: 761/10000..  Training Loss: 0.213.. \n",
      "Epoch: 762/10000..  Training Loss: 0.214.. \n",
      "Epoch: 763/10000..  Training Loss: 0.213.. \n",
      "Epoch: 764/10000..  Training Loss: 0.213.. \n",
      "Epoch: 765/10000..  Training Loss: 0.211.. \n",
      "Epoch: 766/10000..  Training Loss: 0.210.. \n",
      "Epoch: 767/10000..  Training Loss: 0.209.. \n",
      "Epoch: 768/10000..  Training Loss: 0.209.. \n",
      "Epoch: 769/10000..  Training Loss: 0.210.. \n",
      "Epoch: 770/10000..  Training Loss: 0.210.. \n",
      "Epoch: 771/10000..  Training Loss: 0.211.. \n",
      "Epoch: 772/10000..  Training Loss: 0.212.. \n",
      "Epoch: 773/10000..  Training Loss: 0.213.. \n",
      "Epoch: 774/10000..  Training Loss: 0.213.. \n",
      "Epoch: 775/10000..  Training Loss: 0.212.. \n",
      "Epoch: 776/10000..  Training Loss: 0.211.. \n",
      "Epoch: 777/10000..  Training Loss: 0.210.. \n",
      "Epoch: 778/10000..  Training Loss: 0.209.. \n",
      "Epoch: 779/10000..  Training Loss: 0.208.. \n",
      "Epoch: 780/10000..  Training Loss: 0.208.. \n",
      "Epoch: 781/10000..  Training Loss: 0.209.. \n",
      "Epoch: 782/10000..  Training Loss: 0.209.. \n",
      "Epoch: 783/10000..  Training Loss: 0.209.. \n",
      "Epoch: 784/10000..  Training Loss: 0.211.. \n",
      "Epoch: 785/10000..  Training Loss: 0.212.. \n",
      "Epoch: 786/10000..  Training Loss: 0.214.. \n",
      "Epoch: 787/10000..  Training Loss: 0.214.. \n",
      "Epoch: 788/10000..  Training Loss: 0.214.. \n",
      "Epoch: 789/10000..  Training Loss: 0.212.. \n",
      "Epoch: 790/10000..  Training Loss: 0.210.. \n",
      "Epoch: 791/10000..  Training Loss: 0.209.. \n",
      "Epoch: 792/10000..  Training Loss: 0.208.. \n",
      "Epoch: 793/10000..  Training Loss: 0.208.. \n",
      "Epoch: 794/10000..  Training Loss: 0.209.. \n",
      "Epoch: 795/10000..  Training Loss: 0.210.. \n",
      "Epoch: 796/10000..  Training Loss: 0.211.. \n",
      "Epoch: 797/10000..  Training Loss: 0.214.. \n",
      "Epoch: 798/10000..  Training Loss: 0.214.. \n",
      "Epoch: 799/10000..  Training Loss: 0.214.. \n",
      "Epoch: 800/10000..  Training Loss: 0.212.. \n",
      "Epoch: 801/10000..  Training Loss: 0.209.. \n",
      "Epoch: 802/10000..  Training Loss: 0.208.. \n",
      "Epoch: 803/10000..  Training Loss: 0.208.. \n",
      "Epoch: 804/10000..  Training Loss: 0.208.. \n",
      "Epoch: 805/10000..  Training Loss: 0.209.. \n",
      "Epoch: 806/10000..  Training Loss: 0.211.. \n",
      "Epoch: 807/10000..  Training Loss: 0.212.. \n",
      "Epoch: 808/10000..  Training Loss: 0.213.. \n",
      "Epoch: 809/10000..  Training Loss: 0.212.. \n",
      "Epoch: 810/10000..  Training Loss: 0.211.. \n",
      "Epoch: 811/10000..  Training Loss: 0.209.. \n",
      "Epoch: 812/10000..  Training Loss: 0.208.. \n",
      "Epoch: 813/10000..  Training Loss: 0.207.. \n",
      "Epoch: 814/10000..  Training Loss: 0.207.. \n",
      "Epoch: 815/10000..  Training Loss: 0.208.. \n",
      "Epoch: 816/10000..  Training Loss: 0.209.. \n",
      "Epoch: 817/10000..  Training Loss: 0.210.. \n",
      "Epoch: 818/10000..  Training Loss: 0.211.. \n",
      "Epoch: 819/10000..  Training Loss: 0.212.. \n",
      "Epoch: 820/10000..  Training Loss: 0.211.. \n",
      "Epoch: 821/10000..  Training Loss: 0.210.. \n",
      "Epoch: 822/10000..  Training Loss: 0.208.. \n",
      "Epoch: 823/10000..  Training Loss: 0.207.. \n",
      "Epoch: 824/10000..  Training Loss: 0.206.. \n",
      "Epoch: 825/10000..  Training Loss: 0.207.. \n",
      "Epoch: 826/10000..  Training Loss: 0.207.. \n",
      "Epoch: 827/10000..  Training Loss: 0.208.. \n",
      "Epoch: 828/10000..  Training Loss: 0.210.. \n",
      "Epoch: 829/10000..  Training Loss: 0.210.. \n",
      "Epoch: 830/10000..  Training Loss: 0.212.. \n",
      "Epoch: 831/10000..  Training Loss: 0.211.. \n",
      "Epoch: 832/10000..  Training Loss: 0.211.. \n",
      "Epoch: 833/10000..  Training Loss: 0.208.. \n",
      "Epoch: 834/10000..  Training Loss: 0.207.. \n",
      "Epoch: 835/10000..  Training Loss: 0.206.. \n",
      "Epoch: 836/10000..  Training Loss: 0.206.. \n",
      "Epoch: 837/10000..  Training Loss: 0.207.. \n",
      "Epoch: 838/10000..  Training Loss: 0.208.. \n",
      "Epoch: 839/10000..  Training Loss: 0.210.. \n",
      "Epoch: 840/10000..  Training Loss: 0.211.. \n",
      "Epoch: 841/10000..  Training Loss: 0.212.. \n",
      "Epoch: 842/10000..  Training Loss: 0.210.. \n",
      "Epoch: 843/10000..  Training Loss: 0.209.. \n",
      "Epoch: 844/10000..  Training Loss: 0.207.. \n",
      "Epoch: 845/10000..  Training Loss: 0.206.. \n",
      "Epoch: 846/10000..  Training Loss: 0.206.. \n",
      "Epoch: 847/10000..  Training Loss: 0.206.. \n",
      "Epoch: 848/10000..  Training Loss: 0.206.. \n",
      "Epoch: 849/10000..  Training Loss: 0.207.. \n",
      "Epoch: 850/10000..  Training Loss: 0.207.. \n",
      "Epoch: 851/10000..  Training Loss: 0.208.. \n",
      "Epoch: 852/10000..  Training Loss: 0.209.. \n",
      "Epoch: 853/10000..  Training Loss: 0.210.. \n",
      "Epoch: 854/10000..  Training Loss: 0.210.. \n",
      "Epoch: 855/10000..  Training Loss: 0.209.. \n",
      "Epoch: 856/10000..  Training Loss: 0.208.. \n",
      "Epoch: 857/10000..  Training Loss: 0.207.. \n",
      "Epoch: 858/10000..  Training Loss: 0.206.. \n",
      "Epoch: 859/10000..  Training Loss: 0.205.. \n",
      "Epoch: 860/10000..  Training Loss: 0.205.. \n",
      "Epoch: 861/10000..  Training Loss: 0.205.. \n",
      "Epoch: 862/10000..  Training Loss: 0.205.. \n",
      "Epoch: 863/10000..  Training Loss: 0.206.. \n",
      "Epoch: 864/10000..  Training Loss: 0.207.. \n",
      "Epoch: 865/10000..  Training Loss: 0.208.. \n",
      "Epoch: 866/10000..  Training Loss: 0.209.. \n",
      "Epoch: 867/10000..  Training Loss: 0.210.. \n",
      "Epoch: 868/10000..  Training Loss: 0.210.. \n",
      "Epoch: 869/10000..  Training Loss: 0.210.. \n",
      "Epoch: 870/10000..  Training Loss: 0.208.. \n",
      "Epoch: 871/10000..  Training Loss: 0.207.. \n",
      "Epoch: 872/10000..  Training Loss: 0.206.. \n",
      "Epoch: 873/10000..  Training Loss: 0.205.. \n",
      "Epoch: 874/10000..  Training Loss: 0.205.. \n",
      "Epoch: 875/10000..  Training Loss: 0.205.. \n",
      "Epoch: 876/10000..  Training Loss: 0.205.. \n",
      "Epoch: 877/10000..  Training Loss: 0.205.. \n",
      "Epoch: 878/10000..  Training Loss: 0.206.. \n",
      "Epoch: 879/10000..  Training Loss: 0.206.. \n",
      "Epoch: 880/10000..  Training Loss: 0.208.. \n",
      "Epoch: 881/10000..  Training Loss: 0.209.. \n",
      "Epoch: 882/10000..  Training Loss: 0.211.. \n",
      "Epoch: 883/10000..  Training Loss: 0.211.. \n",
      "Epoch: 884/10000..  Training Loss: 0.212.. \n",
      "Epoch: 885/10000..  Training Loss: 0.209.. \n",
      "Epoch: 886/10000..  Training Loss: 0.206.. \n",
      "Epoch: 887/10000..  Training Loss: 0.205.. \n",
      "Epoch: 888/10000..  Training Loss: 0.204.. \n",
      "Epoch: 889/10000..  Training Loss: 0.205.. \n",
      "Epoch: 890/10000..  Training Loss: 0.206.. \n",
      "Epoch: 891/10000..  Training Loss: 0.208.. \n",
      "Epoch: 892/10000..  Training Loss: 0.209.. \n",
      "Epoch: 893/10000..  Training Loss: 0.210.. \n",
      "Epoch: 894/10000..  Training Loss: 0.209.. \n",
      "Epoch: 895/10000..  Training Loss: 0.207.. \n",
      "Epoch: 896/10000..  Training Loss: 0.205.. \n",
      "Epoch: 897/10000..  Training Loss: 0.204.. \n",
      "Epoch: 898/10000..  Training Loss: 0.204.. \n",
      "Epoch: 899/10000..  Training Loss: 0.204.. \n",
      "Epoch: 900/10000..  Training Loss: 0.205.. \n",
      "Epoch: 901/10000..  Training Loss: 0.205.. \n",
      "Epoch: 902/10000..  Training Loss: 0.206.. \n",
      "Epoch: 903/10000..  Training Loss: 0.206.. \n",
      "Epoch: 904/10000..  Training Loss: 0.207.. \n",
      "Epoch: 905/10000..  Training Loss: 0.208.. \n",
      "Epoch: 906/10000..  Training Loss: 0.209.. \n",
      "Epoch: 907/10000..  Training Loss: 0.208.. \n",
      "Epoch: 908/10000..  Training Loss: 0.206.. \n",
      "Epoch: 909/10000..  Training Loss: 0.205.. \n",
      "Epoch: 910/10000..  Training Loss: 0.204.. \n",
      "Epoch: 911/10000..  Training Loss: 0.204.. \n",
      "Epoch: 912/10000..  Training Loss: 0.204.. \n",
      "Epoch: 913/10000..  Training Loss: 0.204.. \n",
      "Epoch: 914/10000..  Training Loss: 0.203.. \n",
      "Epoch: 915/10000..  Training Loss: 0.204.. \n",
      "Epoch: 916/10000..  Training Loss: 0.204.. \n",
      "Epoch: 917/10000..  Training Loss: 0.205.. \n",
      "Epoch: 918/10000..  Training Loss: 0.206.. \n",
      "Epoch: 919/10000..  Training Loss: 0.207.. \n",
      "Epoch: 920/10000..  Training Loss: 0.208.. \n",
      "Epoch: 921/10000..  Training Loss: 0.209.. \n",
      "Epoch: 922/10000..  Training Loss: 0.208.. \n",
      "Epoch: 923/10000..  Training Loss: 0.207.. \n",
      "Epoch: 924/10000..  Training Loss: 0.206.. \n",
      "Epoch: 925/10000..  Training Loss: 0.204.. \n",
      "Epoch: 926/10000..  Training Loss: 0.203.. \n",
      "Epoch: 927/10000..  Training Loss: 0.203.. \n",
      "Epoch: 928/10000..  Training Loss: 0.203.. \n",
      "Epoch: 929/10000..  Training Loss: 0.203.. \n",
      "Epoch: 930/10000..  Training Loss: 0.203.. \n",
      "Epoch: 931/10000..  Training Loss: 0.204.. \n",
      "Epoch: 932/10000..  Training Loss: 0.205.. \n",
      "Epoch: 933/10000..  Training Loss: 0.206.. \n",
      "Epoch: 934/10000..  Training Loss: 0.208.. \n",
      "Epoch: 935/10000..  Training Loss: 0.209.. \n",
      "Epoch: 936/10000..  Training Loss: 0.210.. \n",
      "Epoch: 937/10000..  Training Loss: 0.207.. \n",
      "Epoch: 938/10000..  Training Loss: 0.205.. \n",
      "Epoch: 939/10000..  Training Loss: 0.203.. \n",
      "Epoch: 940/10000..  Training Loss: 0.202.. \n",
      "Epoch: 941/10000..  Training Loss: 0.203.. \n",
      "Epoch: 942/10000..  Training Loss: 0.204.. \n",
      "Epoch: 943/10000..  Training Loss: 0.207.. \n",
      "Epoch: 944/10000..  Training Loss: 0.209.. \n",
      "Epoch: 945/10000..  Training Loss: 0.212.. \n",
      "Epoch: 946/10000..  Training Loss: 0.209.. \n",
      "Epoch: 947/10000..  Training Loss: 0.207.. \n",
      "Epoch: 948/10000..  Training Loss: 0.203.. \n",
      "Epoch: 949/10000..  Training Loss: 0.202.. \n",
      "Epoch: 950/10000..  Training Loss: 0.203.. \n",
      "Epoch: 951/10000..  Training Loss: 0.204.. \n",
      "Epoch: 952/10000..  Training Loss: 0.208.. \n",
      "Epoch: 953/10000..  Training Loss: 0.209.. \n",
      "Epoch: 954/10000..  Training Loss: 0.210.. \n",
      "Epoch: 955/10000..  Training Loss: 0.207.. \n",
      "Epoch: 956/10000..  Training Loss: 0.204.. \n",
      "Epoch: 957/10000..  Training Loss: 0.202.. \n",
      "Epoch: 958/10000..  Training Loss: 0.202.. \n",
      "Epoch: 959/10000..  Training Loss: 0.204.. \n",
      "Epoch: 960/10000..  Training Loss: 0.205.. \n",
      "Epoch: 961/10000..  Training Loss: 0.207.. \n",
      "Epoch: 962/10000..  Training Loss: 0.207.. \n",
      "Epoch: 963/10000..  Training Loss: 0.207.. \n",
      "Epoch: 964/10000..  Training Loss: 0.204.. \n",
      "Epoch: 965/10000..  Training Loss: 0.202.. \n",
      "Epoch: 966/10000..  Training Loss: 0.202.. \n",
      "Epoch: 967/10000..  Training Loss: 0.203.. \n",
      "Epoch: 968/10000..  Training Loss: 0.204.. \n",
      "Epoch: 969/10000..  Training Loss: 0.205.. \n",
      "Epoch: 970/10000..  Training Loss: 0.206.. \n",
      "Epoch: 971/10000..  Training Loss: 0.205.. \n",
      "Epoch: 972/10000..  Training Loss: 0.203.. \n",
      "Epoch: 973/10000..  Training Loss: 0.202.. \n",
      "Epoch: 974/10000..  Training Loss: 0.202.. \n",
      "Epoch: 975/10000..  Training Loss: 0.202.. \n",
      "Epoch: 976/10000..  Training Loss: 0.201.. \n",
      "Epoch: 977/10000..  Training Loss: 0.202.. \n",
      "Epoch: 978/10000..  Training Loss: 0.202.. \n",
      "Epoch: 979/10000..  Training Loss: 0.202.. \n",
      "Epoch: 980/10000..  Training Loss: 0.203.. \n",
      "Epoch: 981/10000..  Training Loss: 0.203.. \n",
      "Epoch: 982/10000..  Training Loss: 0.202.. \n",
      "Epoch: 983/10000..  Training Loss: 0.202.. \n",
      "Epoch: 984/10000..  Training Loss: 0.202.. \n",
      "Epoch: 985/10000..  Training Loss: 0.201.. \n",
      "Epoch: 986/10000..  Training Loss: 0.201.. \n",
      "Epoch: 987/10000..  Training Loss: 0.201.. \n",
      "Epoch: 988/10000..  Training Loss: 0.201.. \n",
      "Epoch: 989/10000..  Training Loss: 0.201.. \n",
      "Epoch: 990/10000..  Training Loss: 0.201.. \n",
      "Epoch: 991/10000..  Training Loss: 0.201.. \n",
      "Epoch: 992/10000..  Training Loss: 0.201.. \n",
      "Epoch: 993/10000..  Training Loss: 0.201.. \n",
      "Epoch: 994/10000..  Training Loss: 0.201.. \n",
      "Epoch: 995/10000..  Training Loss: 0.201.. \n",
      "Epoch: 996/10000..  Training Loss: 0.202.. \n",
      "Epoch: 997/10000..  Training Loss: 0.202.. \n",
      "Epoch: 998/10000..  Training Loss: 0.203.. \n",
      "Epoch: 999/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1000/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1001/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1002/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1003/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1004/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1005/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1006/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1007/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1008/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1009/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1010/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1011/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1012/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1013/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1014/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1015/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1016/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1017/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1018/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1019/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1020/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1021/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1022/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1023/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1024/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1025/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1026/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1027/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1028/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1029/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1030/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1031/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1032/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1033/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1034/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1035/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1036/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1037/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1038/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1039/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1040/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1041/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1042/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1043/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1044/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1045/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1046/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1047/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1048/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1049/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1050/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1051/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1052/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1053/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1054/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1055/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1056/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1057/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1058/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1059/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1060/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1061/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1062/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1063/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1064/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1065/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1066/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1067/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1068/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1069/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1070/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1071/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1072/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1073/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1074/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1075/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1076/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1077/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1078/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1079/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1080/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1081/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1082/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1083/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1084/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1085/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1086/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1087/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1088/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1089/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1090/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1091/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1092/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1093/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1094/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1095/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1096/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1097/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1098/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1099/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1100/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1101/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1102/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1103/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1104/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1105/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1106/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1107/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1108/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1109/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1110/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1111/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1112/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1113/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1114/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1115/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1116/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1117/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1118/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1119/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1120/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1121/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1122/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1123/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1124/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1125/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1126/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1127/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1128/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1129/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1130/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1131/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1132/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1133/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1134/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1135/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1136/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1137/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1138/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1139/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1140/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1141/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1142/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1143/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1144/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1145/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1146/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1147/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1148/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1149/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1150/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1151/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1152/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1153/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1154/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1155/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1156/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1157/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1158/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1159/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1160/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1161/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1162/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1163/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1164/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1165/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1166/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1167/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1168/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1169/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1170/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1171/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1172/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1173/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1174/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1175/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1176/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1177/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1178/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1179/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1180/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1181/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1182/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1183/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1184/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1185/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1186/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1187/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1188/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1189/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1190/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1191/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1192/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1193/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1194/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1195/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1196/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1197/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1198/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1199/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1200/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1201/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1202/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1203/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1204/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1205/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1206/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1207/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1208/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1209/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1210/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1211/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1212/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1213/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1214/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1215/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1216/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1217/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1218/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1219/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1220/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1221/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1222/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1223/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1224/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1225/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1226/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1227/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1228/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1229/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1230/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1231/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1232/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1233/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1234/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1235/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1236/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1237/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1238/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1239/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1240/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1241/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1242/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1243/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1244/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1245/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1246/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1247/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1248/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1249/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1250/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1251/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1252/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1253/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1254/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1255/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1256/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1257/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1258/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1259/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1260/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1261/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1262/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1263/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1264/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1265/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1266/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1267/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1268/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1269/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1270/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1271/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1272/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1273/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1274/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1275/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1276/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1277/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1278/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1279/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1280/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1281/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1282/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1283/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1284/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1285/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1286/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1287/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1288/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1289/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1290/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1291/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1292/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1293/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1294/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1295/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1296/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1297/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1298/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1299/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1300/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1301/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1302/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1303/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1304/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1305/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1306/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1307/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1308/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1309/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1310/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1311/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1312/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1313/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1314/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1315/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1316/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1317/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1318/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1319/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1320/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1321/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1322/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1323/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1324/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1325/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1326/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1327/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1328/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1329/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1330/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1331/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1332/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1333/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1334/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1335/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1336/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1337/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1338/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1339/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1340/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1341/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1342/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1343/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1344/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1345/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1346/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1347/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1348/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1349/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1350/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1351/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1352/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1353/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1354/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1355/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1356/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1357/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1358/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1359/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1360/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1361/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1362/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1363/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1364/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1365/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1366/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1367/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1368/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1369/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1370/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1371/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1372/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1373/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1374/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1375/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1376/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1377/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1378/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1379/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1380/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1381/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1382/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1383/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1384/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1385/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1386/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1387/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1388/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1389/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1390/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1391/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1392/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1393/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1394/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1395/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1396/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1397/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1398/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1399/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1400/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1401/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1402/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1403/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1404/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1405/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1406/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1407/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1408/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1409/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1410/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1411/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1412/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1413/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1414/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1415/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1416/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1417/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1418/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1419/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1420/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1421/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1422/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1423/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1424/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1425/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1426/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1427/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1428/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1429/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1430/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1431/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1432/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1433/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1434/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1435/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1436/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1437/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1438/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1439/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1440/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1441/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1442/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1443/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1444/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1445/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1446/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1447/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1448/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1449/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1450/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1451/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1452/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1453/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1454/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1455/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1456/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1457/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1458/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1459/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1460/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1461/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1462/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1463/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1464/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1465/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1466/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1467/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1468/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1469/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1470/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1471/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1472/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1473/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1474/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1475/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1476/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1477/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1478/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1479/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1480/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1481/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1482/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1483/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1484/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1485/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1486/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1487/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1488/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1489/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1490/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1491/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1492/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1493/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1494/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1495/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1496/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1497/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1498/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1499/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1500/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1501/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1502/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1503/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1504/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1505/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1506/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1507/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1508/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1509/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1510/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1511/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1512/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1513/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1514/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1515/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1516/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1517/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1518/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1519/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1520/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1521/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1522/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1523/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1524/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1525/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1526/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1527/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1528/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1529/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1530/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1531/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1532/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1533/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1534/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1535/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1536/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1537/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1538/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1539/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1540/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1541/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1542/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1543/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1544/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1545/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1546/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1547/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1548/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1549/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1550/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1551/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1552/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1553/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1554/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1555/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1556/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1557/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1558/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1559/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1560/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1561/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1562/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1563/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1564/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1565/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1566/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1567/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1568/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1569/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1570/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1571/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1572/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1573/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1574/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1575/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1576/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1577/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1578/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1579/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1580/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1581/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1582/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1583/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1584/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1585/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1586/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1587/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1588/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1589/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1590/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1591/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1592/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1593/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1594/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1595/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1596/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1597/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1598/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1599/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1600/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1601/10000..  Training Loss: 0.319.. \n",
      "Epoch: 1602/10000..  Training Loss: 0.707.. \n",
      "Epoch: 1603/10000..  Training Loss: 0.493.. \n",
      "Epoch: 1604/10000..  Training Loss: 0.316.. \n",
      "Epoch: 1605/10000..  Training Loss: 0.734.. \n",
      "Epoch: 1606/10000..  Training Loss: 0.710.. \n",
      "Epoch: 1607/10000..  Training Loss: 0.990.. \n",
      "Epoch: 1608/10000..  Training Loss: 0.831.. \n",
      "Epoch: 1609/10000..  Training Loss: 0.672.. \n",
      "Epoch: 1610/10000..  Training Loss: 0.403.. \n",
      "Epoch: 1611/10000..  Training Loss: 0.412.. \n",
      "Epoch: 1612/10000..  Training Loss: 0.549.. \n",
      "Epoch: 1613/10000..  Training Loss: 0.400.. \n",
      "Epoch: 1614/10000..  Training Loss: 0.334.. \n",
      "Epoch: 1615/10000..  Training Loss: 0.362.. \n",
      "Epoch: 1616/10000..  Training Loss: 0.393.. \n",
      "Epoch: 1617/10000..  Training Loss: 0.379.. \n",
      "Epoch: 1618/10000..  Training Loss: 0.321.. \n",
      "Epoch: 1619/10000..  Training Loss: 0.288.. \n",
      "Epoch: 1620/10000..  Training Loss: 0.270.. \n",
      "Epoch: 1621/10000..  Training Loss: 0.268.. \n",
      "Epoch: 1622/10000..  Training Loss: 0.264.. \n",
      "Epoch: 1623/10000..  Training Loss: 0.273.. \n",
      "Epoch: 1624/10000..  Training Loss: 0.276.. \n",
      "Epoch: 1625/10000..  Training Loss: 0.262.. \n",
      "Epoch: 1626/10000..  Training Loss: 0.240.. \n",
      "Epoch: 1627/10000..  Training Loss: 0.236.. \n",
      "Epoch: 1628/10000..  Training Loss: 0.254.. \n",
      "Epoch: 1629/10000..  Training Loss: 0.236.. \n",
      "Epoch: 1630/10000..  Training Loss: 0.225.. \n",
      "Epoch: 1631/10000..  Training Loss: 0.232.. \n",
      "Epoch: 1632/10000..  Training Loss: 0.231.. \n",
      "Epoch: 1633/10000..  Training Loss: 0.230.. \n",
      "Epoch: 1634/10000..  Training Loss: 0.225.. \n",
      "Epoch: 1635/10000..  Training Loss: 0.219.. \n",
      "Epoch: 1636/10000..  Training Loss: 0.222.. \n",
      "Epoch: 1637/10000..  Training Loss: 0.222.. \n",
      "Epoch: 1638/10000..  Training Loss: 0.217.. \n",
      "Epoch: 1639/10000..  Training Loss: 0.217.. \n",
      "Epoch: 1640/10000..  Training Loss: 0.217.. \n",
      "Epoch: 1641/10000..  Training Loss: 0.214.. \n",
      "Epoch: 1642/10000..  Training Loss: 0.213.. \n",
      "Epoch: 1643/10000..  Training Loss: 0.213.. \n",
      "Epoch: 1644/10000..  Training Loss: 0.212.. \n",
      "Epoch: 1645/10000..  Training Loss: 0.212.. \n",
      "Epoch: 1646/10000..  Training Loss: 0.212.. \n",
      "Epoch: 1647/10000..  Training Loss: 0.210.. \n",
      "Epoch: 1648/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1649/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1650/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1651/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1652/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1653/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1654/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1655/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1656/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1657/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1658/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1659/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1660/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1661/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1662/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1663/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1664/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1665/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1666/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1667/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1668/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1669/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1670/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1671/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1672/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1673/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1674/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1675/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1676/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1677/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1678/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1679/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1680/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1681/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1682/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1683/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1684/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1685/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1686/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1687/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1688/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1689/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1690/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1691/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1692/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1693/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1694/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1695/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1696/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1697/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1698/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1699/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1700/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1701/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1702/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1703/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1704/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1705/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1706/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1707/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1708/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1709/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1710/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1711/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1712/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1713/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1714/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1715/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1716/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1717/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1718/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1719/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1720/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1721/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1722/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1723/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1724/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1725/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1726/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1727/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1728/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1729/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1730/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1731/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1732/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1733/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1734/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1735/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1736/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1737/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1738/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1739/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1740/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1741/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1742/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1743/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1744/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1745/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1746/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1747/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1748/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1749/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1750/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1751/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1752/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1753/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1754/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1755/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1756/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1757/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1758/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1759/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1760/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1761/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1762/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1763/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1764/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1765/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1766/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1767/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1768/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1769/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1770/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1771/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1772/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1773/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1774/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1775/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1776/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1777/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1778/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1779/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1780/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1781/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1782/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1783/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1784/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1785/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1786/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1787/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1788/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1789/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1790/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1791/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1792/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1793/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1794/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1795/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1796/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1797/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1798/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1799/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1800/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1801/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1802/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1803/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1804/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1805/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1806/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1807/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1808/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1809/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1810/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1811/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1812/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1813/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1814/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1815/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1816/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1817/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1818/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1819/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1820/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1821/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1822/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1823/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1824/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1825/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1826/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1827/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1828/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1829/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1830/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1831/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1832/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1833/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1834/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1835/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1836/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1837/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1838/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1839/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1840/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1841/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1842/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1843/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1844/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1845/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1846/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1847/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1848/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1849/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1850/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1851/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1852/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1853/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1854/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1855/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1856/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1857/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1858/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1859/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1860/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1861/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1862/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1863/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1864/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1865/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1866/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1867/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1868/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1869/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1870/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1871/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1872/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1873/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1874/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1875/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1876/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1877/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1878/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1879/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1880/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1881/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1882/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1883/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1884/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1885/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1886/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1887/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1888/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1889/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1890/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1891/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1892/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1893/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1894/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1895/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1896/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1897/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1898/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1899/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1900/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1901/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1902/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1903/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1904/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1905/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1906/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1907/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1908/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1909/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1910/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1911/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1912/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1913/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1914/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1915/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1916/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1917/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1918/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1919/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1920/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1921/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1922/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1923/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1924/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1925/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1926/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1927/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1928/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1929/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1930/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1931/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1932/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1933/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1934/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1935/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1936/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1937/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1938/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1939/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1940/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1941/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1942/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1943/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1944/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1945/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1946/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1947/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1948/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1949/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1950/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1951/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1952/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1953/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1954/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1955/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1956/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1957/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1958/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1959/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1960/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1961/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1962/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1963/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1964/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1965/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1966/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1967/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1968/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1969/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1970/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1971/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1972/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1973/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1974/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1975/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1976/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1977/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1978/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1979/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1980/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1981/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1982/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1983/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1984/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1985/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1986/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1987/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1988/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1989/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1990/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1991/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1992/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1993/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1994/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1995/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1996/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1997/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1998/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1999/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2000/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2001/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2002/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2003/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2004/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2005/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2006/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2007/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2008/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2009/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2010/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2011/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2012/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2013/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2014/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2015/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2016/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2017/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2018/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2019/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2020/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2021/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2022/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2023/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2024/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2025/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2026/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2027/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2028/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2029/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2030/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2031/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2032/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2033/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2034/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2035/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2036/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2037/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2038/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2039/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2040/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2041/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2042/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2043/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2044/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2045/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2046/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2047/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2048/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2049/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2050/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2051/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2052/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2053/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2054/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2055/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2056/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2057/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2058/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2059/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2060/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2061/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2062/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2063/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2064/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2065/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2066/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2067/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2068/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2069/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2070/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2071/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2072/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2073/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2074/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2075/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2076/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2077/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2078/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2079/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2080/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2081/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2082/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2083/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2084/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2085/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2086/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2087/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2088/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2089/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2090/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2091/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2092/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2093/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2094/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2095/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2096/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2097/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2098/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2099/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2100/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2101/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2102/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2103/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2104/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2105/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2106/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2107/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2108/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2109/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2110/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2111/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2112/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2113/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2114/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2115/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2116/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2117/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2118/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2119/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2120/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2121/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2122/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2123/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2124/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2125/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2126/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2127/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2128/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2129/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2130/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2131/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2132/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2133/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2134/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2135/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2136/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2137/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2138/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2139/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2140/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2141/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2142/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2143/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2144/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2145/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2146/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2147/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2148/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2149/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2150/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2151/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2152/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2153/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2154/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2155/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2156/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2157/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2158/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2159/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2160/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2161/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2162/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2163/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2164/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2165/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2166/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2167/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2168/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2169/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2170/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2171/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2172/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2173/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2174/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2175/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2176/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2177/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2178/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2179/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2180/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2181/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2182/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2183/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2184/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2185/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2186/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2187/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2188/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2189/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2190/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2191/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2192/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2193/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2194/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2195/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2196/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2197/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2198/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2199/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2200/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2201/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2202/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2203/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2204/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2205/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2206/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2207/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2208/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2209/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2210/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2211/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2212/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2213/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2214/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2215/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2216/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2217/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2218/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2219/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2220/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2221/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2222/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2223/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2224/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2225/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2226/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2227/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2228/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2229/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2230/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2231/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2232/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2233/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2234/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2235/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2236/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2237/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2238/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2239/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2240/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2241/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2242/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2243/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2244/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2245/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2246/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2247/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2248/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2249/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2250/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2251/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2252/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2253/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2254/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2255/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2256/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2257/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2258/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2259/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2260/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2261/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2262/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2263/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2264/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2265/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2266/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2267/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2268/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2269/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2270/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2271/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2272/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2273/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2274/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2275/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2276/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2277/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2278/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2279/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2280/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2281/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2282/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2283/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2284/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2285/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2286/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2287/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2288/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2289/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2290/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2291/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2292/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2293/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2294/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2295/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2296/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2297/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2298/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2299/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2300/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2301/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2302/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2303/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2304/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2305/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2306/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2307/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2308/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2309/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2310/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2311/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2312/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2313/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2314/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2315/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2316/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2317/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2318/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2319/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2320/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2321/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2322/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2323/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2324/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2325/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2326/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2327/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2328/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2329/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2330/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2331/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2332/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2333/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2334/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2335/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2336/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2337/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2338/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2339/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2340/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2341/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2342/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2343/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2344/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2345/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2346/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2347/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2348/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2349/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2350/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2351/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2352/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2353/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2354/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2355/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2356/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2357/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2358/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2359/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2360/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2361/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2362/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2363/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2364/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2365/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2366/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2367/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2368/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2369/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2370/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2371/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2372/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2373/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2374/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2375/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2376/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2377/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2378/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2379/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2380/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2381/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2382/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2383/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2384/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2385/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2386/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2387/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2388/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2389/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2390/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2391/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2392/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2393/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2394/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2395/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2396/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2397/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2398/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2399/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2400/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2401/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2402/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2403/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2404/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2405/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2406/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2407/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2408/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2409/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2410/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2411/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2412/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2413/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2414/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2415/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2416/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2417/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2418/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2419/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2420/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2421/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2422/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2423/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2424/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2425/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2426/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2427/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2428/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2429/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2430/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2431/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2432/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2433/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2434/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2435/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2436/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2437/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2438/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2439/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2440/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2441/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2442/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2443/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2444/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2445/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2446/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2447/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2448/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2449/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2450/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2451/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2452/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2453/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2454/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2455/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2456/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2457/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2458/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2459/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2460/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2461/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2462/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2463/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2464/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2465/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2466/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2467/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2468/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2469/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2470/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2471/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2472/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2473/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2474/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2475/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2476/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2477/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2478/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2479/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2480/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2481/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2482/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2483/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2484/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2485/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2486/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2487/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2488/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2489/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2490/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2491/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2492/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2493/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2494/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2495/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2496/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2497/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2498/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2499/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2500/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2501/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2502/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2503/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2504/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2505/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2506/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2507/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2508/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2509/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2510/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2511/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2512/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2513/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2514/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2515/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2516/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2517/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2518/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2519/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2520/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2521/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2522/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2523/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2524/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2525/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2526/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2527/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2528/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2529/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2530/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2531/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2532/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2533/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2534/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2535/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2536/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2537/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2538/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2539/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2540/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2541/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2542/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2543/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2544/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2545/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2546/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2547/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2548/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2549/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2550/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2551/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2552/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2553/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2554/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2555/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2556/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2557/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2558/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2559/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2560/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2561/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2562/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2563/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2564/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2565/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2566/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2567/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2568/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2569/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2570/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2571/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2572/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2573/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2574/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2575/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2576/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2577/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2578/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2579/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2580/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2581/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2582/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2583/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2584/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2585/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2586/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2587/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2588/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2589/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2590/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2591/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2592/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2593/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2594/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2595/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2596/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2597/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2598/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2599/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2600/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2601/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2602/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2603/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2604/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2605/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2606/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2607/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2608/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2609/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2610/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2611/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2612/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2613/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2614/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2615/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2616/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2617/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2618/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2619/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2620/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2621/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2622/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2623/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2624/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2625/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2626/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2627/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2628/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2629/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2630/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2631/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2632/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2633/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2634/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2635/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2636/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2637/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2638/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2639/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2640/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2641/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2642/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2643/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2644/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2645/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2646/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2647/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2648/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2649/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2650/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2651/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2652/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2653/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2654/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2655/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2656/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2657/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2658/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2659/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2660/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2661/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2662/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2663/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2664/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2665/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2666/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2667/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2668/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2669/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2670/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2671/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2672/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2673/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2674/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2675/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2676/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2677/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2678/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2679/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2680/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2681/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2682/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2683/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2684/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2685/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2686/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2687/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2688/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2689/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2690/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2691/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2692/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2693/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2694/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2695/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2696/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2697/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2698/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2699/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2700/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2701/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2702/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2703/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2704/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2705/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2706/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2707/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2708/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2709/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2710/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2711/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2712/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2713/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2714/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2715/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2716/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2717/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2718/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2719/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2720/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2721/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2722/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2723/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2724/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2725/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2726/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2727/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2728/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2729/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2730/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2731/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2732/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2733/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2734/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2735/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2736/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2737/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2738/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2739/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2740/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2741/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2742/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2743/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2744/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2745/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2746/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2747/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2748/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2749/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2750/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2751/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2752/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2753/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2754/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2755/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2756/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2757/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2758/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2759/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2760/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2761/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2762/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2763/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2764/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2765/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2766/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2767/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2768/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2769/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2770/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2771/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2772/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2773/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2774/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2775/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2776/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2777/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2778/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2779/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2780/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2781/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2782/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2783/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2784/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2785/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2786/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2787/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2788/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2789/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2790/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2791/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2792/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2793/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2794/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2795/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2796/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2797/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2798/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2799/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2800/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2801/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2802/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2803/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2804/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2805/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2806/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2807/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2808/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2809/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2810/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2811/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2812/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2813/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2814/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2815/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2816/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2817/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2818/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2819/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2820/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2821/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2822/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2823/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2824/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2825/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2826/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2827/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2828/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2829/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2830/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2831/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2832/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2833/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2834/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2835/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2836/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2837/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2838/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2839/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2840/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2841/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2842/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2843/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2844/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2845/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2846/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2847/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2848/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2849/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2850/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2851/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2852/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2853/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2854/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2855/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2856/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2857/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2858/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2859/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2860/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2861/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2862/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2863/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2864/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2865/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2866/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2867/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2868/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2869/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2870/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2871/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2872/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2873/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2874/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2875/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2876/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2877/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2878/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2879/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2880/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2881/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2882/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2883/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2884/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2885/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2886/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2887/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2888/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2889/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2890/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2891/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2892/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2893/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2894/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2895/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2896/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2897/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2898/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2899/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2900/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2901/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2902/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2903/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2904/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2905/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2906/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2907/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2908/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2909/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2910/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2911/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2912/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2913/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2914/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2915/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2916/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2917/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2918/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2919/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2920/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2921/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2922/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2923/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2924/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2925/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2926/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2927/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2928/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2929/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2930/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2931/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2932/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2933/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2934/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2935/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2936/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2937/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2938/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2939/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2940/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2941/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2942/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2943/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2944/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2945/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2946/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2947/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2948/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2949/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2950/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2951/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2952/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2953/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2954/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2955/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2956/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2957/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2958/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2959/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2960/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2961/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2962/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2963/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2964/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2965/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2966/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2967/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2968/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2969/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2970/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2971/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2972/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2973/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2974/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2975/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2976/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2977/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2978/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2979/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2980/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2981/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2982/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2983/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2984/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2985/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2986/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2987/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2988/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2989/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2990/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2991/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2992/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2993/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2994/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2995/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2996/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2997/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2998/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2999/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3000/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3001/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3002/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3003/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3004/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3005/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3006/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3007/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3008/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3009/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3010/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3011/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3012/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3013/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3014/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3015/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3016/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3017/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3018/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3019/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3020/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3021/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3022/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3023/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3024/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3025/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3026/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3027/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3028/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3029/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3030/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3031/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3032/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3033/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3034/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3035/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3036/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3037/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3038/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3039/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3040/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3041/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3042/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3043/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3044/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3045/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3046/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3047/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3048/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3049/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3050/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3051/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3052/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3053/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3054/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3055/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3056/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3057/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3058/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3059/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3060/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3061/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3062/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3063/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3064/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3065/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3066/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3067/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3068/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3069/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3070/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3071/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3072/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3073/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3074/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3075/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3076/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3077/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3078/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3079/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3080/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3081/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3082/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3083/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3084/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3085/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3086/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3087/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3088/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3089/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3090/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3091/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3092/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3093/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3094/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3095/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3096/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3097/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3098/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3099/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3100/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3101/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3102/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3103/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3104/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3105/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3106/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3107/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3108/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3109/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3110/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3111/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3112/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3113/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3114/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3115/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3116/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3117/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3118/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3119/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3120/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3121/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3122/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3123/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3124/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3125/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3126/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3127/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3128/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3129/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3130/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3131/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3132/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3133/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3134/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3135/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3136/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3137/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3138/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3139/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3140/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3141/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3142/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3143/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3144/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3145/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3146/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3147/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3148/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3149/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3150/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3151/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3152/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3153/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3154/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3155/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3156/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3157/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3158/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3159/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3160/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3161/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3162/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3163/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3164/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3165/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3166/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3167/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3168/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3169/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3170/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3171/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3172/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3173/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3174/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3175/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3176/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3177/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3178/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3179/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3180/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3181/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3182/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3183/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3184/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3185/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3186/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3187/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3188/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3189/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3190/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3191/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3192/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3193/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3194/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3195/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3196/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3197/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3198/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3199/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3200/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3201/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3202/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3203/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3204/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3205/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3206/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3207/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3208/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3209/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3210/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3211/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3212/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3213/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3214/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3215/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3216/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3217/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3218/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3219/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3220/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3221/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3222/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3223/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3224/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3225/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3226/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3227/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3228/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3229/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3230/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3231/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3232/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3233/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3234/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3235/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3236/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3237/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3238/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3239/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3240/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3241/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3242/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3243/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3244/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3245/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3246/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3247/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3248/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3249/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3250/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3251/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3252/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3253/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3254/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3255/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3256/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3257/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3258/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3259/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3260/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3261/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3262/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3263/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3264/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3265/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3266/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3267/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3268/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3269/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3270/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3271/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3272/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3273/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3274/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3275/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3276/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3277/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3278/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3279/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3280/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3281/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3282/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3283/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3284/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3285/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3286/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3287/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3288/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3289/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3290/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3291/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3292/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3293/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3294/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3295/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3296/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3297/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3298/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3299/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3300/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3301/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3302/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3303/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3304/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3305/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3306/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3307/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3308/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3309/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3310/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3311/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3312/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3313/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3314/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3315/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3316/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3317/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3318/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3319/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3320/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3321/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3322/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3323/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3324/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3325/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3326/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3327/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3328/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3329/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3330/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3331/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3332/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3333/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3334/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3335/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3336/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3337/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3338/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3339/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3340/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3341/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3342/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3343/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3344/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3345/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3346/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3347/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3348/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3349/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3350/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3351/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3352/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3353/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3354/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3355/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3356/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3357/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3358/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3359/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3360/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3361/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3362/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3363/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3364/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3365/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3366/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3367/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3368/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3369/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3370/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3371/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3372/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3373/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3374/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3375/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3376/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3377/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3378/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3379/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3380/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3381/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3382/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3383/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3384/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3385/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3386/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3387/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3388/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3389/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3390/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3391/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3392/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3393/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3394/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3395/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3396/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3397/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3398/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3399/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3400/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3401/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3402/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3403/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3404/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3405/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3406/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3407/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3408/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3409/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3410/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3411/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3412/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3413/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3414/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3415/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3416/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3417/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3418/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3419/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3420/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3421/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3422/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3423/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3424/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3425/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3426/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3427/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3428/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3429/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3430/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3431/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3432/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3433/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3434/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3435/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3436/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3437/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3438/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3439/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3440/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3441/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3442/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3443/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3444/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3445/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3446/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3447/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3448/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3449/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3450/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3451/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3452/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3453/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3454/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3455/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3456/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3457/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3458/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3459/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3460/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3461/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3462/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3463/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3464/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3465/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3466/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3467/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3468/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3469/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3470/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3471/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3472/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3473/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3474/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3475/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3476/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3477/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3478/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3479/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3480/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3481/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3482/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3483/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3484/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3485/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3486/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3487/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3488/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3489/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3490/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3491/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3492/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3493/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3494/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3495/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3496/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3497/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3498/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3499/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3500/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3501/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3502/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3503/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3504/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3505/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3506/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3507/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3508/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3509/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3510/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3511/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3512/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3513/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3514/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3515/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3516/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3517/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3518/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3519/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3520/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3521/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3522/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3523/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3524/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3525/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3526/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3527/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3528/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3529/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3530/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3531/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3532/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3533/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3534/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3535/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3536/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3537/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3538/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3539/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3540/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3541/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3542/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3543/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3544/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3545/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3546/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3547/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3548/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3549/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3550/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3551/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3552/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3553/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3554/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3555/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3556/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3557/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3558/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3559/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3560/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3561/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3562/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3563/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3564/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3565/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3566/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3567/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3568/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3569/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3570/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3571/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3572/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3573/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3574/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3575/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3576/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3577/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3578/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3579/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3580/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3581/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3582/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3583/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3584/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3585/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3586/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3587/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3588/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3589/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3590/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3591/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3592/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3593/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3594/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3595/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3596/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3597/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3598/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3599/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3600/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3601/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3602/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3603/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3604/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3605/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3606/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3607/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3608/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3609/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3610/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3611/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3612/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3613/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3614/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3615/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3616/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3617/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3618/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3619/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3620/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3621/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3622/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3623/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3624/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3625/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3626/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3627/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3628/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3629/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3630/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3631/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3632/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3633/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3634/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3635/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3636/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3637/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3638/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3639/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3640/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3641/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3642/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3643/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3644/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3645/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3646/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3647/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3648/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3649/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3650/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3651/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3652/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3653/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3654/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3655/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3656/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3657/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3658/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3659/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3660/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3661/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3662/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3663/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3664/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3665/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3666/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3667/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3668/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3669/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3670/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3671/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3672/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3673/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3674/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3675/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3676/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3677/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3678/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3679/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3680/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3681/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3682/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3683/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3684/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3685/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3686/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3687/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3688/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3689/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3690/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3691/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3692/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3693/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3694/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3695/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3696/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3697/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3698/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3699/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3700/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3701/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3702/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3703/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3704/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3705/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3706/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3707/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3708/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3709/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3710/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3711/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3712/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3713/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3714/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3715/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3716/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3717/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3718/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3719/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3720/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3721/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3722/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3723/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3724/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3725/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3726/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3727/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3728/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3729/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3730/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3731/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3732/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3733/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3734/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3735/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3736/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3737/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3738/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3739/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3740/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3741/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3742/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3743/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3744/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3745/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3746/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3747/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3748/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3749/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3750/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3751/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3752/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3753/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3754/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3755/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3756/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3757/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3758/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3759/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3760/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3761/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3762/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3763/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3764/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3765/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3766/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3767/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3768/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3769/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3770/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3771/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3772/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3773/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3774/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3775/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3776/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3777/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3778/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3779/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3780/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3781/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3782/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3783/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3784/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3785/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3786/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3787/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3788/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3789/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3790/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3791/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3792/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3793/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3794/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3795/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3796/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3797/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3798/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3799/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3800/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3801/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3802/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3803/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3804/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3805/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3806/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3807/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3808/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3809/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3810/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3811/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3812/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3813/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3814/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3815/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3816/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3817/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3818/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3819/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3820/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3821/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3822/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3823/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3824/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3825/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3826/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3827/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3828/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3829/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3830/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3831/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3832/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3833/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3834/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3835/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3836/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3837/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3838/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3839/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3840/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3841/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3842/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3843/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3844/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3845/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3846/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3847/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3848/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3849/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3850/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3851/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3852/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3853/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3854/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3855/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3856/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3857/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3858/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3859/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3860/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3861/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3862/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3863/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3864/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3865/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3866/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3867/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3868/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3869/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3870/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3871/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3872/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3873/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3874/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3875/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3876/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3877/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3878/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3879/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3880/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3881/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3882/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3883/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3884/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3885/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3886/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3887/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3888/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3889/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3890/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3891/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3892/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3893/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3894/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3895/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3896/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3897/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3898/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3899/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3900/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3901/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3902/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3903/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3904/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3905/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3906/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3907/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3908/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3909/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3910/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3911/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3912/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3913/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3914/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3915/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3916/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3917/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3918/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3919/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3920/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3921/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3922/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3923/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3924/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3925/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3926/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3927/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3928/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3929/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3930/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3931/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3932/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3933/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3934/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3935/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3936/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3937/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3938/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3939/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3940/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3941/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3942/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3943/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3944/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3945/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3946/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3947/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3948/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3949/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3950/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3951/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3952/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3953/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3954/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3955/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3956/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3957/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3958/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3959/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3960/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3961/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3962/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3963/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3964/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3965/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3966/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3967/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3968/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3969/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3970/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3971/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3972/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3973/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3974/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3975/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3976/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3977/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3978/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3979/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3980/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3981/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3982/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3983/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3984/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3985/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3986/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3987/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3988/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3989/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3990/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3991/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3992/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3993/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3994/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3995/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3996/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3997/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3998/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3999/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4000/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4001/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4002/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4003/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4004/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4005/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4006/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4007/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4008/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4009/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4010/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4011/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4012/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4013/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4014/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4015/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4016/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4017/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4018/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4019/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4020/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4021/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4022/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4023/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4024/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4025/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4026/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4027/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4028/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4029/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4030/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4031/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4032/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4033/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4034/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4035/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4036/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4037/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4038/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4039/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4040/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4041/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4042/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4043/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4044/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4045/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4046/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4047/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4048/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4049/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4050/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4051/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4052/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4053/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4054/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4055/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4056/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4057/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4058/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4059/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4060/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4061/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4062/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4063/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4064/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4065/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4066/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4067/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4068/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4069/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4070/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4071/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4072/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4073/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4074/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4075/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4076/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4077/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4078/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4079/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4080/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4081/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4082/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4083/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4084/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4085/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4086/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4087/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4088/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4089/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4090/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4091/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4092/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4093/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4094/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4095/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4096/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4097/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4098/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4099/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4100/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4101/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4102/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4103/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4104/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4105/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4106/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4107/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4108/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4109/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4110/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4111/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4112/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4113/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4114/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4115/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4116/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4117/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4118/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4119/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4120/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4121/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4122/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4123/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4124/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4125/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4126/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4127/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4128/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4129/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4130/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4131/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4132/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4133/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4134/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4135/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4136/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4137/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4138/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4139/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4140/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4141/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4142/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4143/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4144/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4145/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4146/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4147/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4148/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4149/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4150/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4151/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4152/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4153/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4154/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4155/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4156/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4157/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4158/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4159/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4160/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4161/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4162/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4163/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4164/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4165/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4166/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4167/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4168/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4169/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4170/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4171/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4172/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4173/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4174/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4175/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4176/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4177/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4178/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4179/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4180/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4181/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4182/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4183/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4184/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4185/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4186/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4187/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4188/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4189/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4190/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4191/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4192/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4193/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4194/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4195/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4196/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4197/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4198/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4199/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4200/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4201/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4202/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4203/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4204/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4205/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4206/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4207/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4208/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4209/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4210/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4211/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4212/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4213/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4214/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4215/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4216/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4217/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4218/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4219/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4220/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4221/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4222/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4223/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4224/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4225/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4226/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4227/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4228/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4229/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4230/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4231/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4232/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4233/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4234/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4235/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4236/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4237/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4238/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4239/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4240/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4241/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4242/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4243/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4244/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4245/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4246/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4247/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4248/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4249/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4250/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4251/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4252/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4253/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4254/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4255/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4256/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4257/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4258/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4259/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4260/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4261/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4262/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4263/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4264/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4265/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4266/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4267/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4268/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4269/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4270/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4271/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4272/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4273/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4274/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4275/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4276/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4277/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4278/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4279/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4280/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4281/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4282/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4283/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4284/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4285/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4286/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4287/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4288/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4289/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4290/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4291/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4292/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4293/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4294/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4295/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4296/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4297/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4298/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4299/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4300/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4301/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4302/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4303/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4304/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4305/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4306/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4307/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4308/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4309/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4310/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4311/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4312/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4313/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4314/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4315/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4316/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4317/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4318/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4319/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4320/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4321/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4322/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4323/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4324/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4325/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4326/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4327/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4328/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4329/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4330/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4331/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4332/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4333/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4334/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4335/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4336/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4337/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4338/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4339/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4340/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4341/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4342/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4343/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4344/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4345/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4346/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4347/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4348/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4349/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4350/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4351/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4352/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4353/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4354/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4355/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4356/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4357/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4358/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4359/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4360/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4361/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4362/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4363/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4364/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4365/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4366/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4367/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4368/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4369/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4370/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4371/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4372/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4373/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4374/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4375/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4376/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4377/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4378/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4379/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4380/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4381/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4382/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4383/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4384/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4385/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4386/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4387/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4388/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4389/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4390/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4391/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4392/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4393/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4394/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4395/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4396/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4397/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4398/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4399/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4400/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4401/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4402/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4403/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4404/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4405/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4406/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4407/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4408/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4409/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4410/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4411/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4412/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4413/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4414/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4415/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4416/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4417/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4418/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4419/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4420/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4421/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4422/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4423/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4424/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4425/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4426/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4427/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4428/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4429/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4430/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4431/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4432/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4433/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4434/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4435/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4436/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4437/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4438/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4439/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4440/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4441/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4442/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4443/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4444/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4445/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4446/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4447/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4448/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4449/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4450/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4451/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4452/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4453/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4454/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4455/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4456/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4457/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4458/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4459/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4460/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4461/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4462/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4463/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4464/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4465/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4466/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4467/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4468/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4469/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4470/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4471/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4472/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4473/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4474/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4475/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4476/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4477/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4478/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4479/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4480/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4481/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4482/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4483/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4484/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4485/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4486/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4487/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4488/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4489/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4490/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4491/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4492/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4493/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4494/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4495/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4496/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4497/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4498/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4499/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4500/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4501/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4502/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4503/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4504/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4505/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4506/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4507/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4508/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4509/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4510/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4511/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4512/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4513/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4514/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4515/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4516/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4517/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4518/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4519/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4520/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4521/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4522/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4523/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4524/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4525/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4526/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4527/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4528/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4529/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4530/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4531/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4532/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4533/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4534/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4535/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4536/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4537/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4538/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4539/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4540/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4541/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4542/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4543/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4544/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4545/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4546/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4547/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4548/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4549/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4550/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4551/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4552/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4553/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4554/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4555/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4556/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4557/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4558/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4559/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4560/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4561/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4562/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4563/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4564/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4565/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4566/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4567/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4568/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4569/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4570/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4571/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4572/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4573/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4574/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4575/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4576/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4577/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4578/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4579/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4580/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4581/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4582/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4583/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4584/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4585/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4586/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4587/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4588/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4589/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4590/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4591/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4592/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4593/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4594/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4595/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4596/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4597/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4598/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4599/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4600/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4601/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4602/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4603/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4604/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4605/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4606/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4607/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4608/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4609/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4610/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4611/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4612/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4613/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4614/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4615/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4616/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4617/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4618/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4619/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4620/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4621/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4622/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4623/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4624/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4625/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4626/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4627/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4628/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4629/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4630/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4631/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4632/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4633/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4634/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4635/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4636/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4637/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4638/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4639/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4640/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4641/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4642/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4643/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4644/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4645/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4646/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4647/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4648/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4649/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4650/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4651/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4652/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4653/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4654/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4655/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4656/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4657/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4658/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4659/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4660/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4661/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4662/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4663/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4664/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4665/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4666/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4667/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4668/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4669/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4670/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4671/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4672/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4673/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4674/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4675/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4676/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4677/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4678/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4679/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4680/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4681/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4682/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4683/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4684/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4685/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4686/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4687/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4688/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4689/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4690/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4691/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4692/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4693/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4694/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4695/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4696/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4697/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4698/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4699/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4700/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4701/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4702/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4703/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4704/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4705/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4706/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4707/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4708/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4709/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4710/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4711/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4712/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4713/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4714/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4715/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4716/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4717/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4718/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4719/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4720/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4721/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4722/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4723/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4724/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4725/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4726/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4727/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4728/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4729/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4730/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4731/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4732/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4733/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4734/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4735/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4736/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4737/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4738/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4739/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4740/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4741/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4742/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4743/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4744/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4745/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4746/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4747/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4748/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4749/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4750/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4751/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4752/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4753/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4754/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4755/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4756/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4757/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4758/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4759/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4760/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4761/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4762/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4763/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4764/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4765/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4766/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4767/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4768/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4769/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4770/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4771/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4772/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4773/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4774/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4775/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4776/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4777/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4778/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4779/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4780/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4781/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4782/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4783/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4784/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4785/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4786/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4787/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4788/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4789/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4790/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4791/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4792/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4793/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4794/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4795/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4796/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4797/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4798/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4799/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4800/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4801/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4802/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4803/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4804/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4805/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4806/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4807/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4808/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4809/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4810/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4811/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4812/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4813/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4814/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4815/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4816/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4817/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4818/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4819/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4820/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4821/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4822/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4823/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4824/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4825/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4826/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4827/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4828/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4829/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4830/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4831/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4832/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4833/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4834/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4835/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4836/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4837/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4838/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4839/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4840/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4841/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4842/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4843/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4844/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4845/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4846/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4847/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4848/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4849/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4850/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4851/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4852/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4853/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4854/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4855/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4856/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4857/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4858/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4859/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4860/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4861/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4862/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4863/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4864/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4865/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4866/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4867/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4868/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4869/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4870/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4871/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4872/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4873/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4874/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4875/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4876/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4877/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4878/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4879/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4880/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4881/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4882/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4883/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4884/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4885/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4886/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4887/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4888/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4889/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4890/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4891/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4892/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4893/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4894/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4895/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4896/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4897/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4898/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4899/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4900/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4901/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4902/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4903/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4904/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4905/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4906/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4907/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4908/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4909/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4910/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4911/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4912/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4913/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4914/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4915/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4916/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4917/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4918/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4919/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4920/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4921/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4922/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4923/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4924/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4925/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4926/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4927/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4928/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4929/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4930/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4931/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4932/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4933/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4934/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4935/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4936/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4937/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4938/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4939/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4940/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4941/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4942/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4943/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4944/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4945/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4946/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4947/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4948/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4949/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4950/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4951/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4952/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4953/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4954/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4955/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4956/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4957/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4958/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4959/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4960/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4961/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4962/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4963/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4964/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4965/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4966/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4967/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4968/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4969/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4970/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4971/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4972/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4973/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4974/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4975/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4976/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4977/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4978/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4979/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4980/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4981/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4982/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4983/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4984/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4985/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4986/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4987/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4988/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4989/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4990/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4991/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4992/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4993/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4994/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4995/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4996/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4997/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4998/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4999/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5000/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5001/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5002/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5003/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5004/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5005/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5006/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5007/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5008/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5009/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5010/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5011/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5012/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5013/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5014/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5015/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5016/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5017/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5018/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5019/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5020/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5021/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5022/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5023/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5024/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5025/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5026/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5027/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5028/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5029/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5030/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5031/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5032/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5033/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5034/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5035/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5036/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5037/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5038/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5039/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5040/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5041/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5042/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5043/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5044/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5045/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5046/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5047/10000..  Training Loss: 0.170.. \n",
      "Epoch: 5048/10000..  Training Loss: 0.168.. \n",
      "Epoch: 5049/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5050/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5051/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5052/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5053/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5054/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5055/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5056/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5057/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5058/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5059/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5060/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5061/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5062/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5063/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5064/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5065/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5066/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5067/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5068/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5069/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5070/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5071/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5072/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5073/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5074/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5075/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5076/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5077/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5078/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5079/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5080/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5081/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5082/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5083/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5084/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5085/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5086/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5087/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5088/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5089/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5090/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5091/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5092/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5093/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5094/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5095/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5096/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5097/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5098/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5099/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5100/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5101/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5102/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5103/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5104/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5105/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5106/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5107/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5108/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5109/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5110/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5111/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5112/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5113/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5114/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5115/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5116/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5117/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5118/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5119/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5120/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5121/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5122/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5123/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5124/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5125/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5126/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5127/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5128/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5129/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5130/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5131/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5132/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5133/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5134/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5135/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5136/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5137/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5138/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5139/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5140/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5141/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5142/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5143/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5144/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5145/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5146/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5147/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5148/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5149/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5150/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5151/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5152/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5153/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5154/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5155/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5156/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5157/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5158/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5159/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5160/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5161/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5162/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5163/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5164/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5165/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5166/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5167/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5168/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5169/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5170/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5171/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5172/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5173/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5174/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5175/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5176/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5177/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5178/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5179/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5180/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5181/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5182/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5183/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5184/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5185/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5186/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5187/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5188/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5189/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5190/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5191/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5192/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5193/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5194/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5195/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5196/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5197/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5198/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5199/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5200/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5201/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5202/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5203/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5204/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5205/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5206/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5207/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5208/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5209/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5210/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5211/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5212/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5213/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5214/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5215/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5216/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5217/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5218/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5219/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5220/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5221/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5222/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5223/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5224/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5225/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5226/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5227/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5228/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5229/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5230/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5231/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5232/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5233/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5234/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5235/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5236/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5237/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5238/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5239/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5240/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5241/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5242/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5243/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5244/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5245/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5246/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5247/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5248/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5249/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5250/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5251/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5252/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5253/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5254/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5255/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5256/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5257/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5258/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5259/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5260/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5261/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5262/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5263/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5264/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5265/10000..  Training Loss: 0.169.. \n",
      "Epoch: 5266/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5267/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5268/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5269/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5270/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5271/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5272/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5273/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5274/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5275/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5276/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5277/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5278/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5279/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5280/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5281/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5282/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5283/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5284/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5285/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5286/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5287/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5288/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5289/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5290/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5291/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5292/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5293/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5294/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5295/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5296/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5297/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5298/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5299/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5300/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5301/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5302/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5303/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5304/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5305/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5306/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5307/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5308/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5309/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5310/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5311/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5312/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5313/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5314/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5315/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5316/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5317/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5318/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5319/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5320/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5321/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5322/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5323/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5324/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5325/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5326/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5327/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5328/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5329/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5330/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5331/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5332/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5333/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5334/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5335/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5336/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5337/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5338/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5339/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5340/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5341/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5342/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5343/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5344/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5345/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5346/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5347/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5348/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5349/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5350/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5351/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5352/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5353/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5354/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5355/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5356/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5357/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5358/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5359/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5360/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5361/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5362/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5363/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5364/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5365/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5366/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5367/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5368/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5369/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5370/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5371/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5372/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5373/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5374/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5375/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5376/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5377/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5378/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5379/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5380/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5381/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5382/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5383/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5384/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5385/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5386/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5387/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5388/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5389/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5390/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5391/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5392/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5393/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5394/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5395/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5396/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5397/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5398/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5399/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5400/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5401/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5402/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5403/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5404/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5405/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5406/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5407/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5408/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5409/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5410/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5411/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5412/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5413/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5414/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5415/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5416/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5417/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5418/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5419/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5420/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5421/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5422/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5423/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5424/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5425/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5426/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5427/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5428/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5429/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5430/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5431/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5432/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5433/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5434/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5435/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5436/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5437/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5438/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5439/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5440/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5441/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5442/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5443/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5444/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5445/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5446/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5447/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5448/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5449/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5450/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5451/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5452/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5453/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5454/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5455/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5456/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5457/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5458/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5459/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5460/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5461/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5462/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5463/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5464/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5465/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5466/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5467/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5468/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5469/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5470/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5471/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5472/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5473/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5474/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5475/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5476/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5477/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5478/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5479/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5480/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5481/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5482/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5483/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5484/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5485/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5486/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5487/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5488/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5489/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5490/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5491/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5492/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5493/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5494/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5495/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5496/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5497/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5498/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5499/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5500/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5501/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5502/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5503/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5504/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5505/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5506/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5507/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5508/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5509/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5510/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5511/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5512/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5513/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5514/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5515/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5516/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5517/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5518/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5519/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5520/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5521/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5522/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5523/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5524/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5525/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5526/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5527/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5528/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5529/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5530/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5531/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5532/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5533/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5534/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5535/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5536/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5537/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5538/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5539/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5540/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5541/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5542/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5543/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5544/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5545/10000..  Training Loss: 0.167.. \n",
      "Epoch: 5546/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5547/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5548/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5549/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5550/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5551/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5552/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5553/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5554/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5555/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5556/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5557/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5558/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5559/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5560/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5561/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5562/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5563/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5564/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5565/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5566/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5567/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5568/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5569/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5570/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5571/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5572/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5573/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5574/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5575/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5576/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5577/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5578/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5579/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5580/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5581/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5582/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5583/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5584/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5585/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5586/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5587/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5588/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5589/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5590/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5591/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5592/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5593/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5594/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5595/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5596/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5597/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5598/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5599/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5600/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5601/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5602/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5603/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5604/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5605/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5606/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5607/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5608/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5609/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5610/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5611/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5612/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5613/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5614/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5615/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5616/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5617/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5618/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5619/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5620/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5621/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5622/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5623/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5624/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5625/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5626/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5627/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5628/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5629/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5630/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5631/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5632/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5633/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5634/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5635/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5636/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5637/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5638/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5639/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5640/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5641/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5642/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5643/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5644/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5645/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5646/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5647/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5648/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5649/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5650/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5651/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5652/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5653/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5654/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5655/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5656/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5657/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5658/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5659/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5660/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5661/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5662/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5663/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5664/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5665/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5666/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5667/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5668/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5669/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5670/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5671/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5672/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5673/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5674/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5675/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5676/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5677/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5678/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5679/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5680/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5681/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5682/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5683/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5684/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5685/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5686/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5687/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5688/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5689/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5690/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5691/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5692/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5693/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5694/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5695/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5696/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5697/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5698/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5699/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5700/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5701/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5702/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5703/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5704/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5705/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5706/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5707/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5708/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5709/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5710/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5711/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5712/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5713/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5714/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5715/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5716/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5717/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5718/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5719/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5720/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5721/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5722/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5723/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5724/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5725/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5726/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5727/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5728/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5729/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5730/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5731/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5732/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5733/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5734/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5735/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5736/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5737/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5738/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5739/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5740/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5741/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5742/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5743/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5744/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5745/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5746/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5747/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5748/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5749/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5750/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5751/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5752/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5753/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5754/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5755/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5756/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5757/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5758/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5759/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5760/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5761/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5762/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5763/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5764/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5765/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5766/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5767/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5768/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5769/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5770/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5771/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5772/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5773/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5774/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5775/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5776/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5777/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5778/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5779/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5780/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5781/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5782/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5783/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5784/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5785/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5786/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5787/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5788/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5789/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5790/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5791/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5792/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5793/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5794/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5795/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5796/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5797/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5798/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5799/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5800/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5801/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5802/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5803/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5804/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5805/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5806/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5807/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5808/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5809/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5810/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5811/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5812/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5813/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5814/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5815/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5816/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5817/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5818/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5819/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5820/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5821/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5822/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5823/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5824/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5825/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5826/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5827/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5828/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5829/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5830/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5831/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5832/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5833/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5834/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5835/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5836/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5837/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5838/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5839/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5840/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5841/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5842/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5843/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5844/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5845/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5846/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5847/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5848/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5849/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5850/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5851/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5852/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5853/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5854/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5855/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5856/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5857/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5858/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5859/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5860/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5861/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5862/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5863/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5864/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5865/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5866/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5867/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5868/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5869/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5870/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5871/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5872/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5873/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5874/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5875/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5876/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5877/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5878/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5879/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5880/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5881/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5882/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5883/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5884/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5885/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5886/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5887/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5888/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5889/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5890/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5891/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5892/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5893/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5894/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5895/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5896/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5897/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5898/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5899/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5900/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5901/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5902/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5903/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5904/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5905/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5906/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5907/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5908/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5909/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5910/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5911/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5912/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5913/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5914/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5915/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5916/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5917/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5918/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5919/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5920/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5921/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5922/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5923/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5924/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5925/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5926/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5927/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5928/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5929/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5930/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5931/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5932/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5933/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5934/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5935/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5936/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5937/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5938/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5939/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5940/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5941/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5942/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5943/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5944/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5945/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5946/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5947/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5948/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5949/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5950/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5951/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5952/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5953/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5954/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5955/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5956/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5957/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5958/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5959/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5960/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5961/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5962/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5963/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5964/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5965/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5966/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5967/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5968/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5969/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5970/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5971/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5972/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5973/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5974/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5975/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5976/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5977/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5978/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5979/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5980/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5981/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5982/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5983/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5984/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5985/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5986/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5987/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5988/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5989/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5990/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5991/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5992/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5993/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5994/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5995/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5996/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5997/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5998/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5999/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6000/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6001/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6002/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6003/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6004/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6005/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6006/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6007/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6008/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6009/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6010/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6011/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6012/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6013/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6014/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6015/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6016/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6017/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6018/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6019/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6020/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6021/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6022/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6023/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6024/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6025/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6026/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6027/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6028/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6029/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6030/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6031/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6032/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6033/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6034/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6035/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6036/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6037/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6038/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6039/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6040/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6041/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6042/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6043/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6044/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6045/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6046/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6047/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6048/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6049/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6050/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6051/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6052/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6053/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6054/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6055/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6056/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6057/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6058/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6059/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6060/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6061/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6062/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6063/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6064/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6065/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6066/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6067/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6068/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6069/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6070/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6071/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6072/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6073/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6074/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6075/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6076/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6077/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6078/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6079/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6080/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6081/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6082/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6083/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6084/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6085/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6086/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6087/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6088/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6089/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6090/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6091/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6092/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6093/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6094/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6095/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6096/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6097/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6098/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6099/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6100/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6101/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6102/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6103/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6104/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6105/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6106/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6107/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6108/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6109/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6110/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6111/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6112/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6113/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6114/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6115/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6116/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6117/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6118/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6119/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6120/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6121/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6122/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6123/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6124/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6125/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6126/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6127/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6128/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6129/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6130/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6131/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6132/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6133/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6134/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6135/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6136/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6137/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6138/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6139/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6140/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6141/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6142/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6143/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6144/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6145/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6146/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6147/10000..  Training Loss: 0.161.. \n",
      "Epoch: 6148/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6149/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6150/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6151/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6152/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6153/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6154/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6155/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6156/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6157/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6158/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6159/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6160/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6161/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6162/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6163/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6164/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6165/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6166/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6167/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6168/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6169/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6170/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6171/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6172/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6173/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6174/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6175/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6176/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6177/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6178/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6179/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6180/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6181/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6182/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6183/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6184/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6185/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6186/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6187/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6188/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6189/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6190/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6191/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6192/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6193/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6194/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6195/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6196/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6197/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6198/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6199/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6200/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6201/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6202/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6203/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6204/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6205/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6206/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6207/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6208/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6209/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6210/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6211/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6212/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6213/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6214/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6215/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6216/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6217/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6218/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6219/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6220/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6221/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6222/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6223/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6224/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6225/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6226/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6227/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6228/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6229/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6230/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6231/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6232/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6233/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6234/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6235/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6236/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6237/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6238/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6239/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6240/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6241/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6242/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6243/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6244/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6245/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6246/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6247/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6248/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6249/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6250/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6251/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6252/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6253/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6254/10000..  Training Loss: 0.163.. \n",
      "Epoch: 6255/10000..  Training Loss: 0.160.. \n",
      "Epoch: 6256/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6257/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6258/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6259/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6260/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6261/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6262/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6263/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6264/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6265/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6266/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6267/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6268/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6269/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6270/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6271/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6272/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6273/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6274/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6275/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6276/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6277/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6278/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6279/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6280/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6281/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6282/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6283/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6284/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6285/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6286/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6287/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6288/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6289/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6290/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6291/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6292/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6293/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6294/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6295/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6296/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6297/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6298/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6299/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6300/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6301/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6302/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6303/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6304/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6305/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6306/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6307/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6308/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6309/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6310/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6311/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6312/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6313/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6314/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6315/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6316/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6317/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6318/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6319/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6320/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6321/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6322/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6323/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6324/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6325/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6326/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6327/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6328/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6329/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6330/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6331/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6332/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6333/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6334/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6335/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6336/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6337/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6338/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6339/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6340/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6341/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6342/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6343/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6344/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6345/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6346/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6347/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6348/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6349/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6350/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6351/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6352/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6353/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6354/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6355/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6356/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6357/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6358/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6359/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6360/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6361/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6362/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6363/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6364/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6365/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6366/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6367/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6368/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6369/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6370/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6371/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6372/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6373/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6374/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6375/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6376/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6377/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6378/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6379/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6380/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6381/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6382/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6383/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6384/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6385/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6386/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6387/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6388/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6389/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6390/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6391/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6392/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6393/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6394/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6395/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6396/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6397/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6398/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6399/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6400/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6401/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6402/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6403/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6404/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6405/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6406/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6407/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6408/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6409/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6410/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6411/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6412/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6413/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6414/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6415/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6416/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6417/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6418/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6419/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6420/10000..  Training Loss: 0.162.. \n",
      "Epoch: 6421/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6422/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6423/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6424/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6425/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6426/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6427/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6428/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6429/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6430/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6431/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6432/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6433/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6434/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6435/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6436/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6437/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6438/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6439/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6440/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6441/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6442/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6443/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6444/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6445/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6446/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6447/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6448/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6449/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6450/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6451/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6452/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6453/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6454/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6455/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6456/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6457/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6458/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6459/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6460/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6461/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6462/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6463/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6464/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6465/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6466/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6467/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6468/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6469/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6470/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6471/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6472/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6473/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6474/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6475/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6476/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6477/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6478/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6479/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6480/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6481/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6482/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6483/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6484/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6485/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6486/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6487/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6488/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6489/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6490/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6491/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6492/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6493/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6494/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6495/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6496/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6497/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6498/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6499/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6500/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6501/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6502/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6503/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6504/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6505/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6506/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6507/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6508/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6509/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6510/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6511/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6512/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6513/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6514/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6515/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6516/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6517/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6518/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6519/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6520/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6521/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6522/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6523/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6524/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6525/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6526/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6527/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6528/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6529/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6530/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6531/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6532/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6533/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6534/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6535/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6536/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6537/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6538/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6539/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6540/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6541/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6542/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6543/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6544/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6545/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6546/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6547/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6548/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6549/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6550/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6551/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6552/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6553/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6554/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6555/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6556/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6557/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6558/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6559/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6560/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6561/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6562/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6563/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6564/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6565/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6566/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6567/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6568/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6569/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6570/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6571/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6572/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6573/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6574/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6575/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6576/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6577/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6578/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6579/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6580/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6581/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6582/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6583/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6584/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6585/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6586/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6587/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6588/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6589/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6590/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6591/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6592/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6593/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6594/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6595/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6596/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6597/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6598/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6599/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6600/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6601/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6602/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6603/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6604/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6605/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6606/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6607/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6608/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6609/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6610/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6611/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6612/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6613/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6614/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6615/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6616/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6617/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6618/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6619/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6620/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6621/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6622/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6623/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6624/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6625/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6626/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6627/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6628/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6629/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6630/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6631/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6632/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6633/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6634/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6635/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6636/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6637/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6638/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6639/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6640/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6641/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6642/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6643/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6644/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6645/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6646/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6647/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6648/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6649/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6650/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6651/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6652/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6653/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6654/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6655/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6656/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6657/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6658/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6659/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6660/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6661/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6662/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6663/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6664/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6665/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6666/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6667/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6668/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6669/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6670/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6671/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6672/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6673/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6674/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6675/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6676/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6677/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6678/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6679/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6680/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6681/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6682/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6683/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6684/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6685/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6686/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6687/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6688/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6689/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6690/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6691/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6692/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6693/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6694/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6695/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6696/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6697/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6698/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6699/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6700/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6701/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6702/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6703/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6704/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6705/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6706/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6707/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6708/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6709/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6710/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6711/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6712/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6713/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6714/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6715/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6716/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6717/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6718/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6719/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6720/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6721/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6722/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6723/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6724/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6725/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6726/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6727/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6728/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6729/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6730/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6731/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6732/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6733/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6734/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6735/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6736/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6737/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6738/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6739/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6740/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6741/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6742/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6743/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6744/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6745/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6746/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6747/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6748/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6749/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6750/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6751/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6752/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6753/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6754/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6755/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6756/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6757/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6758/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6759/10000..  Training Loss: 0.159.. \n",
      "Epoch: 6760/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6761/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6762/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6763/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6764/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6765/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6766/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6767/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6768/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6769/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6770/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6771/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6772/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6773/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6774/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6775/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6776/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6777/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6778/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6779/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6780/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6781/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6782/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6783/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6784/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6785/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6786/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6787/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6788/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6789/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6790/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6791/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6792/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6793/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6794/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6795/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6796/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6797/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6798/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6799/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6800/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6801/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6802/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6803/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6804/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6805/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6806/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6807/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6808/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6809/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6810/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6811/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6812/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6813/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6814/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6815/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6816/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6817/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6818/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6819/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6820/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6821/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6822/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6823/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6824/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6825/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6826/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6827/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6828/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6829/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6830/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6831/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6832/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6833/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6834/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6835/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6836/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6837/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6838/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6839/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6840/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6841/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6842/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6843/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6844/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6845/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6846/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6847/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6848/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6849/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6850/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6851/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6852/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6853/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6854/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6855/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6856/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6857/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6858/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6859/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6860/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6861/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6862/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6863/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6864/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6865/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6866/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6867/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6868/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6869/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6870/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6871/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6872/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6873/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6874/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6875/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6876/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6877/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6878/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6879/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6880/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6881/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6882/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6883/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6884/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6885/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6886/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6887/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6888/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6889/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6890/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6891/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6892/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6893/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6894/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6895/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6896/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6897/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6898/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6899/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6900/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6901/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6902/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6903/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6904/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6905/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6906/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6907/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6908/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6909/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6910/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6911/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6912/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6913/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6914/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6915/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6916/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6917/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6918/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6919/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6920/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6921/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6922/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6923/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6924/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6925/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6926/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6927/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6928/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6929/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6930/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6931/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6932/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6933/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6934/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6935/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6936/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6937/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6938/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6939/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6940/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6941/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6942/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6943/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6944/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6945/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6946/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6947/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6948/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6949/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6950/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6951/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6952/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6953/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6954/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6955/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6956/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6957/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6958/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6959/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6960/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6961/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6962/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6963/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6964/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6965/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6966/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6967/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6968/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6969/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6970/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6971/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6972/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6973/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6974/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6975/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6976/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6977/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6978/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6979/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6980/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6981/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6982/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6983/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6984/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6985/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6986/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6987/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6988/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6989/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6990/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6991/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6992/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6993/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6994/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6995/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6996/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6997/10000..  Training Loss: 0.158.. \n",
      "Epoch: 6998/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6999/10000..  Training Loss: 0.156.. \n",
      "Epoch: 7000/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7001/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7002/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7003/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7004/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7005/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7006/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7007/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7008/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7009/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7010/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7011/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7012/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7013/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7014/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7015/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7016/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7017/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7018/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7019/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7020/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7021/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7022/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7023/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7024/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7025/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7026/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7027/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7028/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7029/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7030/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7031/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7032/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7033/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7034/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7035/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7036/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7037/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7038/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7039/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7040/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7041/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7042/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7043/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7044/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7045/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7046/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7047/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7048/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7049/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7050/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7051/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7052/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7053/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7054/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7055/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7056/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7057/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7058/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7059/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7060/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7061/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7062/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7063/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7064/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7065/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7066/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7067/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7068/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7069/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7070/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7071/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7072/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7073/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7074/10000..  Training Loss: 0.158.. \n",
      "Epoch: 7075/10000..  Training Loss: 0.156.. \n",
      "Epoch: 7076/10000..  Training Loss: 0.155.. \n",
      "Epoch: 7077/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7078/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7079/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7080/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7081/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7082/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7083/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7084/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7085/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7086/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7087/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7088/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7089/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7090/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7091/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7092/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7093/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7094/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7095/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7096/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7097/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7098/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7099/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7100/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7101/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7102/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7103/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7104/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7105/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7106/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7107/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7108/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7109/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7110/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7111/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7112/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7113/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7114/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7115/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7116/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7117/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7118/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7119/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7120/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7121/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7122/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7123/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7124/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7125/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7126/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7127/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7128/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7129/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7130/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7131/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7132/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7133/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7134/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7135/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7136/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7137/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7138/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7139/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7140/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7141/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7142/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7143/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7144/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7145/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7146/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7147/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7148/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7149/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7150/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7151/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7152/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7153/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7154/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7155/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7156/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7157/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7158/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7159/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7160/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7161/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7162/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7163/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7164/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7165/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7166/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7167/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7168/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7169/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7170/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7171/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7172/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7173/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7174/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7175/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7176/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7177/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7178/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7179/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7180/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7181/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7182/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7183/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7184/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7185/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7186/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7187/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7188/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7189/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7190/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7191/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7192/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7193/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7194/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7195/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7196/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7197/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7198/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7199/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7200/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7201/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7202/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7203/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7204/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7205/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7206/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7207/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7208/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7209/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7210/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7211/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7212/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7213/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7214/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7215/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7216/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7217/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7218/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7219/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7220/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7221/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7222/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7223/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7224/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7225/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7226/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7227/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7228/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7229/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7230/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7231/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7232/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7233/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7234/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7235/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7236/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7237/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7238/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7239/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7240/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7241/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7242/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7243/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7244/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7245/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7246/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7247/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7248/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7249/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7250/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7251/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7252/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7253/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7254/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7255/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7256/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7257/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7258/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7259/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7260/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7261/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7262/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7263/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7264/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7265/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7266/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7267/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7268/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7269/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7270/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7271/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7272/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7273/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7274/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7275/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7276/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7277/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7278/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7279/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7280/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7281/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7282/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7283/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7284/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7285/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7286/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7287/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7288/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7289/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7290/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7291/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7292/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7293/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7294/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7295/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7296/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7297/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7298/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7299/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7300/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7301/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7302/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7303/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7304/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7305/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7306/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7307/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7308/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7309/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7310/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7311/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7312/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7313/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7314/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7315/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7316/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7317/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7318/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7319/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7320/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7321/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7322/10000..  Training Loss: 0.155.. \n",
      "Epoch: 7323/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7324/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7325/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7326/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7327/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7328/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7329/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7330/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7331/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7332/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7333/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7334/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7335/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7336/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7337/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7338/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7339/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7340/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7341/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7342/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7343/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7344/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7345/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7346/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7347/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7348/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7349/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7350/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7351/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7352/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7353/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7354/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7355/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7356/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7357/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7358/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7359/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7360/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7361/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7362/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7363/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7364/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7365/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7366/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7367/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7368/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7369/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7370/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7371/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7372/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7373/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7374/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7375/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7376/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7377/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7378/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7379/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7380/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7381/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7382/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7383/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7384/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7385/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7386/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7387/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7388/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7389/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7390/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7391/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7392/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7393/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7394/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7395/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7396/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7397/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7398/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7399/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7400/10000..  Training Loss: 0.156.. \n",
      "Epoch: 7401/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7402/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7403/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7404/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7405/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7406/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7407/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7408/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7409/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7410/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7411/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7412/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7413/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7414/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7415/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7416/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7417/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7418/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7419/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7420/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7421/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7422/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7423/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7424/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7425/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7426/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7427/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7428/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7429/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7430/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7431/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7432/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7433/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7434/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7435/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7436/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7437/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7438/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7439/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7440/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7441/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7442/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7443/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7444/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7445/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7446/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7447/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7448/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7449/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7450/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7451/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7452/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7453/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7454/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7455/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7456/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7457/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7458/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7459/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7460/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7461/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7462/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7463/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7464/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7465/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7466/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7467/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7468/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7469/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7470/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7471/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7472/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7473/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7474/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7475/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7476/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7477/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7478/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7479/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7480/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7481/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7482/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7483/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7484/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7485/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7486/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7487/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7488/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7489/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7490/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7491/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7492/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7493/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7494/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7495/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7496/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7497/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7498/10000..  Training Loss: 0.156.. \n",
      "Epoch: 7499/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7500/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7501/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7502/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7503/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7504/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7505/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7506/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7507/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7508/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7509/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7510/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7511/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7512/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7513/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7514/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7515/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7516/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7517/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7518/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7519/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7520/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7521/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7522/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7523/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7524/10000..  Training Loss: 0.155.. \n",
      "Epoch: 7525/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7526/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7527/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7528/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7529/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7530/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7531/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7532/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7533/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7534/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7535/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7536/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7537/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7538/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7539/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7540/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7541/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7542/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7543/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7544/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7545/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7546/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7547/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7548/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7549/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7550/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7551/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7552/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7553/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7554/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7555/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7556/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7557/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7558/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7559/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7560/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7561/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7562/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7563/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7564/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7565/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7566/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7567/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7568/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7569/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7570/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7571/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7572/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7573/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7574/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7575/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7576/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7577/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7578/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7579/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7580/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7581/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7582/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7583/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7584/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7585/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7586/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7587/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7588/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7589/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7590/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7591/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7592/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7593/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7594/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7595/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7596/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7597/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7598/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7599/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7600/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7601/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7602/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7603/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7604/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7605/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7606/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7607/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7608/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7609/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7610/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7611/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7612/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7613/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7614/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7615/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7616/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7617/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7618/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7619/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7620/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7621/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7622/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7623/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7624/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7625/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7626/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7627/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7628/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7629/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7630/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7631/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7632/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7633/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7634/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7635/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7636/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7637/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7638/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7639/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7640/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7641/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7642/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7643/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7644/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7645/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7646/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7647/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7648/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7649/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7650/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7651/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7652/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7653/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7654/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7655/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7656/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7657/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7658/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7659/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7660/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7661/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7662/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7663/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7664/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7665/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7666/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7667/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7668/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7669/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7670/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7671/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7672/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7673/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7674/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7675/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7676/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7677/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7678/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7679/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7680/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7681/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7682/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7683/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7684/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7685/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7686/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7687/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7688/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7689/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7690/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7691/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7692/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7693/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7694/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7695/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7696/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7697/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7698/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7699/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7700/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7701/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7702/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7703/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7704/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7705/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7706/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7707/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7708/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7709/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7710/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7711/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7712/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7713/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7714/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7715/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7716/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7717/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7718/10000..  Training Loss: 0.155.. \n",
      "Epoch: 7719/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7720/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7721/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7722/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7723/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7724/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7725/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7726/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7727/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7728/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7729/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7730/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7731/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7732/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7733/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7734/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7735/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7736/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7737/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7738/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7739/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7740/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7741/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7742/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7743/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7744/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7745/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7746/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7747/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7748/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7749/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7750/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7751/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7752/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7753/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7754/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7755/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7756/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7757/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7758/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7759/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7760/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7761/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7762/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7763/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7764/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7765/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7766/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7767/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7768/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7769/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7770/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7771/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7772/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7773/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7774/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7775/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7776/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7777/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7778/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7779/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7780/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7781/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7782/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7783/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7784/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7785/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7786/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7787/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7788/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7789/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7790/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7791/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7792/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7793/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7794/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7795/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7796/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7797/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7798/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7799/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7800/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7801/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7802/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7803/10000..  Training Loss: 0.154.. \n",
      "Epoch: 7804/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7805/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7806/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7807/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7808/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7809/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7810/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7811/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7812/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7813/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7814/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7815/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7816/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7817/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7818/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7819/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7820/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7821/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7822/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7823/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7824/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7825/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7826/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7827/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7828/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7829/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7830/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7831/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7832/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7833/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7834/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7835/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7836/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7837/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7838/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7839/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7840/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7841/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7842/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7843/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7844/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7845/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7846/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7847/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7848/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7849/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7850/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7851/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7852/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7853/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7854/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7855/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7856/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7857/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7858/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7859/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7860/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7861/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7862/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7863/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7864/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7865/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7866/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7867/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7868/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7869/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7870/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7871/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7872/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7873/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7874/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7875/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7876/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7877/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7878/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7879/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7880/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7881/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7882/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7883/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7884/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7885/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7886/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7887/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7888/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7889/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7890/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7891/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7892/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7893/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7894/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7895/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7896/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7897/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7898/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7899/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7900/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7901/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7902/10000..  Training Loss: 0.153.. \n",
      "Epoch: 7903/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7904/10000..  Training Loss: 0.151.. \n",
      "Epoch: 7905/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7906/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7907/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7908/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7909/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7910/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7911/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7912/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7913/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7914/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7915/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7916/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7917/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7918/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7919/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7920/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7921/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7922/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7923/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7924/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7925/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7926/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7927/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7928/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7929/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7930/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7931/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7932/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7933/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7934/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7935/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7936/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7937/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7938/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7939/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7940/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7941/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7942/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7943/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7944/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7945/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7946/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7947/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7948/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7949/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7950/10000..  Training Loss: 0.150.. \n",
      "Epoch: 7951/10000..  Training Loss: 0.152.. \n",
      "Epoch: 7952/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7953/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7954/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7955/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7956/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7957/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7958/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7959/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7960/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7961/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7962/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7963/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7964/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7965/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7966/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7967/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7968/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7969/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7970/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7971/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7972/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7973/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7974/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7975/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7976/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7977/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7978/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7979/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7980/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7981/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7982/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7983/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7984/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7985/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7986/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7987/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7988/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7989/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7990/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7991/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7992/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7993/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7994/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7995/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7996/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7997/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7998/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7999/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8000/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8001/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8002/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8003/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8004/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8005/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8006/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8007/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8008/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8009/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8010/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8011/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8012/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8013/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8014/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8015/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8016/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8017/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8018/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8019/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8020/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8021/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8022/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8023/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8024/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8025/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8026/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8027/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8028/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8029/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8030/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8031/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8032/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8033/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8034/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8035/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8036/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8037/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8038/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8039/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8040/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8041/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8042/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8043/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8044/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8045/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8046/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8047/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8048/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8049/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8050/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8051/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8052/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8053/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8054/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8055/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8056/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8057/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8058/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8059/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8060/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8061/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8062/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8063/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8064/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8065/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8066/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8067/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8068/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8069/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8070/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8071/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8072/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8073/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8074/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8075/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8076/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8077/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8078/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8079/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8080/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8081/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8082/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8083/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8084/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8085/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8086/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8087/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8088/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8089/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8090/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8091/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8092/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8093/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8094/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8095/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8096/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8097/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8098/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8099/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8100/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8101/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8102/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8103/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8104/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8105/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8106/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8107/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8108/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8109/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8110/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8111/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8112/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8113/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8114/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8115/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8116/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8117/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8118/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8119/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8120/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8121/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8122/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8123/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8124/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8125/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8126/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8127/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8128/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8129/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8130/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8131/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8132/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8133/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8134/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8135/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8136/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8137/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8138/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8139/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8140/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8141/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8142/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8143/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8144/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8145/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8146/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8147/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8148/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8149/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8150/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8151/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8152/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8153/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8154/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8155/10000..  Training Loss: 0.150.. \n",
      "Epoch: 8156/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8157/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8158/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8159/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8160/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8161/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8162/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8163/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8164/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8165/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8166/10000..  Training Loss: 0.150.. \n",
      "Epoch: 8167/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8168/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8169/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8170/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8171/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8172/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8173/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8174/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8175/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8176/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8177/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8178/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8179/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8180/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8181/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8182/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8183/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8184/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8185/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8186/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8187/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8188/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8189/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8190/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8191/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8192/10000..  Training Loss: 0.152.. \n",
      "Epoch: 8193/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8194/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8195/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8196/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8197/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8198/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8199/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8200/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8201/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8202/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8203/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8204/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8205/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8206/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8207/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8208/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8209/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8210/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8211/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8212/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8213/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8214/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8215/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8216/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8217/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8218/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8219/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8220/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8221/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8222/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8223/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8224/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8225/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8226/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8227/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8228/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8229/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8230/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8231/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8232/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8233/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8234/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8235/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8236/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8237/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8238/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8239/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8240/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8241/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8242/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8243/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8244/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8245/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8246/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8247/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8248/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8249/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8250/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8251/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8252/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8253/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8254/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8255/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8256/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8257/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8258/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8259/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8260/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8261/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8262/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8263/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8264/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8265/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8266/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8267/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8268/10000..  Training Loss: 0.151.. \n",
      "Epoch: 8269/10000..  Training Loss: 0.151.. \n",
      "Epoch: 8270/10000..  Training Loss: 0.153.. \n",
      "Epoch: 8271/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8272/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8273/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8274/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8275/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8276/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8277/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8278/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8279/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8280/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8281/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8282/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8283/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8284/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8285/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8286/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8287/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8288/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8289/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8290/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8291/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8292/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8293/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8294/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8295/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8296/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8297/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8298/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8299/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8300/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8301/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8302/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8303/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8304/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8305/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8306/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8307/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8308/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8309/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8310/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8311/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8312/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8313/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8314/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8315/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8316/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8317/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8318/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8319/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8320/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8321/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8322/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8323/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8324/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8325/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8326/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8327/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8328/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8329/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8330/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8331/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8332/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8333/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8334/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8335/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8336/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8337/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8338/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8339/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8340/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8341/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8342/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8343/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8344/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8345/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8346/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8347/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8348/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8349/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8350/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8351/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8352/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8353/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8354/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8355/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8356/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8357/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8358/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8359/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8360/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8361/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8362/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8363/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8364/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8365/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8366/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8367/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8368/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8369/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8370/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8371/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8372/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8373/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8374/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8375/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8376/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8377/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8378/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8379/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8380/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8381/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8382/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8383/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8384/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8385/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8386/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8387/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8388/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8389/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8390/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8391/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8392/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8393/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8394/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8395/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8396/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8397/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8398/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8399/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8400/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8401/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8402/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8403/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8404/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8405/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8406/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8407/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8408/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8409/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8410/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8411/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8412/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8413/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8414/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8415/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8416/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8417/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8418/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8419/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8420/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8421/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8422/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8423/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8424/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8425/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8426/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8427/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8428/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8429/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8430/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8431/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8432/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8433/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8434/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8435/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8436/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8437/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8438/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8439/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8440/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8441/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8442/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8443/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8444/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8445/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8446/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8447/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8448/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8449/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8450/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8451/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8452/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8453/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8454/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8455/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8456/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8457/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8458/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8459/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8460/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8461/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8462/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8463/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8464/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8465/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8466/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8467/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8468/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8469/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8470/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8471/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8472/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8473/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8474/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8475/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8476/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8477/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8478/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8479/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8480/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8481/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8482/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8483/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8484/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8485/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8486/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8487/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8488/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8489/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8490/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8491/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8492/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8493/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8494/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8495/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8496/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8497/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8498/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8499/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8500/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8501/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8502/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8503/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8504/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8505/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8506/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8507/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8508/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8509/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8510/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8511/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8512/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8513/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8514/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8515/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8516/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8517/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8518/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8519/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8520/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8521/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8522/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8523/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8524/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8525/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8526/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8527/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8528/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8529/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8530/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8531/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8532/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8533/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8534/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8535/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8536/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8537/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8538/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8539/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8540/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8541/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8542/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8543/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8544/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8545/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8546/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8547/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8548/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8549/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8550/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8551/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8552/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8553/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8554/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8555/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8556/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8557/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8558/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8559/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8560/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8561/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8562/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8563/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8564/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8565/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8566/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8567/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8568/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8569/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8570/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8571/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8572/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8573/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8574/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8575/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8576/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8577/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8578/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8579/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8580/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8581/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8582/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8583/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8584/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8585/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8586/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8587/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8588/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8589/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8590/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8591/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8592/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8593/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8594/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8595/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8596/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8597/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8598/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8599/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8600/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8601/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8602/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8603/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8604/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8605/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8606/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8607/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8608/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8609/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8610/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8611/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8612/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8613/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8614/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8615/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8616/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8617/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8618/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8619/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8620/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8621/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8622/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8623/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8624/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8625/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8626/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8627/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8628/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8629/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8630/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8631/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8632/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8633/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8634/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8635/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8636/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8637/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8638/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8639/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8640/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8641/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8642/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8643/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8644/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8645/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8646/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8647/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8648/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8649/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8650/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8651/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8652/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8653/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8654/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8655/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8656/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8657/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8658/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8659/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8660/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8661/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8662/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8663/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8664/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8665/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8666/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8667/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8668/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8669/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8670/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8671/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8672/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8673/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8674/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8675/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8676/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8677/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8678/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8679/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8680/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8681/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8682/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8683/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8684/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8685/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8686/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8687/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8688/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8689/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8690/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8691/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8692/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8693/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8694/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8695/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8696/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8697/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8698/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8699/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8700/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8701/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8702/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8703/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8704/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8705/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8706/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8707/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8708/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8709/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8710/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8711/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8712/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8713/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8714/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8715/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8716/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8717/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8718/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8719/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8720/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8721/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8722/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8723/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8724/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8725/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8726/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8727/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8728/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8729/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8730/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8731/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8732/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8733/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8734/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8735/10000..  Training Loss: 0.147.. \n",
      "Epoch: 8736/10000..  Training Loss: 0.149.. \n",
      "Epoch: 8737/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8738/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8739/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8740/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8741/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8742/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8743/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8744/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8745/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8746/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8747/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8748/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8749/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8750/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8751/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8752/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8753/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8754/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8755/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8756/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8757/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8758/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8759/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8760/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8761/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8762/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8763/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8764/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8765/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8766/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8767/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8768/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8769/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8770/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8771/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8772/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8773/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8774/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8775/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8776/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8777/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8778/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8779/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8780/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8781/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8782/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8783/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8784/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8785/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8786/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8787/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8788/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8789/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8790/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8791/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8792/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8793/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8794/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8795/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8796/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8797/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8798/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8799/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8800/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8801/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8802/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8803/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8804/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8805/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8806/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8807/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8808/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8809/10000..  Training Loss: 0.148.. \n",
      "Epoch: 8810/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8811/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8812/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8813/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8814/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8815/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8816/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8817/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8818/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8819/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8820/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8821/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8822/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8823/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8824/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8825/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8826/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8827/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8828/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8829/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8830/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8831/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8832/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8833/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8834/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8835/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8836/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8837/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8838/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8839/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8840/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8841/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8842/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8843/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8844/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8845/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8846/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8847/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8848/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8849/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8850/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8851/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8852/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8853/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8854/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8855/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8856/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8857/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8858/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8859/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8860/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8861/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8862/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8863/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8864/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8865/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8866/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8867/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8868/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8869/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8870/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8871/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8872/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8873/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8874/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8875/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8876/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8877/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8878/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8879/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8880/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8881/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8882/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8883/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8884/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8885/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8886/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8887/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8888/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8889/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8890/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8891/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8892/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8893/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8894/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8895/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8896/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8897/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8898/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8899/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8900/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8901/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8902/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8903/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8904/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8905/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8906/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8907/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8908/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8909/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8910/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8911/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8912/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8913/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8914/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8915/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8916/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8917/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8918/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8919/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8920/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8921/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8922/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8923/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8924/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8925/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8926/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8927/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8928/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8929/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8930/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8931/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8932/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8933/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8934/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8935/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8936/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8937/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8938/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8939/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8940/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8941/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8942/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8943/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8944/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8945/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8946/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8947/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8948/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8949/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8950/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8951/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8952/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8953/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8954/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8955/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8956/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8957/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8958/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8959/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8960/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8961/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8962/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8963/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8964/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8965/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8966/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8967/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8968/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8969/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8970/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8971/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8972/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8973/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8974/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8975/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8976/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8977/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8978/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8979/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8980/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8981/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8982/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8983/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8984/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8985/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8986/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8987/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8988/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8989/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8990/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8991/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8992/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8993/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8994/10000..  Training Loss: 0.143.. \n",
      "Epoch: 8995/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8996/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8997/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8998/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8999/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9000/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9001/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9002/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9003/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9004/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9005/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9006/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9007/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9008/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9009/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9010/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9011/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9012/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9013/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9014/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9015/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9016/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9017/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9018/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9019/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9020/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9021/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9022/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9023/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9024/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9025/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9026/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9027/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9028/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9029/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9030/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9031/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9032/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9033/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9034/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9035/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9036/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9037/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9038/10000..  Training Loss: 0.146.. \n",
      "Epoch: 9039/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9040/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9041/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9042/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9043/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9044/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9045/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9046/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9047/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9048/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9049/10000..  Training Loss: 0.147.. \n",
      "Epoch: 9050/10000..  Training Loss: 0.146.. \n",
      "Epoch: 9051/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9052/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9053/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9054/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9055/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9056/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9057/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9058/10000..  Training Loss: 0.146.. \n",
      "Epoch: 9059/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9060/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9061/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9062/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9063/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9064/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9065/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9066/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9067/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9068/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9069/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9070/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9071/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9072/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9073/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9074/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9075/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9076/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9077/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9078/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9079/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9080/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9081/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9082/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9083/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9084/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9085/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9086/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9087/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9088/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9089/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9090/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9091/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9092/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9093/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9094/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9095/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9096/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9097/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9098/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9099/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9100/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9101/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9102/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9103/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9104/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9105/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9106/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9107/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9108/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9109/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9110/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9111/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9112/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9113/10000..  Training Loss: 0.146.. \n",
      "Epoch: 9114/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9115/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9116/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9117/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9118/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9119/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9120/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9121/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9122/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9123/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9124/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9125/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9126/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9127/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9128/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9129/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9130/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9131/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9132/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9133/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9134/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9135/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9136/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9137/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9138/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9139/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9140/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9141/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9142/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9143/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9144/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9145/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9146/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9147/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9148/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9149/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9150/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9151/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9152/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9153/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9154/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9155/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9156/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9157/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9158/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9159/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9160/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9161/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9162/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9163/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9164/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9165/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9166/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9167/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9168/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9169/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9170/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9171/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9172/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9173/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9174/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9175/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9176/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9177/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9178/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9179/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9180/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9181/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9182/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9183/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9184/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9185/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9186/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9187/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9188/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9189/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9190/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9191/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9192/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9193/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9194/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9195/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9196/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9197/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9198/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9199/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9200/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9201/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9202/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9203/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9204/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9205/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9206/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9207/10000..  Training Loss: 0.147.. \n",
      "Epoch: 9208/10000..  Training Loss: 0.146.. \n",
      "Epoch: 9209/10000..  Training Loss: 0.147.. \n",
      "Epoch: 9210/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9211/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9212/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9213/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9214/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9215/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9216/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9217/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9218/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9219/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9220/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9221/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9222/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9223/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9224/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9225/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9226/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9227/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9228/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9229/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9230/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9231/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9232/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9233/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9234/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9235/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9236/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9237/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9238/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9239/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9240/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9241/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9242/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9243/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9244/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9245/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9246/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9247/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9248/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9249/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9250/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9251/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9252/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9253/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9254/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9255/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9256/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9257/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9258/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9259/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9260/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9261/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9262/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9263/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9264/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9265/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9266/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9267/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9268/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9269/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9270/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9271/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9272/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9273/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9274/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9275/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9276/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9277/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9278/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9279/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9280/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9281/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9282/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9283/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9284/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9285/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9286/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9287/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9288/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9289/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9290/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9291/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9292/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9293/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9294/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9295/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9296/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9297/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9298/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9299/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9300/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9301/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9302/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9303/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9304/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9305/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9306/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9307/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9308/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9309/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9310/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9311/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9312/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9313/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9314/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9315/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9316/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9317/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9318/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9319/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9320/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9321/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9322/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9323/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9324/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9325/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9326/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9327/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9328/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9329/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9330/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9331/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9332/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9333/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9334/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9335/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9336/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9337/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9338/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9339/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9340/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9341/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9342/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9343/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9344/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9345/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9346/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9347/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9348/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9349/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9350/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9351/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9352/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9353/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9354/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9355/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9356/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9357/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9358/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9359/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9360/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9361/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9362/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9363/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9364/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9365/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9366/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9367/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9368/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9369/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9370/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9371/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9372/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9373/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9374/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9375/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9376/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9377/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9378/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9379/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9380/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9381/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9382/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9383/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9384/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9385/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9386/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9387/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9388/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9389/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9390/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9391/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9392/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9393/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9394/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9395/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9396/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9397/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9398/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9399/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9400/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9401/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9402/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9403/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9404/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9405/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9406/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9407/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9408/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9409/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9410/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9411/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9412/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9413/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9414/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9415/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9416/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9417/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9418/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9419/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9420/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9421/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9422/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9423/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9424/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9425/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9426/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9427/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9428/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9429/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9430/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9431/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9432/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9433/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9434/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9435/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9436/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9437/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9438/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9439/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9440/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9441/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9442/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9443/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9444/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9445/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9446/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9447/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9448/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9449/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9450/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9451/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9452/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9453/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9454/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9455/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9456/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9457/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9458/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9459/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9460/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9461/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9462/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9463/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9464/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9465/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9466/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9467/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9468/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9469/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9470/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9471/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9472/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9473/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9474/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9475/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9476/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9477/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9478/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9479/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9480/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9481/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9482/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9483/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9484/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9485/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9486/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9487/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9488/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9489/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9490/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9491/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9492/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9493/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9494/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9495/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9496/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9497/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9498/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9499/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9500/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9501/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9502/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9503/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9504/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9505/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9506/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9507/10000..  Training Loss: 0.146.. \n",
      "Epoch: 9508/10000..  Training Loss: 0.146.. \n",
      "Epoch: 9509/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9510/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9511/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9512/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9513/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9514/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9515/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9516/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9517/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9518/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9519/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9520/10000..  Training Loss: 0.149.. \n",
      "Epoch: 9521/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9522/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9523/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9524/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9525/10000..  Training Loss: 0.154.. \n",
      "Epoch: 9526/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9527/10000..  Training Loss: 0.149.. \n",
      "Epoch: 9528/10000..  Training Loss: 0.147.. \n",
      "Epoch: 9529/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9530/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9531/10000..  Training Loss: 0.150.. \n",
      "Epoch: 9532/10000..  Training Loss: 0.149.. \n",
      "Epoch: 9533/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9534/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9535/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9536/10000..  Training Loss: 0.151.. \n",
      "Epoch: 9537/10000..  Training Loss: 0.153.. \n",
      "Epoch: 9538/10000..  Training Loss: 0.175.. \n",
      "Epoch: 9539/10000..  Training Loss: 1.219.. \n",
      "Epoch: 9540/10000..  Training Loss: 1.305.. \n",
      "Epoch: 9541/10000..  Training Loss: 2.019.. \n",
      "Epoch: 9542/10000..  Training Loss: 0.774.. \n",
      "Epoch: 9543/10000..  Training Loss: 0.874.. \n",
      "Epoch: 9544/10000..  Training Loss: 1.553.. \n",
      "Epoch: 9545/10000..  Training Loss: 1.296.. \n",
      "Epoch: 9546/10000..  Training Loss: 0.627.. \n",
      "Epoch: 9547/10000..  Training Loss: 0.631.. \n",
      "Epoch: 9548/10000..  Training Loss: 0.960.. \n",
      "Epoch: 9549/10000..  Training Loss: 0.810.. \n",
      "Epoch: 9550/10000..  Training Loss: 0.562.. \n",
      "Epoch: 9551/10000..  Training Loss: 0.677.. \n",
      "Epoch: 9552/10000..  Training Loss: 0.507.. \n",
      "Epoch: 9553/10000..  Training Loss: 0.555.. \n",
      "Epoch: 9554/10000..  Training Loss: 0.602.. \n",
      "Epoch: 9555/10000..  Training Loss: 0.604.. \n",
      "Epoch: 9556/10000..  Training Loss: 0.545.. \n",
      "Epoch: 9557/10000..  Training Loss: 0.483.. \n",
      "Epoch: 9558/10000..  Training Loss: 0.432.. \n",
      "Epoch: 9559/10000..  Training Loss: 0.417.. \n",
      "Epoch: 9560/10000..  Training Loss: 0.332.. \n",
      "Epoch: 9561/10000..  Training Loss: 0.301.. \n",
      "Epoch: 9562/10000..  Training Loss: 0.278.. \n",
      "Epoch: 9563/10000..  Training Loss: 0.398.. \n",
      "Epoch: 9564/10000..  Training Loss: 0.271.. \n",
      "Epoch: 9565/10000..  Training Loss: 0.294.. \n",
      "Epoch: 9566/10000..  Training Loss: 0.303.. \n",
      "Epoch: 9567/10000..  Training Loss: 0.287.. \n",
      "Epoch: 9568/10000..  Training Loss: 0.297.. \n",
      "Epoch: 9569/10000..  Training Loss: 0.392.. \n",
      "Epoch: 9570/10000..  Training Loss: 0.483.. \n",
      "Epoch: 9571/10000..  Training Loss: 1.106.. \n",
      "Epoch: 9572/10000..  Training Loss: 1.240.. \n",
      "Epoch: 9573/10000..  Training Loss: 0.908.. \n",
      "Epoch: 9574/10000..  Training Loss: 0.596.. \n",
      "Epoch: 9575/10000..  Training Loss: 0.386.. \n",
      "Epoch: 9576/10000..  Training Loss: 0.385.. \n",
      "Epoch: 9577/10000..  Training Loss: 0.444.. \n",
      "Epoch: 9578/10000..  Training Loss: 0.962.. \n",
      "Epoch: 9579/10000..  Training Loss: 0.381.. \n",
      "Epoch: 9580/10000..  Training Loss: 0.576.. \n",
      "Epoch: 9581/10000..  Training Loss: 0.720.. \n",
      "Epoch: 9582/10000..  Training Loss: 1.012.. \n",
      "Epoch: 9583/10000..  Training Loss: 0.947.. \n",
      "Epoch: 9584/10000..  Training Loss: 0.565.. \n",
      "Epoch: 9585/10000..  Training Loss: 0.427.. \n",
      "Epoch: 9586/10000..  Training Loss: 0.422.. \n",
      "Epoch: 9587/10000..  Training Loss: 0.439.. \n",
      "Epoch: 9588/10000..  Training Loss: 0.377.. \n",
      "Epoch: 9589/10000..  Training Loss: 0.338.. \n",
      "Epoch: 9590/10000..  Training Loss: 0.376.. \n",
      "Epoch: 9591/10000..  Training Loss: 0.357.. \n",
      "Epoch: 9592/10000..  Training Loss: 0.357.. \n",
      "Epoch: 9593/10000..  Training Loss: 0.332.. \n",
      "Epoch: 9594/10000..  Training Loss: 0.301.. \n",
      "Epoch: 9595/10000..  Training Loss: 0.270.. \n",
      "Epoch: 9596/10000..  Training Loss: 0.236.. \n",
      "Epoch: 9597/10000..  Training Loss: 0.228.. \n",
      "Epoch: 9598/10000..  Training Loss: 0.251.. \n",
      "Epoch: 9599/10000..  Training Loss: 0.248.. \n",
      "Epoch: 9600/10000..  Training Loss: 0.230.. \n",
      "Epoch: 9601/10000..  Training Loss: 0.220.. \n",
      "Epoch: 9602/10000..  Training Loss: 0.207.. \n",
      "Epoch: 9603/10000..  Training Loss: 0.196.. \n",
      "Epoch: 9604/10000..  Training Loss: 0.207.. \n",
      "Epoch: 9605/10000..  Training Loss: 0.212.. \n",
      "Epoch: 9606/10000..  Training Loss: 0.198.. \n",
      "Epoch: 9607/10000..  Training Loss: 0.190.. \n",
      "Epoch: 9608/10000..  Training Loss: 0.185.. \n",
      "Epoch: 9609/10000..  Training Loss: 0.189.. \n",
      "Epoch: 9610/10000..  Training Loss: 0.188.. \n",
      "Epoch: 9611/10000..  Training Loss: 0.181.. \n",
      "Epoch: 9612/10000..  Training Loss: 0.180.. \n",
      "Epoch: 9613/10000..  Training Loss: 0.172.. \n",
      "Epoch: 9614/10000..  Training Loss: 0.179.. \n",
      "Epoch: 9615/10000..  Training Loss: 0.180.. \n",
      "Epoch: 9616/10000..  Training Loss: 0.175.. \n",
      "Epoch: 9617/10000..  Training Loss: 0.174.. \n",
      "Epoch: 9618/10000..  Training Loss: 0.169.. \n",
      "Epoch: 9619/10000..  Training Loss: 0.170.. \n",
      "Epoch: 9620/10000..  Training Loss: 0.171.. \n",
      "Epoch: 9621/10000..  Training Loss: 0.169.. \n",
      "Epoch: 9622/10000..  Training Loss: 0.169.. \n",
      "Epoch: 9623/10000..  Training Loss: 0.166.. \n",
      "Epoch: 9624/10000..  Training Loss: 0.166.. \n",
      "Epoch: 9625/10000..  Training Loss: 0.167.. \n",
      "Epoch: 9626/10000..  Training Loss: 0.165.. \n",
      "Epoch: 9627/10000..  Training Loss: 0.165.. \n",
      "Epoch: 9628/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9629/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9630/10000..  Training Loss: 0.164.. \n",
      "Epoch: 9631/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9632/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9633/10000..  Training Loss: 0.162.. \n",
      "Epoch: 9634/10000..  Training Loss: 0.162.. \n",
      "Epoch: 9635/10000..  Training Loss: 0.163.. \n",
      "Epoch: 9636/10000..  Training Loss: 0.162.. \n",
      "Epoch: 9637/10000..  Training Loss: 0.162.. \n",
      "Epoch: 9638/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9639/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9640/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9641/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9642/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9643/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9644/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9645/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9646/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9647/10000..  Training Loss: 0.161.. \n",
      "Epoch: 9648/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9649/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9650/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9651/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9652/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9653/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9654/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9655/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9656/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9657/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9658/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9659/10000..  Training Loss: 0.160.. \n",
      "Epoch: 9660/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9661/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9662/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9663/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9664/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9665/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9666/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9667/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9668/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9669/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9670/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9671/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9672/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9673/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9674/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9675/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9676/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9677/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9678/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9679/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9680/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9681/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9682/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9683/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9684/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9685/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9686/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9687/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9688/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9689/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9690/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9691/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9692/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9693/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9694/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9695/10000..  Training Loss: 0.159.. \n",
      "Epoch: 9696/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9697/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9698/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9699/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9700/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9701/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9702/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9703/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9704/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9705/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9706/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9707/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9708/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9709/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9710/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9711/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9712/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9713/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9714/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9715/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9716/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9717/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9718/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9719/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9720/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9721/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9722/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9723/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9724/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9725/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9726/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9727/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9728/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9729/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9730/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9731/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9732/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9733/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9734/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9735/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9736/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9737/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9738/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9739/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9740/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9741/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9742/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9743/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9744/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9745/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9746/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9747/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9748/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9749/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9750/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9751/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9752/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9753/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9754/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9755/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9756/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9757/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9758/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9759/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9760/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9761/10000..  Training Loss: 0.158.. \n",
      "Epoch: 9762/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9763/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9764/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9765/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9766/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9767/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9768/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9769/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9770/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9771/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9772/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9773/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9774/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9775/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9776/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9777/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9778/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9779/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9780/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9781/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9782/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9783/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9784/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9785/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9786/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9787/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9788/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9789/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9790/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9791/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9792/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9793/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9794/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9795/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9796/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9797/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9798/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9799/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9800/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9801/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9802/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9803/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9804/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9805/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9806/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9807/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9808/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9809/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9810/10000..  Training Loss: 0.157.. \n",
      "Epoch: 9811/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9812/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9813/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9814/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9815/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9816/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9817/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9818/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9819/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9820/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9821/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9822/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9823/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9824/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9825/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9826/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9827/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9828/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9829/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9830/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9831/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9832/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9833/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9834/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9835/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9836/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9837/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9838/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9839/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9840/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9841/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9842/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9843/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9844/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9845/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9846/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9847/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9848/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9849/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9850/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9851/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9852/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9853/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9854/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9855/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9856/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9857/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9858/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9859/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9860/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9861/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9862/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9863/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9864/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9865/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9866/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9867/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9868/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9869/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9870/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9871/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9872/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9873/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9874/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9875/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9876/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9877/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9878/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9879/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9880/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9881/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9882/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9883/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9884/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9885/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9886/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9887/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9888/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9889/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9890/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9891/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9892/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9893/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9894/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9895/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9896/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9897/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9898/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9899/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9900/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9901/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9902/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9903/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9904/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9905/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9906/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9907/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9908/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9909/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9910/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9911/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9912/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9913/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9914/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9915/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9916/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9917/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9918/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9919/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9920/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9921/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9922/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9923/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9924/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9925/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9926/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9927/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9928/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9929/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9930/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9931/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9932/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9933/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9934/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9935/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9936/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9937/10000..  Training Loss: 0.156.. \n",
      "Epoch: 9938/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9939/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9940/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9941/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9942/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9943/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9944/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9945/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9946/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9947/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9948/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9949/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9950/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9951/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9952/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9953/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9954/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9955/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9956/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9957/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9958/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9959/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9960/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9961/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9962/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9963/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9964/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9965/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9966/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9967/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9968/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9969/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9970/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9971/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9972/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9973/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9974/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9975/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9976/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9977/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9978/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9979/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9980/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9981/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9982/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9983/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9984/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9985/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9986/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9987/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9988/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9989/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9990/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9991/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9992/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9993/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9994/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9995/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9996/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9997/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9998/10000..  Training Loss: 0.155.. \n",
      "Epoch: 9999/10000..  Training Loss: 0.155.. \n",
      "Epoch: 10000/10000..  Training Loss: 0.155.. \n",
      "Epoch: 1/10000..  Training Loss: 0.713.. \n",
      "Epoch: 2/10000..  Training Loss: 0.512.. \n",
      "Epoch: 3/10000..  Training Loss: 0.472.. \n",
      "Epoch: 4/10000..  Training Loss: 0.465.. \n",
      "Epoch: 5/10000..  Training Loss: 0.456.. \n",
      "Epoch: 6/10000..  Training Loss: 0.434.. \n",
      "Epoch: 7/10000..  Training Loss: 0.416.. \n",
      "Epoch: 8/10000..  Training Loss: 0.411.. \n",
      "Epoch: 9/10000..  Training Loss: 0.413.. \n",
      "Epoch: 10/10000..  Training Loss: 0.413.. \n",
      "Epoch: 11/10000..  Training Loss: 0.408.. \n",
      "Epoch: 12/10000..  Training Loss: 0.402.. \n",
      "Epoch: 13/10000..  Training Loss: 0.398.. \n",
      "Epoch: 14/10000..  Training Loss: 0.395.. \n",
      "Epoch: 15/10000..  Training Loss: 0.393.. \n",
      "Epoch: 16/10000..  Training Loss: 0.391.. \n",
      "Epoch: 17/10000..  Training Loss: 0.390.. \n",
      "Epoch: 18/10000..  Training Loss: 0.388.. \n",
      "Epoch: 19/10000..  Training Loss: 0.385.. \n",
      "Epoch: 20/10000..  Training Loss: 0.382.. \n",
      "Epoch: 21/10000..  Training Loss: 0.379.. \n",
      "Epoch: 22/10000..  Training Loss: 0.378.. \n",
      "Epoch: 23/10000..  Training Loss: 0.377.. \n",
      "Epoch: 24/10000..  Training Loss: 0.376.. \n",
      "Epoch: 25/10000..  Training Loss: 0.374.. \n",
      "Epoch: 26/10000..  Training Loss: 0.373.. \n",
      "Epoch: 27/10000..  Training Loss: 0.371.. \n",
      "Epoch: 28/10000..  Training Loss: 0.369.. \n",
      "Epoch: 29/10000..  Training Loss: 0.368.. \n",
      "Epoch: 30/10000..  Training Loss: 0.367.. \n",
      "Epoch: 31/10000..  Training Loss: 0.366.. \n",
      "Epoch: 32/10000..  Training Loss: 0.364.. \n",
      "Epoch: 33/10000..  Training Loss: 0.363.. \n",
      "Epoch: 34/10000..  Training Loss: 0.362.. \n",
      "Epoch: 35/10000..  Training Loss: 0.361.. \n",
      "Epoch: 36/10000..  Training Loss: 0.359.. \n",
      "Epoch: 37/10000..  Training Loss: 0.358.. \n",
      "Epoch: 38/10000..  Training Loss: 0.357.. \n",
      "Epoch: 39/10000..  Training Loss: 0.355.. \n",
      "Epoch: 40/10000..  Training Loss: 0.354.. \n",
      "Epoch: 41/10000..  Training Loss: 0.353.. \n",
      "Epoch: 42/10000..  Training Loss: 0.351.. \n",
      "Epoch: 43/10000..  Training Loss: 0.350.. \n",
      "Epoch: 44/10000..  Training Loss: 0.349.. \n",
      "Epoch: 45/10000..  Training Loss: 0.348.. \n",
      "Epoch: 46/10000..  Training Loss: 0.346.. \n",
      "Epoch: 47/10000..  Training Loss: 0.345.. \n",
      "Epoch: 48/10000..  Training Loss: 0.343.. \n",
      "Epoch: 49/10000..  Training Loss: 0.342.. \n",
      "Epoch: 50/10000..  Training Loss: 0.341.. \n",
      "Epoch: 51/10000..  Training Loss: 0.339.. \n",
      "Epoch: 52/10000..  Training Loss: 0.338.. \n",
      "Epoch: 53/10000..  Training Loss: 0.337.. \n",
      "Epoch: 54/10000..  Training Loss: 0.335.. \n",
      "Epoch: 55/10000..  Training Loss: 0.334.. \n",
      "Epoch: 56/10000..  Training Loss: 0.332.. \n",
      "Epoch: 57/10000..  Training Loss: 0.331.. \n",
      "Epoch: 58/10000..  Training Loss: 0.330.. \n",
      "Epoch: 59/10000..  Training Loss: 0.328.. \n",
      "Epoch: 60/10000..  Training Loss: 0.327.. \n",
      "Epoch: 61/10000..  Training Loss: 0.325.. \n",
      "Epoch: 62/10000..  Training Loss: 0.324.. \n",
      "Epoch: 63/10000..  Training Loss: 0.323.. \n",
      "Epoch: 64/10000..  Training Loss: 0.321.. \n",
      "Epoch: 65/10000..  Training Loss: 0.320.. \n",
      "Epoch: 66/10000..  Training Loss: 0.319.. \n",
      "Epoch: 67/10000..  Training Loss: 0.318.. \n",
      "Epoch: 68/10000..  Training Loss: 0.316.. \n",
      "Epoch: 69/10000..  Training Loss: 0.316.. \n",
      "Epoch: 70/10000..  Training Loss: 0.316.. \n",
      "Epoch: 71/10000..  Training Loss: 0.316.. \n",
      "Epoch: 72/10000..  Training Loss: 0.313.. \n",
      "Epoch: 73/10000..  Training Loss: 0.311.. \n",
      "Epoch: 74/10000..  Training Loss: 0.312.. \n",
      "Epoch: 75/10000..  Training Loss: 0.311.. \n",
      "Epoch: 76/10000..  Training Loss: 0.308.. \n",
      "Epoch: 77/10000..  Training Loss: 0.308.. \n",
      "Epoch: 78/10000..  Training Loss: 0.308.. \n",
      "Epoch: 79/10000..  Training Loss: 0.306.. \n",
      "Epoch: 80/10000..  Training Loss: 0.304.. \n",
      "Epoch: 81/10000..  Training Loss: 0.305.. \n",
      "Epoch: 82/10000..  Training Loss: 0.304.. \n",
      "Epoch: 83/10000..  Training Loss: 0.302.. \n",
      "Epoch: 84/10000..  Training Loss: 0.301.. \n",
      "Epoch: 85/10000..  Training Loss: 0.301.. \n",
      "Epoch: 86/10000..  Training Loss: 0.301.. \n",
      "Epoch: 87/10000..  Training Loss: 0.299.. \n",
      "Epoch: 88/10000..  Training Loss: 0.298.. \n",
      "Epoch: 89/10000..  Training Loss: 0.298.. \n",
      "Epoch: 90/10000..  Training Loss: 0.297.. \n",
      "Epoch: 91/10000..  Training Loss: 0.296.. \n",
      "Epoch: 92/10000..  Training Loss: 0.295.. \n",
      "Epoch: 93/10000..  Training Loss: 0.295.. \n",
      "Epoch: 94/10000..  Training Loss: 0.294.. \n",
      "Epoch: 95/10000..  Training Loss: 0.294.. \n",
      "Epoch: 96/10000..  Training Loss: 0.294.. \n",
      "Epoch: 97/10000..  Training Loss: 0.293.. \n",
      "Epoch: 98/10000..  Training Loss: 0.292.. \n",
      "Epoch: 99/10000..  Training Loss: 0.291.. \n",
      "Epoch: 100/10000..  Training Loss: 0.290.. \n",
      "Epoch: 101/10000..  Training Loss: 0.289.. \n",
      "Epoch: 102/10000..  Training Loss: 0.289.. \n",
      "Epoch: 103/10000..  Training Loss: 0.288.. \n",
      "Epoch: 104/10000..  Training Loss: 0.288.. \n",
      "Epoch: 105/10000..  Training Loss: 0.288.. \n",
      "Epoch: 106/10000..  Training Loss: 0.288.. \n",
      "Epoch: 107/10000..  Training Loss: 0.290.. \n",
      "Epoch: 108/10000..  Training Loss: 0.291.. \n",
      "Epoch: 109/10000..  Training Loss: 0.291.. \n",
      "Epoch: 110/10000..  Training Loss: 0.286.. \n",
      "Epoch: 111/10000..  Training Loss: 0.284.. \n",
      "Epoch: 112/10000..  Training Loss: 0.286.. \n",
      "Epoch: 113/10000..  Training Loss: 0.287.. \n",
      "Epoch: 114/10000..  Training Loss: 0.285.. \n",
      "Epoch: 115/10000..  Training Loss: 0.282.. \n",
      "Epoch: 116/10000..  Training Loss: 0.283.. \n",
      "Epoch: 117/10000..  Training Loss: 0.284.. \n",
      "Epoch: 118/10000..  Training Loss: 0.283.. \n",
      "Epoch: 119/10000..  Training Loss: 0.281.. \n",
      "Epoch: 120/10000..  Training Loss: 0.280.. \n",
      "Epoch: 121/10000..  Training Loss: 0.281.. \n",
      "Epoch: 122/10000..  Training Loss: 0.281.. \n",
      "Epoch: 123/10000..  Training Loss: 0.279.. \n",
      "Epoch: 124/10000..  Training Loss: 0.278.. \n",
      "Epoch: 125/10000..  Training Loss: 0.278.. \n",
      "Epoch: 126/10000..  Training Loss: 0.279.. \n",
      "Epoch: 127/10000..  Training Loss: 0.278.. \n",
      "Epoch: 128/10000..  Training Loss: 0.277.. \n",
      "Epoch: 129/10000..  Training Loss: 0.276.. \n",
      "Epoch: 130/10000..  Training Loss: 0.276.. \n",
      "Epoch: 131/10000..  Training Loss: 0.276.. \n",
      "Epoch: 132/10000..  Training Loss: 0.276.. \n",
      "Epoch: 133/10000..  Training Loss: 0.275.. \n",
      "Epoch: 134/10000..  Training Loss: 0.275.. \n",
      "Epoch: 135/10000..  Training Loss: 0.274.. \n",
      "Epoch: 136/10000..  Training Loss: 0.273.. \n",
      "Epoch: 137/10000..  Training Loss: 0.273.. \n",
      "Epoch: 138/10000..  Training Loss: 0.273.. \n",
      "Epoch: 139/10000..  Training Loss: 0.273.. \n",
      "Epoch: 140/10000..  Training Loss: 0.273.. \n",
      "Epoch: 141/10000..  Training Loss: 0.273.. \n",
      "Epoch: 142/10000..  Training Loss: 0.273.. \n",
      "Epoch: 143/10000..  Training Loss: 0.273.. \n",
      "Epoch: 144/10000..  Training Loss: 0.273.. \n",
      "Epoch: 145/10000..  Training Loss: 0.272.. \n",
      "Epoch: 146/10000..  Training Loss: 0.271.. \n",
      "Epoch: 147/10000..  Training Loss: 0.270.. \n",
      "Epoch: 148/10000..  Training Loss: 0.269.. \n",
      "Epoch: 149/10000..  Training Loss: 0.269.. \n",
      "Epoch: 150/10000..  Training Loss: 0.269.. \n",
      "Epoch: 151/10000..  Training Loss: 0.269.. \n",
      "Epoch: 152/10000..  Training Loss: 0.270.. \n",
      "Epoch: 153/10000..  Training Loss: 0.271.. \n",
      "Epoch: 154/10000..  Training Loss: 0.271.. \n",
      "Epoch: 155/10000..  Training Loss: 0.271.. \n",
      "Epoch: 156/10000..  Training Loss: 0.269.. \n",
      "Epoch: 157/10000..  Training Loss: 0.267.. \n",
      "Epoch: 158/10000..  Training Loss: 0.266.. \n",
      "Epoch: 159/10000..  Training Loss: 0.266.. \n",
      "Epoch: 160/10000..  Training Loss: 0.267.. \n",
      "Epoch: 161/10000..  Training Loss: 0.268.. \n",
      "Epoch: 162/10000..  Training Loss: 0.268.. \n",
      "Epoch: 163/10000..  Training Loss: 0.266.. \n",
      "Epoch: 164/10000..  Training Loss: 0.265.. \n",
      "Epoch: 165/10000..  Training Loss: 0.264.. \n",
      "Epoch: 166/10000..  Training Loss: 0.264.. \n",
      "Epoch: 167/10000..  Training Loss: 0.265.. \n",
      "Epoch: 168/10000..  Training Loss: 0.266.. \n",
      "Epoch: 169/10000..  Training Loss: 0.265.. \n",
      "Epoch: 170/10000..  Training Loss: 0.264.. \n",
      "Epoch: 171/10000..  Training Loss: 0.263.. \n",
      "Epoch: 172/10000..  Training Loss: 0.262.. \n",
      "Epoch: 173/10000..  Training Loss: 0.262.. \n",
      "Epoch: 174/10000..  Training Loss: 0.262.. \n",
      "Epoch: 175/10000..  Training Loss: 0.263.. \n",
      "Epoch: 176/10000..  Training Loss: 0.263.. \n",
      "Epoch: 177/10000..  Training Loss: 0.263.. \n",
      "Epoch: 178/10000..  Training Loss: 0.262.. \n",
      "Epoch: 179/10000..  Training Loss: 0.261.. \n",
      "Epoch: 180/10000..  Training Loss: 0.260.. \n",
      "Epoch: 181/10000..  Training Loss: 0.259.. \n",
      "Epoch: 182/10000..  Training Loss: 0.259.. \n",
      "Epoch: 183/10000..  Training Loss: 0.260.. \n",
      "Epoch: 184/10000..  Training Loss: 0.260.. \n",
      "Epoch: 185/10000..  Training Loss: 0.261.. \n",
      "Epoch: 186/10000..  Training Loss: 0.262.. \n",
      "Epoch: 187/10000..  Training Loss: 0.261.. \n",
      "Epoch: 188/10000..  Training Loss: 0.261.. \n",
      "Epoch: 189/10000..  Training Loss: 0.259.. \n",
      "Epoch: 190/10000..  Training Loss: 0.257.. \n",
      "Epoch: 191/10000..  Training Loss: 0.257.. \n",
      "Epoch: 192/10000..  Training Loss: 0.257.. \n",
      "Epoch: 193/10000..  Training Loss: 0.258.. \n",
      "Epoch: 194/10000..  Training Loss: 0.259.. \n",
      "Epoch: 195/10000..  Training Loss: 0.260.. \n",
      "Epoch: 196/10000..  Training Loss: 0.260.. \n",
      "Epoch: 197/10000..  Training Loss: 0.259.. \n",
      "Epoch: 198/10000..  Training Loss: 0.258.. \n",
      "Epoch: 199/10000..  Training Loss: 0.256.. \n",
      "Epoch: 200/10000..  Training Loss: 0.255.. \n",
      "Epoch: 201/10000..  Training Loss: 0.255.. \n",
      "Epoch: 202/10000..  Training Loss: 0.255.. \n",
      "Epoch: 203/10000..  Training Loss: 0.256.. \n",
      "Epoch: 204/10000..  Training Loss: 0.257.. \n",
      "Epoch: 205/10000..  Training Loss: 0.257.. \n",
      "Epoch: 206/10000..  Training Loss: 0.256.. \n",
      "Epoch: 207/10000..  Training Loss: 0.255.. \n",
      "Epoch: 208/10000..  Training Loss: 0.254.. \n",
      "Epoch: 209/10000..  Training Loss: 0.253.. \n",
      "Epoch: 210/10000..  Training Loss: 0.253.. \n",
      "Epoch: 211/10000..  Training Loss: 0.253.. \n",
      "Epoch: 212/10000..  Training Loss: 0.254.. \n",
      "Epoch: 213/10000..  Training Loss: 0.255.. \n",
      "Epoch: 214/10000..  Training Loss: 0.255.. \n",
      "Epoch: 215/10000..  Training Loss: 0.255.. \n",
      "Epoch: 216/10000..  Training Loss: 0.254.. \n",
      "Epoch: 217/10000..  Training Loss: 0.253.. \n",
      "Epoch: 218/10000..  Training Loss: 0.251.. \n",
      "Epoch: 219/10000..  Training Loss: 0.251.. \n",
      "Epoch: 220/10000..  Training Loss: 0.251.. \n",
      "Epoch: 221/10000..  Training Loss: 0.251.. \n",
      "Epoch: 222/10000..  Training Loss: 0.252.. \n",
      "Epoch: 223/10000..  Training Loss: 0.253.. \n",
      "Epoch: 224/10000..  Training Loss: 0.254.. \n",
      "Epoch: 225/10000..  Training Loss: 0.254.. \n",
      "Epoch: 226/10000..  Training Loss: 0.253.. \n",
      "Epoch: 227/10000..  Training Loss: 0.251.. \n",
      "Epoch: 228/10000..  Training Loss: 0.250.. \n",
      "Epoch: 229/10000..  Training Loss: 0.249.. \n",
      "Epoch: 230/10000..  Training Loss: 0.249.. \n",
      "Epoch: 231/10000..  Training Loss: 0.250.. \n",
      "Epoch: 232/10000..  Training Loss: 0.251.. \n",
      "Epoch: 233/10000..  Training Loss: 0.252.. \n",
      "Epoch: 234/10000..  Training Loss: 0.252.. \n",
      "Epoch: 235/10000..  Training Loss: 0.251.. \n",
      "Epoch: 236/10000..  Training Loss: 0.250.. \n",
      "Epoch: 237/10000..  Training Loss: 0.248.. \n",
      "Epoch: 238/10000..  Training Loss: 0.247.. \n",
      "Epoch: 239/10000..  Training Loss: 0.247.. \n",
      "Epoch: 240/10000..  Training Loss: 0.247.. \n",
      "Epoch: 241/10000..  Training Loss: 0.248.. \n",
      "Epoch: 242/10000..  Training Loss: 0.249.. \n",
      "Epoch: 243/10000..  Training Loss: 0.249.. \n",
      "Epoch: 244/10000..  Training Loss: 0.249.. \n",
      "Epoch: 245/10000..  Training Loss: 0.249.. \n",
      "Epoch: 246/10000..  Training Loss: 0.249.. \n",
      "Epoch: 247/10000..  Training Loss: 0.248.. \n",
      "Epoch: 248/10000..  Training Loss: 0.247.. \n",
      "Epoch: 249/10000..  Training Loss: 0.246.. \n",
      "Epoch: 250/10000..  Training Loss: 0.245.. \n",
      "Epoch: 251/10000..  Training Loss: 0.245.. \n",
      "Epoch: 252/10000..  Training Loss: 0.245.. \n",
      "Epoch: 253/10000..  Training Loss: 0.245.. \n",
      "Epoch: 254/10000..  Training Loss: 0.246.. \n",
      "Epoch: 255/10000..  Training Loss: 0.247.. \n",
      "Epoch: 256/10000..  Training Loss: 0.248.. \n",
      "Epoch: 257/10000..  Training Loss: 0.249.. \n",
      "Epoch: 258/10000..  Training Loss: 0.250.. \n",
      "Epoch: 259/10000..  Training Loss: 0.249.. \n",
      "Epoch: 260/10000..  Training Loss: 0.246.. \n",
      "Epoch: 261/10000..  Training Loss: 0.244.. \n",
      "Epoch: 262/10000..  Training Loss: 0.243.. \n",
      "Epoch: 263/10000..  Training Loss: 0.244.. \n",
      "Epoch: 264/10000..  Training Loss: 0.246.. \n",
      "Epoch: 265/10000..  Training Loss: 0.247.. \n",
      "Epoch: 266/10000..  Training Loss: 0.248.. \n",
      "Epoch: 267/10000..  Training Loss: 0.246.. \n",
      "Epoch: 268/10000..  Training Loss: 0.244.. \n",
      "Epoch: 269/10000..  Training Loss: 0.242.. \n",
      "Epoch: 270/10000..  Training Loss: 0.242.. \n",
      "Epoch: 271/10000..  Training Loss: 0.243.. \n",
      "Epoch: 272/10000..  Training Loss: 0.244.. \n",
      "Epoch: 273/10000..  Training Loss: 0.245.. \n",
      "Epoch: 274/10000..  Training Loss: 0.244.. \n",
      "Epoch: 275/10000..  Training Loss: 0.243.. \n",
      "Epoch: 276/10000..  Training Loss: 0.242.. \n",
      "Epoch: 277/10000..  Training Loss: 0.241.. \n",
      "Epoch: 278/10000..  Training Loss: 0.241.. \n",
      "Epoch: 279/10000..  Training Loss: 0.241.. \n",
      "Epoch: 280/10000..  Training Loss: 0.242.. \n",
      "Epoch: 281/10000..  Training Loss: 0.242.. \n",
      "Epoch: 282/10000..  Training Loss: 0.243.. \n",
      "Epoch: 283/10000..  Training Loss: 0.243.. \n",
      "Epoch: 284/10000..  Training Loss: 0.242.. \n",
      "Epoch: 285/10000..  Training Loss: 0.241.. \n",
      "Epoch: 286/10000..  Training Loss: 0.240.. \n",
      "Epoch: 287/10000..  Training Loss: 0.239.. \n",
      "Epoch: 288/10000..  Training Loss: 0.239.. \n",
      "Epoch: 289/10000..  Training Loss: 0.239.. \n",
      "Epoch: 290/10000..  Training Loss: 0.240.. \n",
      "Epoch: 291/10000..  Training Loss: 0.240.. \n",
      "Epoch: 292/10000..  Training Loss: 0.241.. \n",
      "Epoch: 293/10000..  Training Loss: 0.242.. \n",
      "Epoch: 294/10000..  Training Loss: 0.242.. \n",
      "Epoch: 295/10000..  Training Loss: 0.242.. \n",
      "Epoch: 296/10000..  Training Loss: 0.240.. \n",
      "Epoch: 297/10000..  Training Loss: 0.239.. \n",
      "Epoch: 298/10000..  Training Loss: 0.238.. \n",
      "Epoch: 299/10000..  Training Loss: 0.237.. \n",
      "Epoch: 300/10000..  Training Loss: 0.237.. \n",
      "Epoch: 301/10000..  Training Loss: 0.238.. \n",
      "Epoch: 302/10000..  Training Loss: 0.238.. \n",
      "Epoch: 303/10000..  Training Loss: 0.240.. \n",
      "Epoch: 304/10000..  Training Loss: 0.241.. \n",
      "Epoch: 305/10000..  Training Loss: 0.242.. \n",
      "Epoch: 306/10000..  Training Loss: 0.243.. \n",
      "Epoch: 307/10000..  Training Loss: 0.241.. \n",
      "Epoch: 308/10000..  Training Loss: 0.238.. \n",
      "Epoch: 309/10000..  Training Loss: 0.236.. \n",
      "Epoch: 310/10000..  Training Loss: 0.236.. \n",
      "Epoch: 311/10000..  Training Loss: 0.237.. \n",
      "Epoch: 312/10000..  Training Loss: 0.238.. \n",
      "Epoch: 313/10000..  Training Loss: 0.240.. \n",
      "Epoch: 314/10000..  Training Loss: 0.240.. \n",
      "Epoch: 315/10000..  Training Loss: 0.239.. \n",
      "Epoch: 316/10000..  Training Loss: 0.237.. \n",
      "Epoch: 317/10000..  Training Loss: 0.235.. \n",
      "Epoch: 318/10000..  Training Loss: 0.235.. \n",
      "Epoch: 319/10000..  Training Loss: 0.235.. \n",
      "Epoch: 320/10000..  Training Loss: 0.236.. \n",
      "Epoch: 321/10000..  Training Loss: 0.237.. \n",
      "Epoch: 322/10000..  Training Loss: 0.238.. \n",
      "Epoch: 323/10000..  Training Loss: 0.237.. \n",
      "Epoch: 324/10000..  Training Loss: 0.236.. \n",
      "Epoch: 325/10000..  Training Loss: 0.235.. \n",
      "Epoch: 326/10000..  Training Loss: 0.234.. \n",
      "Epoch: 327/10000..  Training Loss: 0.233.. \n",
      "Epoch: 328/10000..  Training Loss: 0.233.. \n",
      "Epoch: 329/10000..  Training Loss: 0.234.. \n",
      "Epoch: 330/10000..  Training Loss: 0.235.. \n",
      "Epoch: 331/10000..  Training Loss: 0.236.. \n",
      "Epoch: 332/10000..  Training Loss: 0.236.. \n",
      "Epoch: 333/10000..  Training Loss: 0.238.. \n",
      "Epoch: 334/10000..  Training Loss: 0.237.. \n",
      "Epoch: 335/10000..  Training Loss: 0.236.. \n",
      "Epoch: 336/10000..  Training Loss: 0.234.. \n",
      "Epoch: 337/10000..  Training Loss: 0.233.. \n",
      "Epoch: 338/10000..  Training Loss: 0.232.. \n",
      "Epoch: 339/10000..  Training Loss: 0.232.. \n",
      "Epoch: 340/10000..  Training Loss: 0.232.. \n",
      "Epoch: 341/10000..  Training Loss: 0.233.. \n",
      "Epoch: 342/10000..  Training Loss: 0.234.. \n",
      "Epoch: 343/10000..  Training Loss: 0.235.. \n",
      "Epoch: 344/10000..  Training Loss: 0.235.. \n",
      "Epoch: 345/10000..  Training Loss: 0.235.. \n",
      "Epoch: 346/10000..  Training Loss: 0.233.. \n",
      "Epoch: 347/10000..  Training Loss: 0.232.. \n",
      "Epoch: 348/10000..  Training Loss: 0.231.. \n",
      "Epoch: 349/10000..  Training Loss: 0.230.. \n",
      "Epoch: 350/10000..  Training Loss: 0.231.. \n",
      "Epoch: 351/10000..  Training Loss: 0.231.. \n",
      "Epoch: 352/10000..  Training Loss: 0.232.. \n",
      "Epoch: 353/10000..  Training Loss: 0.234.. \n",
      "Epoch: 354/10000..  Training Loss: 0.234.. \n",
      "Epoch: 355/10000..  Training Loss: 0.235.. \n",
      "Epoch: 356/10000..  Training Loss: 0.234.. \n",
      "Epoch: 357/10000..  Training Loss: 0.232.. \n",
      "Epoch: 358/10000..  Training Loss: 0.230.. \n",
      "Epoch: 359/10000..  Training Loss: 0.229.. \n",
      "Epoch: 360/10000..  Training Loss: 0.229.. \n",
      "Epoch: 361/10000..  Training Loss: 0.230.. \n",
      "Epoch: 362/10000..  Training Loss: 0.231.. \n",
      "Epoch: 363/10000..  Training Loss: 0.232.. \n",
      "Epoch: 364/10000..  Training Loss: 0.234.. \n",
      "Epoch: 365/10000..  Training Loss: 0.234.. \n",
      "Epoch: 366/10000..  Training Loss: 0.234.. \n",
      "Epoch: 367/10000..  Training Loss: 0.231.. \n",
      "Epoch: 368/10000..  Training Loss: 0.229.. \n",
      "Epoch: 369/10000..  Training Loss: 0.228.. \n",
      "Epoch: 370/10000..  Training Loss: 0.228.. \n",
      "Epoch: 371/10000..  Training Loss: 0.229.. \n",
      "Epoch: 372/10000..  Training Loss: 0.231.. \n",
      "Epoch: 373/10000..  Training Loss: 0.232.. \n",
      "Epoch: 374/10000..  Training Loss: 0.232.. \n",
      "Epoch: 375/10000..  Training Loss: 0.231.. \n",
      "Epoch: 376/10000..  Training Loss: 0.229.. \n",
      "Epoch: 377/10000..  Training Loss: 0.228.. \n",
      "Epoch: 378/10000..  Training Loss: 0.227.. \n",
      "Epoch: 379/10000..  Training Loss: 0.227.. \n",
      "Epoch: 380/10000..  Training Loss: 0.228.. \n",
      "Epoch: 381/10000..  Training Loss: 0.228.. \n",
      "Epoch: 382/10000..  Training Loss: 0.229.. \n",
      "Epoch: 383/10000..  Training Loss: 0.229.. \n",
      "Epoch: 384/10000..  Training Loss: 0.229.. \n",
      "Epoch: 385/10000..  Training Loss: 0.228.. \n",
      "Epoch: 386/10000..  Training Loss: 0.227.. \n",
      "Epoch: 387/10000..  Training Loss: 0.226.. \n",
      "Epoch: 388/10000..  Training Loss: 0.226.. \n",
      "Epoch: 389/10000..  Training Loss: 0.225.. \n",
      "Epoch: 390/10000..  Training Loss: 0.225.. \n",
      "Epoch: 391/10000..  Training Loss: 0.226.. \n",
      "Epoch: 392/10000..  Training Loss: 0.226.. \n",
      "Epoch: 393/10000..  Training Loss: 0.227.. \n",
      "Epoch: 394/10000..  Training Loss: 0.228.. \n",
      "Epoch: 395/10000..  Training Loss: 0.230.. \n",
      "Epoch: 396/10000..  Training Loss: 0.230.. \n",
      "Epoch: 397/10000..  Training Loss: 0.230.. \n",
      "Epoch: 398/10000..  Training Loss: 0.228.. \n",
      "Epoch: 399/10000..  Training Loss: 0.226.. \n",
      "Epoch: 400/10000..  Training Loss: 0.225.. \n",
      "Epoch: 401/10000..  Training Loss: 0.224.. \n",
      "Epoch: 402/10000..  Training Loss: 0.224.. \n",
      "Epoch: 403/10000..  Training Loss: 0.225.. \n",
      "Epoch: 404/10000..  Training Loss: 0.226.. \n",
      "Epoch: 405/10000..  Training Loss: 0.227.. \n",
      "Epoch: 406/10000..  Training Loss: 0.229.. \n",
      "Epoch: 407/10000..  Training Loss: 0.228.. \n",
      "Epoch: 408/10000..  Training Loss: 0.227.. \n",
      "Epoch: 409/10000..  Training Loss: 0.225.. \n",
      "Epoch: 410/10000..  Training Loss: 0.224.. \n",
      "Epoch: 411/10000..  Training Loss: 0.223.. \n",
      "Epoch: 412/10000..  Training Loss: 0.223.. \n",
      "Epoch: 413/10000..  Training Loss: 0.223.. \n",
      "Epoch: 414/10000..  Training Loss: 0.224.. \n",
      "Epoch: 415/10000..  Training Loss: 0.225.. \n",
      "Epoch: 416/10000..  Training Loss: 0.226.. \n",
      "Epoch: 417/10000..  Training Loss: 0.227.. \n",
      "Epoch: 418/10000..  Training Loss: 0.226.. \n",
      "Epoch: 419/10000..  Training Loss: 0.226.. \n",
      "Epoch: 420/10000..  Training Loss: 0.224.. \n",
      "Epoch: 421/10000..  Training Loss: 0.222.. \n",
      "Epoch: 422/10000..  Training Loss: 0.221.. \n",
      "Epoch: 423/10000..  Training Loss: 0.221.. \n",
      "Epoch: 424/10000..  Training Loss: 0.222.. \n",
      "Epoch: 425/10000..  Training Loss: 0.223.. \n",
      "Epoch: 426/10000..  Training Loss: 0.225.. \n",
      "Epoch: 427/10000..  Training Loss: 0.226.. \n",
      "Epoch: 428/10000..  Training Loss: 0.227.. \n",
      "Epoch: 429/10000..  Training Loss: 0.225.. \n",
      "Epoch: 430/10000..  Training Loss: 0.224.. \n",
      "Epoch: 431/10000..  Training Loss: 0.222.. \n",
      "Epoch: 432/10000..  Training Loss: 0.220.. \n",
      "Epoch: 433/10000..  Training Loss: 0.220.. \n",
      "Epoch: 434/10000..  Training Loss: 0.221.. \n",
      "Epoch: 435/10000..  Training Loss: 0.222.. \n",
      "Epoch: 436/10000..  Training Loss: 0.223.. \n",
      "Epoch: 437/10000..  Training Loss: 0.225.. \n",
      "Epoch: 438/10000..  Training Loss: 0.224.. \n",
      "Epoch: 439/10000..  Training Loss: 0.224.. \n",
      "Epoch: 440/10000..  Training Loss: 0.222.. \n",
      "Epoch: 441/10000..  Training Loss: 0.221.. \n",
      "Epoch: 442/10000..  Training Loss: 0.220.. \n",
      "Epoch: 443/10000..  Training Loss: 0.219.. \n",
      "Epoch: 444/10000..  Training Loss: 0.219.. \n",
      "Epoch: 445/10000..  Training Loss: 0.220.. \n",
      "Epoch: 446/10000..  Training Loss: 0.221.. \n",
      "Epoch: 447/10000..  Training Loss: 0.222.. \n",
      "Epoch: 448/10000..  Training Loss: 0.223.. \n",
      "Epoch: 449/10000..  Training Loss: 0.223.. \n",
      "Epoch: 450/10000..  Training Loss: 0.223.. \n",
      "Epoch: 451/10000..  Training Loss: 0.222.. \n",
      "Epoch: 452/10000..  Training Loss: 0.221.. \n",
      "Epoch: 453/10000..  Training Loss: 0.219.. \n",
      "Epoch: 454/10000..  Training Loss: 0.218.. \n",
      "Epoch: 455/10000..  Training Loss: 0.218.. \n",
      "Epoch: 456/10000..  Training Loss: 0.218.. \n",
      "Epoch: 457/10000..  Training Loss: 0.219.. \n",
      "Epoch: 458/10000..  Training Loss: 0.220.. \n",
      "Epoch: 459/10000..  Training Loss: 0.222.. \n",
      "Epoch: 460/10000..  Training Loss: 0.222.. \n",
      "Epoch: 461/10000..  Training Loss: 0.222.. \n",
      "Epoch: 462/10000..  Training Loss: 0.221.. \n",
      "Epoch: 463/10000..  Training Loss: 0.220.. \n",
      "Epoch: 464/10000..  Training Loss: 0.218.. \n",
      "Epoch: 465/10000..  Training Loss: 0.217.. \n",
      "Epoch: 466/10000..  Training Loss: 0.217.. \n",
      "Epoch: 467/10000..  Training Loss: 0.217.. \n",
      "Epoch: 468/10000..  Training Loss: 0.218.. \n",
      "Epoch: 469/10000..  Training Loss: 0.219.. \n",
      "Epoch: 470/10000..  Training Loss: 0.220.. \n",
      "Epoch: 471/10000..  Training Loss: 0.221.. \n",
      "Epoch: 472/10000..  Training Loss: 0.222.. \n",
      "Epoch: 473/10000..  Training Loss: 0.221.. \n",
      "Epoch: 474/10000..  Training Loss: 0.220.. \n",
      "Epoch: 475/10000..  Training Loss: 0.217.. \n",
      "Epoch: 476/10000..  Training Loss: 0.216.. \n",
      "Epoch: 477/10000..  Training Loss: 0.216.. \n",
      "Epoch: 478/10000..  Training Loss: 0.216.. \n",
      "Epoch: 479/10000..  Training Loss: 0.217.. \n",
      "Epoch: 480/10000..  Training Loss: 0.218.. \n",
      "Epoch: 481/10000..  Training Loss: 0.219.. \n",
      "Epoch: 482/10000..  Training Loss: 0.220.. \n",
      "Epoch: 483/10000..  Training Loss: 0.220.. \n",
      "Epoch: 484/10000..  Training Loss: 0.219.. \n",
      "Epoch: 485/10000..  Training Loss: 0.217.. \n",
      "Epoch: 486/10000..  Training Loss: 0.216.. \n",
      "Epoch: 487/10000..  Training Loss: 0.215.. \n",
      "Epoch: 488/10000..  Training Loss: 0.215.. \n",
      "Epoch: 489/10000..  Training Loss: 0.215.. \n",
      "Epoch: 490/10000..  Training Loss: 0.216.. \n",
      "Epoch: 491/10000..  Training Loss: 0.217.. \n",
      "Epoch: 492/10000..  Training Loss: 0.218.. \n",
      "Epoch: 493/10000..  Training Loss: 0.219.. \n",
      "Epoch: 494/10000..  Training Loss: 0.220.. \n",
      "Epoch: 495/10000..  Training Loss: 0.219.. \n",
      "Epoch: 496/10000..  Training Loss: 0.217.. \n",
      "Epoch: 497/10000..  Training Loss: 0.215.. \n",
      "Epoch: 498/10000..  Training Loss: 0.214.. \n",
      "Epoch: 499/10000..  Training Loss: 0.213.. \n",
      "Epoch: 500/10000..  Training Loss: 0.214.. \n",
      "Epoch: 501/10000..  Training Loss: 0.214.. \n",
      "Epoch: 502/10000..  Training Loss: 0.215.. \n",
      "Epoch: 503/10000..  Training Loss: 0.218.. \n",
      "Epoch: 504/10000..  Training Loss: 0.219.. \n",
      "Epoch: 505/10000..  Training Loss: 0.221.. \n",
      "Epoch: 506/10000..  Training Loss: 0.218.. \n",
      "Epoch: 507/10000..  Training Loss: 0.216.. \n",
      "Epoch: 508/10000..  Training Loss: 0.214.. \n",
      "Epoch: 509/10000..  Training Loss: 0.213.. \n",
      "Epoch: 510/10000..  Training Loss: 0.213.. \n",
      "Epoch: 511/10000..  Training Loss: 0.214.. \n",
      "Epoch: 512/10000..  Training Loss: 0.215.. \n",
      "Epoch: 513/10000..  Training Loss: 0.217.. \n",
      "Epoch: 514/10000..  Training Loss: 0.218.. \n",
      "Epoch: 515/10000..  Training Loss: 0.217.. \n",
      "Epoch: 516/10000..  Training Loss: 0.215.. \n",
      "Epoch: 517/10000..  Training Loss: 0.213.. \n",
      "Epoch: 518/10000..  Training Loss: 0.212.. \n",
      "Epoch: 519/10000..  Training Loss: 0.212.. \n",
      "Epoch: 520/10000..  Training Loss: 0.212.. \n",
      "Epoch: 521/10000..  Training Loss: 0.214.. \n",
      "Epoch: 522/10000..  Training Loss: 0.215.. \n",
      "Epoch: 523/10000..  Training Loss: 0.217.. \n",
      "Epoch: 524/10000..  Training Loss: 0.216.. \n",
      "Epoch: 525/10000..  Training Loss: 0.216.. \n",
      "Epoch: 526/10000..  Training Loss: 0.213.. \n",
      "Epoch: 527/10000..  Training Loss: 0.211.. \n",
      "Epoch: 528/10000..  Training Loss: 0.211.. \n",
      "Epoch: 529/10000..  Training Loss: 0.211.. \n",
      "Epoch: 530/10000..  Training Loss: 0.213.. \n",
      "Epoch: 531/10000..  Training Loss: 0.214.. \n",
      "Epoch: 532/10000..  Training Loss: 0.215.. \n",
      "Epoch: 533/10000..  Training Loss: 0.215.. \n",
      "Epoch: 534/10000..  Training Loss: 0.214.. \n",
      "Epoch: 535/10000..  Training Loss: 0.212.. \n",
      "Epoch: 536/10000..  Training Loss: 0.211.. \n",
      "Epoch: 537/10000..  Training Loss: 0.210.. \n",
      "Epoch: 538/10000..  Training Loss: 0.210.. \n",
      "Epoch: 539/10000..  Training Loss: 0.211.. \n",
      "Epoch: 540/10000..  Training Loss: 0.211.. \n",
      "Epoch: 541/10000..  Training Loss: 0.213.. \n",
      "Epoch: 542/10000..  Training Loss: 0.213.. \n",
      "Epoch: 543/10000..  Training Loss: 0.214.. \n",
      "Epoch: 544/10000..  Training Loss: 0.212.. \n",
      "Epoch: 545/10000..  Training Loss: 0.212.. \n",
      "Epoch: 546/10000..  Training Loss: 0.210.. \n",
      "Epoch: 547/10000..  Training Loss: 0.209.. \n",
      "Epoch: 548/10000..  Training Loss: 0.209.. \n",
      "Epoch: 549/10000..  Training Loss: 0.209.. \n",
      "Epoch: 550/10000..  Training Loss: 0.209.. \n",
      "Epoch: 551/10000..  Training Loss: 0.210.. \n",
      "Epoch: 552/10000..  Training Loss: 0.211.. \n",
      "Epoch: 553/10000..  Training Loss: 0.211.. \n",
      "Epoch: 554/10000..  Training Loss: 0.212.. \n",
      "Epoch: 555/10000..  Training Loss: 0.212.. \n",
      "Epoch: 556/10000..  Training Loss: 0.212.. \n",
      "Epoch: 557/10000..  Training Loss: 0.211.. \n",
      "Epoch: 558/10000..  Training Loss: 0.210.. \n",
      "Epoch: 559/10000..  Training Loss: 0.209.. \n",
      "Epoch: 560/10000..  Training Loss: 0.208.. \n",
      "Epoch: 561/10000..  Training Loss: 0.208.. \n",
      "Epoch: 562/10000..  Training Loss: 0.208.. \n",
      "Epoch: 563/10000..  Training Loss: 0.208.. \n",
      "Epoch: 564/10000..  Training Loss: 0.208.. \n",
      "Epoch: 565/10000..  Training Loss: 0.209.. \n",
      "Epoch: 566/10000..  Training Loss: 0.210.. \n",
      "Epoch: 567/10000..  Training Loss: 0.212.. \n",
      "Epoch: 568/10000..  Training Loss: 0.213.. \n",
      "Epoch: 569/10000..  Training Loss: 0.216.. \n",
      "Epoch: 570/10000..  Training Loss: 0.215.. \n",
      "Epoch: 571/10000..  Training Loss: 0.214.. \n",
      "Epoch: 572/10000..  Training Loss: 0.209.. \n",
      "Epoch: 573/10000..  Training Loss: 0.207.. \n",
      "Epoch: 574/10000..  Training Loss: 0.207.. \n",
      "Epoch: 575/10000..  Training Loss: 0.208.. \n",
      "Epoch: 576/10000..  Training Loss: 0.211.. \n",
      "Epoch: 577/10000..  Training Loss: 0.213.. \n",
      "Epoch: 578/10000..  Training Loss: 0.215.. \n",
      "Epoch: 579/10000..  Training Loss: 0.213.. \n",
      "Epoch: 580/10000..  Training Loss: 0.211.. \n",
      "Epoch: 581/10000..  Training Loss: 0.207.. \n",
      "Epoch: 582/10000..  Training Loss: 0.206.. \n",
      "Epoch: 583/10000..  Training Loss: 0.208.. \n",
      "Epoch: 584/10000..  Training Loss: 0.210.. \n",
      "Epoch: 585/10000..  Training Loss: 0.212.. \n",
      "Epoch: 586/10000..  Training Loss: 0.211.. \n",
      "Epoch: 587/10000..  Training Loss: 0.210.. \n",
      "Epoch: 588/10000..  Training Loss: 0.208.. \n",
      "Epoch: 589/10000..  Training Loss: 0.206.. \n",
      "Epoch: 590/10000..  Training Loss: 0.206.. \n",
      "Epoch: 591/10000..  Training Loss: 0.207.. \n",
      "Epoch: 592/10000..  Training Loss: 0.208.. \n",
      "Epoch: 593/10000..  Training Loss: 0.209.. \n",
      "Epoch: 594/10000..  Training Loss: 0.210.. \n",
      "Epoch: 595/10000..  Training Loss: 0.208.. \n",
      "Epoch: 596/10000..  Training Loss: 0.207.. \n",
      "Epoch: 597/10000..  Training Loss: 0.206.. \n",
      "Epoch: 598/10000..  Training Loss: 0.205.. \n",
      "Epoch: 599/10000..  Training Loss: 0.205.. \n",
      "Epoch: 600/10000..  Training Loss: 0.206.. \n",
      "Epoch: 601/10000..  Training Loss: 0.207.. \n",
      "Epoch: 602/10000..  Training Loss: 0.207.. \n",
      "Epoch: 603/10000..  Training Loss: 0.208.. \n",
      "Epoch: 604/10000..  Training Loss: 0.207.. \n",
      "Epoch: 605/10000..  Training Loss: 0.207.. \n",
      "Epoch: 606/10000..  Training Loss: 0.206.. \n",
      "Epoch: 607/10000..  Training Loss: 0.205.. \n",
      "Epoch: 608/10000..  Training Loss: 0.204.. \n",
      "Epoch: 609/10000..  Training Loss: 0.204.. \n",
      "Epoch: 610/10000..  Training Loss: 0.204.. \n",
      "Epoch: 611/10000..  Training Loss: 0.204.. \n",
      "Epoch: 612/10000..  Training Loss: 0.205.. \n",
      "Epoch: 613/10000..  Training Loss: 0.205.. \n",
      "Epoch: 614/10000..  Training Loss: 0.206.. \n",
      "Epoch: 615/10000..  Training Loss: 0.206.. \n",
      "Epoch: 616/10000..  Training Loss: 0.206.. \n",
      "Epoch: 617/10000..  Training Loss: 0.206.. \n",
      "Epoch: 618/10000..  Training Loss: 0.207.. \n",
      "Epoch: 619/10000..  Training Loss: 0.206.. \n",
      "Epoch: 620/10000..  Training Loss: 0.205.. \n",
      "Epoch: 621/10000..  Training Loss: 0.205.. \n",
      "Epoch: 622/10000..  Training Loss: 0.204.. \n",
      "Epoch: 623/10000..  Training Loss: 0.203.. \n",
      "Epoch: 624/10000..  Training Loss: 0.203.. \n",
      "Epoch: 625/10000..  Training Loss: 0.203.. \n",
      "Epoch: 626/10000..  Training Loss: 0.202.. \n",
      "Epoch: 627/10000..  Training Loss: 0.202.. \n",
      "Epoch: 628/10000..  Training Loss: 0.202.. \n",
      "Epoch: 629/10000..  Training Loss: 0.202.. \n",
      "Epoch: 630/10000..  Training Loss: 0.202.. \n",
      "Epoch: 631/10000..  Training Loss: 0.203.. \n",
      "Epoch: 632/10000..  Training Loss: 0.203.. \n",
      "Epoch: 633/10000..  Training Loss: 0.205.. \n",
      "Epoch: 634/10000..  Training Loss: 0.208.. \n",
      "Epoch: 635/10000..  Training Loss: 0.214.. \n",
      "Epoch: 636/10000..  Training Loss: 0.216.. \n",
      "Epoch: 637/10000..  Training Loss: 0.217.. \n",
      "Epoch: 638/10000..  Training Loss: 0.208.. \n",
      "Epoch: 639/10000..  Training Loss: 0.202.. \n",
      "Epoch: 640/10000..  Training Loss: 0.203.. \n",
      "Epoch: 641/10000..  Training Loss: 0.208.. \n",
      "Epoch: 642/10000..  Training Loss: 0.214.. \n",
      "Epoch: 643/10000..  Training Loss: 0.213.. \n",
      "Epoch: 644/10000..  Training Loss: 0.208.. \n",
      "Epoch: 645/10000..  Training Loss: 0.202.. \n",
      "Epoch: 646/10000..  Training Loss: 0.202.. \n",
      "Epoch: 647/10000..  Training Loss: 0.206.. \n",
      "Epoch: 648/10000..  Training Loss: 0.209.. \n",
      "Epoch: 649/10000..  Training Loss: 0.210.. \n",
      "Epoch: 650/10000..  Training Loss: 0.205.. \n",
      "Epoch: 651/10000..  Training Loss: 0.202.. \n",
      "Epoch: 652/10000..  Training Loss: 0.201.. \n",
      "Epoch: 653/10000..  Training Loss: 0.203.. \n",
      "Epoch: 654/10000..  Training Loss: 0.206.. \n",
      "Epoch: 655/10000..  Training Loss: 0.206.. \n",
      "Epoch: 656/10000..  Training Loss: 0.205.. \n",
      "Epoch: 657/10000..  Training Loss: 0.202.. \n",
      "Epoch: 658/10000..  Training Loss: 0.201.. \n",
      "Epoch: 659/10000..  Training Loss: 0.201.. \n",
      "Epoch: 660/10000..  Training Loss: 0.203.. \n",
      "Epoch: 661/10000..  Training Loss: 0.205.. \n",
      "Epoch: 662/10000..  Training Loss: 0.204.. \n",
      "Epoch: 663/10000..  Training Loss: 0.203.. \n",
      "Epoch: 664/10000..  Training Loss: 0.201.. \n",
      "Epoch: 665/10000..  Training Loss: 0.200.. \n",
      "Epoch: 666/10000..  Training Loss: 0.200.. \n",
      "Epoch: 667/10000..  Training Loss: 0.201.. \n",
      "Epoch: 668/10000..  Training Loss: 0.202.. \n",
      "Epoch: 669/10000..  Training Loss: 0.202.. \n",
      "Epoch: 670/10000..  Training Loss: 0.202.. \n",
      "Epoch: 671/10000..  Training Loss: 0.201.. \n",
      "Epoch: 672/10000..  Training Loss: 0.200.. \n",
      "Epoch: 673/10000..  Training Loss: 0.200.. \n",
      "Epoch: 674/10000..  Training Loss: 0.200.. \n",
      "Epoch: 675/10000..  Training Loss: 0.200.. \n",
      "Epoch: 676/10000..  Training Loss: 0.201.. \n",
      "Epoch: 677/10000..  Training Loss: 0.202.. \n",
      "Epoch: 678/10000..  Training Loss: 0.201.. \n",
      "Epoch: 679/10000..  Training Loss: 0.201.. \n",
      "Epoch: 680/10000..  Training Loss: 0.201.. \n",
      "Epoch: 681/10000..  Training Loss: 0.200.. \n",
      "Epoch: 682/10000..  Training Loss: 0.199.. \n",
      "Epoch: 683/10000..  Training Loss: 0.199.. \n",
      "Epoch: 684/10000..  Training Loss: 0.199.. \n",
      "Epoch: 685/10000..  Training Loss: 0.199.. \n",
      "Epoch: 686/10000..  Training Loss: 0.199.. \n",
      "Epoch: 687/10000..  Training Loss: 0.199.. \n",
      "Epoch: 688/10000..  Training Loss: 0.200.. \n",
      "Epoch: 689/10000..  Training Loss: 0.200.. \n",
      "Epoch: 690/10000..  Training Loss: 0.201.. \n",
      "Epoch: 691/10000..  Training Loss: 0.201.. \n",
      "Epoch: 692/10000..  Training Loss: 0.202.. \n",
      "Epoch: 693/10000..  Training Loss: 0.201.. \n",
      "Epoch: 694/10000..  Training Loss: 0.201.. \n",
      "Epoch: 695/10000..  Training Loss: 0.200.. \n",
      "Epoch: 696/10000..  Training Loss: 0.200.. \n",
      "Epoch: 697/10000..  Training Loss: 0.199.. \n",
      "Epoch: 698/10000..  Training Loss: 0.198.. \n",
      "Epoch: 699/10000..  Training Loss: 0.198.. \n",
      "Epoch: 700/10000..  Training Loss: 0.198.. \n",
      "Epoch: 701/10000..  Training Loss: 0.198.. \n",
      "Epoch: 702/10000..  Training Loss: 0.198.. \n",
      "Epoch: 703/10000..  Training Loss: 0.198.. \n",
      "Epoch: 704/10000..  Training Loss: 0.198.. \n",
      "Epoch: 705/10000..  Training Loss: 0.198.. \n",
      "Epoch: 706/10000..  Training Loss: 0.199.. \n",
      "Epoch: 707/10000..  Training Loss: 0.200.. \n",
      "Epoch: 708/10000..  Training Loss: 0.201.. \n",
      "Epoch: 709/10000..  Training Loss: 0.204.. \n",
      "Epoch: 710/10000..  Training Loss: 0.204.. \n",
      "Epoch: 711/10000..  Training Loss: 0.205.. \n",
      "Epoch: 712/10000..  Training Loss: 0.203.. \n",
      "Epoch: 713/10000..  Training Loss: 0.201.. \n",
      "Epoch: 714/10000..  Training Loss: 0.198.. \n",
      "Epoch: 715/10000..  Training Loss: 0.197.. \n",
      "Epoch: 716/10000..  Training Loss: 0.197.. \n",
      "Epoch: 717/10000..  Training Loss: 0.198.. \n",
      "Epoch: 718/10000..  Training Loss: 0.199.. \n",
      "Epoch: 719/10000..  Training Loss: 0.201.. \n",
      "Epoch: 720/10000..  Training Loss: 0.203.. \n",
      "Epoch: 721/10000..  Training Loss: 0.203.. \n",
      "Epoch: 722/10000..  Training Loss: 0.203.. \n",
      "Epoch: 723/10000..  Training Loss: 0.200.. \n",
      "Epoch: 724/10000..  Training Loss: 0.198.. \n",
      "Epoch: 725/10000..  Training Loss: 0.196.. \n",
      "Epoch: 726/10000..  Training Loss: 0.197.. \n",
      "Epoch: 727/10000..  Training Loss: 0.197.. \n",
      "Epoch: 728/10000..  Training Loss: 0.198.. \n",
      "Epoch: 729/10000..  Training Loss: 0.200.. \n",
      "Epoch: 730/10000..  Training Loss: 0.202.. \n",
      "Epoch: 731/10000..  Training Loss: 0.203.. \n",
      "Epoch: 732/10000..  Training Loss: 0.200.. \n",
      "Epoch: 733/10000..  Training Loss: 0.199.. \n",
      "Epoch: 734/10000..  Training Loss: 0.197.. \n",
      "Epoch: 735/10000..  Training Loss: 0.196.. \n",
      "Epoch: 736/10000..  Training Loss: 0.196.. \n",
      "Epoch: 737/10000..  Training Loss: 0.197.. \n",
      "Epoch: 738/10000..  Training Loss: 0.197.. \n",
      "Epoch: 739/10000..  Training Loss: 0.198.. \n",
      "Epoch: 740/10000..  Training Loss: 0.199.. \n",
      "Epoch: 741/10000..  Training Loss: 0.199.. \n",
      "Epoch: 742/10000..  Training Loss: 0.199.. \n",
      "Epoch: 743/10000..  Training Loss: 0.198.. \n",
      "Epoch: 744/10000..  Training Loss: 0.197.. \n",
      "Epoch: 745/10000..  Training Loss: 0.196.. \n",
      "Epoch: 746/10000..  Training Loss: 0.196.. \n",
      "Epoch: 747/10000..  Training Loss: 0.195.. \n",
      "Epoch: 748/10000..  Training Loss: 0.195.. \n",
      "Epoch: 749/10000..  Training Loss: 0.195.. \n",
      "Epoch: 750/10000..  Training Loss: 0.195.. \n",
      "Epoch: 751/10000..  Training Loss: 0.195.. \n",
      "Epoch: 752/10000..  Training Loss: 0.195.. \n",
      "Epoch: 753/10000..  Training Loss: 0.195.. \n",
      "Epoch: 754/10000..  Training Loss: 0.195.. \n",
      "Epoch: 755/10000..  Training Loss: 0.195.. \n",
      "Epoch: 756/10000..  Training Loss: 0.196.. \n",
      "Epoch: 757/10000..  Training Loss: 0.197.. \n",
      "Epoch: 758/10000..  Training Loss: 0.198.. \n",
      "Epoch: 759/10000..  Training Loss: 0.201.. \n",
      "Epoch: 760/10000..  Training Loss: 0.202.. \n",
      "Epoch: 761/10000..  Training Loss: 0.204.. \n",
      "Epoch: 762/10000..  Training Loss: 0.201.. \n",
      "Epoch: 763/10000..  Training Loss: 0.198.. \n",
      "Epoch: 764/10000..  Training Loss: 0.195.. \n",
      "Epoch: 765/10000..  Training Loss: 0.194.. \n",
      "Epoch: 766/10000..  Training Loss: 0.195.. \n",
      "Epoch: 767/10000..  Training Loss: 0.196.. \n",
      "Epoch: 768/10000..  Training Loss: 0.199.. \n",
      "Epoch: 769/10000..  Training Loss: 0.201.. \n",
      "Epoch: 770/10000..  Training Loss: 0.204.. \n",
      "Epoch: 771/10000..  Training Loss: 0.201.. \n",
      "Epoch: 772/10000..  Training Loss: 0.198.. \n",
      "Epoch: 773/10000..  Training Loss: 0.195.. \n",
      "Epoch: 774/10000..  Training Loss: 0.194.. \n",
      "Epoch: 775/10000..  Training Loss: 0.195.. \n",
      "Epoch: 776/10000..  Training Loss: 0.197.. \n",
      "Epoch: 777/10000..  Training Loss: 0.200.. \n",
      "Epoch: 778/10000..  Training Loss: 0.201.. \n",
      "Epoch: 779/10000..  Training Loss: 0.202.. \n",
      "Epoch: 780/10000..  Training Loss: 0.198.. \n",
      "Epoch: 781/10000..  Training Loss: 0.195.. \n",
      "Epoch: 782/10000..  Training Loss: 0.194.. \n",
      "Epoch: 783/10000..  Training Loss: 0.195.. \n",
      "Epoch: 784/10000..  Training Loss: 0.197.. \n",
      "Epoch: 785/10000..  Training Loss: 0.199.. \n",
      "Epoch: 786/10000..  Training Loss: 0.199.. \n",
      "Epoch: 787/10000..  Training Loss: 0.197.. \n",
      "Epoch: 788/10000..  Training Loss: 0.195.. \n",
      "Epoch: 789/10000..  Training Loss: 0.194.. \n",
      "Epoch: 790/10000..  Training Loss: 0.194.. \n",
      "Epoch: 791/10000..  Training Loss: 0.194.. \n",
      "Epoch: 792/10000..  Training Loss: 0.194.. \n",
      "Epoch: 793/10000..  Training Loss: 0.196.. \n",
      "Epoch: 794/10000..  Training Loss: 0.196.. \n",
      "Epoch: 795/10000..  Training Loss: 0.196.. \n",
      "Epoch: 796/10000..  Training Loss: 0.196.. \n",
      "Epoch: 797/10000..  Training Loss: 0.196.. \n",
      "Epoch: 798/10000..  Training Loss: 0.194.. \n",
      "Epoch: 799/10000..  Training Loss: 0.193.. \n",
      "Epoch: 800/10000..  Training Loss: 0.193.. \n",
      "Epoch: 801/10000..  Training Loss: 0.193.. \n",
      "Epoch: 802/10000..  Training Loss: 0.193.. \n",
      "Epoch: 803/10000..  Training Loss: 0.193.. \n",
      "Epoch: 804/10000..  Training Loss: 0.194.. \n",
      "Epoch: 805/10000..  Training Loss: 0.194.. \n",
      "Epoch: 806/10000..  Training Loss: 0.195.. \n",
      "Epoch: 807/10000..  Training Loss: 0.195.. \n",
      "Epoch: 808/10000..  Training Loss: 0.196.. \n",
      "Epoch: 809/10000..  Training Loss: 0.196.. \n",
      "Epoch: 810/10000..  Training Loss: 0.196.. \n",
      "Epoch: 811/10000..  Training Loss: 0.194.. \n",
      "Epoch: 812/10000..  Training Loss: 0.193.. \n",
      "Epoch: 813/10000..  Training Loss: 0.192.. \n",
      "Epoch: 814/10000..  Training Loss: 0.192.. \n",
      "Epoch: 815/10000..  Training Loss: 0.192.. \n",
      "Epoch: 816/10000..  Training Loss: 0.192.. \n",
      "Epoch: 817/10000..  Training Loss: 0.192.. \n",
      "Epoch: 818/10000..  Training Loss: 0.192.. \n",
      "Epoch: 819/10000..  Training Loss: 0.193.. \n",
      "Epoch: 820/10000..  Training Loss: 0.194.. \n",
      "Epoch: 821/10000..  Training Loss: 0.195.. \n",
      "Epoch: 822/10000..  Training Loss: 0.196.. \n",
      "Epoch: 823/10000..  Training Loss: 0.197.. \n",
      "Epoch: 824/10000..  Training Loss: 0.196.. \n",
      "Epoch: 825/10000..  Training Loss: 0.195.. \n",
      "Epoch: 826/10000..  Training Loss: 0.194.. \n",
      "Epoch: 827/10000..  Training Loss: 0.193.. \n",
      "Epoch: 828/10000..  Training Loss: 0.192.. \n",
      "Epoch: 829/10000..  Training Loss: 0.191.. \n",
      "Epoch: 830/10000..  Training Loss: 0.191.. \n",
      "Epoch: 831/10000..  Training Loss: 0.191.. \n",
      "Epoch: 832/10000..  Training Loss: 0.191.. \n",
      "Epoch: 833/10000..  Training Loss: 0.191.. \n",
      "Epoch: 834/10000..  Training Loss: 0.192.. \n",
      "Epoch: 835/10000..  Training Loss: 0.192.. \n",
      "Epoch: 836/10000..  Training Loss: 0.193.. \n",
      "Epoch: 837/10000..  Training Loss: 0.194.. \n",
      "Epoch: 838/10000..  Training Loss: 0.197.. \n",
      "Epoch: 839/10000..  Training Loss: 0.197.. \n",
      "Epoch: 840/10000..  Training Loss: 0.199.. \n",
      "Epoch: 841/10000..  Training Loss: 0.196.. \n",
      "Epoch: 842/10000..  Training Loss: 0.194.. \n",
      "Epoch: 843/10000..  Training Loss: 0.191.. \n",
      "Epoch: 844/10000..  Training Loss: 0.190.. \n",
      "Epoch: 845/10000..  Training Loss: 0.191.. \n",
      "Epoch: 846/10000..  Training Loss: 0.192.. \n",
      "Epoch: 847/10000..  Training Loss: 0.194.. \n",
      "Epoch: 848/10000..  Training Loss: 0.196.. \n",
      "Epoch: 849/10000..  Training Loss: 0.200.. \n",
      "Epoch: 850/10000..  Training Loss: 0.198.. \n",
      "Epoch: 851/10000..  Training Loss: 0.196.. \n",
      "Epoch: 852/10000..  Training Loss: 0.192.. \n",
      "Epoch: 853/10000..  Training Loss: 0.190.. \n",
      "Epoch: 854/10000..  Training Loss: 0.190.. \n",
      "Epoch: 855/10000..  Training Loss: 0.191.. \n",
      "Epoch: 856/10000..  Training Loss: 0.193.. \n",
      "Epoch: 857/10000..  Training Loss: 0.195.. \n",
      "Epoch: 858/10000..  Training Loss: 0.197.. \n",
      "Epoch: 859/10000..  Training Loss: 0.195.. \n",
      "Epoch: 860/10000..  Training Loss: 0.194.. \n",
      "Epoch: 861/10000..  Training Loss: 0.191.. \n",
      "Epoch: 862/10000..  Training Loss: 0.190.. \n",
      "Epoch: 863/10000..  Training Loss: 0.190.. \n",
      "Epoch: 864/10000..  Training Loss: 0.191.. \n",
      "Epoch: 865/10000..  Training Loss: 0.192.. \n",
      "Epoch: 866/10000..  Training Loss: 0.193.. \n",
      "Epoch: 867/10000..  Training Loss: 0.194.. \n",
      "Epoch: 868/10000..  Training Loss: 0.193.. \n",
      "Epoch: 869/10000..  Training Loss: 0.192.. \n",
      "Epoch: 870/10000..  Training Loss: 0.191.. \n",
      "Epoch: 871/10000..  Training Loss: 0.190.. \n",
      "Epoch: 872/10000..  Training Loss: 0.189.. \n",
      "Epoch: 873/10000..  Training Loss: 0.189.. \n",
      "Epoch: 874/10000..  Training Loss: 0.189.. \n",
      "Epoch: 875/10000..  Training Loss: 0.190.. \n",
      "Epoch: 876/10000..  Training Loss: 0.190.. \n",
      "Epoch: 877/10000..  Training Loss: 0.191.. \n",
      "Epoch: 878/10000..  Training Loss: 0.192.. \n",
      "Epoch: 879/10000..  Training Loss: 0.193.. \n",
      "Epoch: 880/10000..  Training Loss: 0.194.. \n",
      "Epoch: 881/10000..  Training Loss: 0.193.. \n",
      "Epoch: 882/10000..  Training Loss: 0.192.. \n",
      "Epoch: 883/10000..  Training Loss: 0.191.. \n",
      "Epoch: 884/10000..  Training Loss: 0.189.. \n",
      "Epoch: 885/10000..  Training Loss: 0.189.. \n",
      "Epoch: 886/10000..  Training Loss: 0.189.. \n",
      "Epoch: 887/10000..  Training Loss: 0.189.. \n",
      "Epoch: 888/10000..  Training Loss: 0.189.. \n",
      "Epoch: 889/10000..  Training Loss: 0.189.. \n",
      "Epoch: 890/10000..  Training Loss: 0.190.. \n",
      "Epoch: 891/10000..  Training Loss: 0.192.. \n",
      "Epoch: 892/10000..  Training Loss: 0.192.. \n",
      "Epoch: 893/10000..  Training Loss: 0.194.. \n",
      "Epoch: 894/10000..  Training Loss: 0.193.. \n",
      "Epoch: 895/10000..  Training Loss: 0.193.. \n",
      "Epoch: 896/10000..  Training Loss: 0.192.. \n",
      "Epoch: 897/10000..  Training Loss: 0.191.. \n",
      "Epoch: 898/10000..  Training Loss: 0.189.. \n",
      "Epoch: 899/10000..  Training Loss: 0.188.. \n",
      "Epoch: 900/10000..  Training Loss: 0.188.. \n",
      "Epoch: 901/10000..  Training Loss: 0.188.. \n",
      "Epoch: 902/10000..  Training Loss: 0.188.. \n",
      "Epoch: 903/10000..  Training Loss: 0.188.. \n",
      "Epoch: 904/10000..  Training Loss: 0.189.. \n",
      "Epoch: 905/10000..  Training Loss: 0.190.. \n",
      "Epoch: 906/10000..  Training Loss: 0.192.. \n",
      "Epoch: 907/10000..  Training Loss: 0.193.. \n",
      "Epoch: 908/10000..  Training Loss: 0.195.. \n",
      "Epoch: 909/10000..  Training Loss: 0.194.. \n",
      "Epoch: 910/10000..  Training Loss: 0.194.. \n",
      "Epoch: 911/10000..  Training Loss: 0.190.. \n",
      "Epoch: 912/10000..  Training Loss: 0.189.. \n",
      "Epoch: 913/10000..  Training Loss: 0.188.. \n",
      "Epoch: 914/10000..  Training Loss: 0.188.. \n",
      "Epoch: 915/10000..  Training Loss: 0.188.. \n",
      "Epoch: 916/10000..  Training Loss: 0.189.. \n",
      "Epoch: 917/10000..  Training Loss: 0.192.. \n",
      "Epoch: 918/10000..  Training Loss: 0.193.. \n",
      "Epoch: 919/10000..  Training Loss: 0.195.. \n",
      "Epoch: 920/10000..  Training Loss: 0.193.. \n",
      "Epoch: 921/10000..  Training Loss: 0.192.. \n",
      "Epoch: 922/10000..  Training Loss: 0.190.. \n",
      "Epoch: 923/10000..  Training Loss: 0.188.. \n",
      "Epoch: 924/10000..  Training Loss: 0.187.. \n",
      "Epoch: 925/10000..  Training Loss: 0.187.. \n",
      "Epoch: 926/10000..  Training Loss: 0.188.. \n",
      "Epoch: 927/10000..  Training Loss: 0.188.. \n",
      "Epoch: 928/10000..  Training Loss: 0.190.. \n",
      "Epoch: 929/10000..  Training Loss: 0.191.. \n",
      "Epoch: 930/10000..  Training Loss: 0.192.. \n",
      "Epoch: 931/10000..  Training Loss: 0.192.. \n",
      "Epoch: 932/10000..  Training Loss: 0.192.. \n",
      "Epoch: 933/10000..  Training Loss: 0.190.. \n",
      "Epoch: 934/10000..  Training Loss: 0.189.. \n",
      "Epoch: 935/10000..  Training Loss: 0.187.. \n",
      "Epoch: 936/10000..  Training Loss: 0.187.. \n",
      "Epoch: 937/10000..  Training Loss: 0.187.. \n",
      "Epoch: 938/10000..  Training Loss: 0.187.. \n",
      "Epoch: 939/10000..  Training Loss: 0.187.. \n",
      "Epoch: 940/10000..  Training Loss: 0.188.. \n",
      "Epoch: 941/10000..  Training Loss: 0.189.. \n",
      "Epoch: 942/10000..  Training Loss: 0.190.. \n",
      "Epoch: 943/10000..  Training Loss: 0.192.. \n",
      "Epoch: 944/10000..  Training Loss: 0.192.. \n",
      "Epoch: 945/10000..  Training Loss: 0.192.. \n",
      "Epoch: 946/10000..  Training Loss: 0.190.. \n",
      "Epoch: 947/10000..  Training Loss: 0.189.. \n",
      "Epoch: 948/10000..  Training Loss: 0.187.. \n",
      "Epoch: 949/10000..  Training Loss: 0.186.. \n",
      "Epoch: 950/10000..  Training Loss: 0.186.. \n",
      "Epoch: 951/10000..  Training Loss: 0.186.. \n",
      "Epoch: 952/10000..  Training Loss: 0.187.. \n",
      "Epoch: 953/10000..  Training Loss: 0.188.. \n",
      "Epoch: 954/10000..  Training Loss: 0.190.. \n",
      "Epoch: 955/10000..  Training Loss: 0.191.. \n",
      "Epoch: 956/10000..  Training Loss: 0.193.. \n",
      "Epoch: 957/10000..  Training Loss: 0.192.. \n",
      "Epoch: 958/10000..  Training Loss: 0.192.. \n",
      "Epoch: 959/10000..  Training Loss: 0.189.. \n",
      "Epoch: 960/10000..  Training Loss: 0.187.. \n",
      "Epoch: 961/10000..  Training Loss: 0.186.. \n",
      "Epoch: 962/10000..  Training Loss: 0.186.. \n",
      "Epoch: 963/10000..  Training Loss: 0.186.. \n",
      "Epoch: 964/10000..  Training Loss: 0.186.. \n",
      "Epoch: 965/10000..  Training Loss: 0.187.. \n",
      "Epoch: 966/10000..  Training Loss: 0.188.. \n",
      "Epoch: 967/10000..  Training Loss: 0.190.. \n",
      "Epoch: 968/10000..  Training Loss: 0.191.. \n",
      "Epoch: 969/10000..  Training Loss: 0.193.. \n",
      "Epoch: 970/10000..  Training Loss: 0.191.. \n",
      "Epoch: 971/10000..  Training Loss: 0.191.. \n",
      "Epoch: 972/10000..  Training Loss: 0.188.. \n",
      "Epoch: 973/10000..  Training Loss: 0.186.. \n",
      "Epoch: 974/10000..  Training Loss: 0.186.. \n",
      "Epoch: 975/10000..  Training Loss: 0.185.. \n",
      "Epoch: 976/10000..  Training Loss: 0.186.. \n",
      "Epoch: 977/10000..  Training Loss: 0.186.. \n",
      "Epoch: 978/10000..  Training Loss: 0.188.. \n",
      "Epoch: 979/10000..  Training Loss: 0.189.. \n",
      "Epoch: 980/10000..  Training Loss: 0.191.. \n",
      "Epoch: 981/10000..  Training Loss: 0.190.. \n",
      "Epoch: 982/10000..  Training Loss: 0.191.. \n",
      "Epoch: 983/10000..  Training Loss: 0.189.. \n",
      "Epoch: 984/10000..  Training Loss: 0.187.. \n",
      "Epoch: 985/10000..  Training Loss: 0.186.. \n",
      "Epoch: 986/10000..  Training Loss: 0.185.. \n",
      "Epoch: 987/10000..  Training Loss: 0.185.. \n",
      "Epoch: 988/10000..  Training Loss: 0.185.. \n",
      "Epoch: 989/10000..  Training Loss: 0.186.. \n",
      "Epoch: 990/10000..  Training Loss: 0.187.. \n",
      "Epoch: 991/10000..  Training Loss: 0.189.. \n",
      "Epoch: 992/10000..  Training Loss: 0.189.. \n",
      "Epoch: 993/10000..  Training Loss: 0.191.. \n",
      "Epoch: 994/10000..  Training Loss: 0.190.. \n",
      "Epoch: 995/10000..  Training Loss: 0.189.. \n",
      "Epoch: 996/10000..  Training Loss: 0.188.. \n",
      "Epoch: 997/10000..  Training Loss: 0.186.. \n",
      "Epoch: 998/10000..  Training Loss: 0.185.. \n",
      "Epoch: 999/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1000/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1001/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1002/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1003/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1004/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1005/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1006/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1007/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1008/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1009/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1010/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1011/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1012/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1013/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1014/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1015/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1016/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1017/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1018/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1019/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1020/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1021/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1022/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1023/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1024/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1025/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1026/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1027/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1028/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1029/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1030/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1031/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1032/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1033/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1034/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1035/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1036/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1037/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1038/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1039/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1040/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1041/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1042/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1043/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1044/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1045/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1046/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1047/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1048/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1049/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1050/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1051/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1052/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1053/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1054/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1055/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1056/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1057/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1058/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1059/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1060/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1061/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1062/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1063/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1064/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1065/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1066/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1067/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1068/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1069/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1070/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1071/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1072/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1073/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1074/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1075/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1076/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1077/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1078/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1079/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1080/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1081/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1082/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1083/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1084/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1085/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1086/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1087/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1088/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1089/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1090/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1091/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1092/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1093/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1094/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1095/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1096/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1097/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1098/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1099/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1100/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1101/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1102/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1103/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1104/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1105/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1106/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1107/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1108/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1109/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1110/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1111/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1112/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1113/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1114/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1115/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1116/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1117/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1118/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1119/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1120/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1121/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1122/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1123/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1124/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1125/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1126/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1127/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1128/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1129/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1130/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1131/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1132/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1133/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1134/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1135/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1136/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1137/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1138/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1139/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1140/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1141/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1142/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1143/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1144/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1145/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1146/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1147/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1148/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1149/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1150/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1151/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1152/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1153/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1154/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1155/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1156/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1157/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1158/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1159/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1160/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1161/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1162/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1163/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1164/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1165/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1166/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1167/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1168/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1169/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1170/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1171/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1172/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1173/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1174/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1175/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1176/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1177/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1178/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1179/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1180/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1181/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1182/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1183/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1184/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1185/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1186/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1187/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1188/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1189/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1190/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1191/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1192/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1193/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1194/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1195/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1196/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1197/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1198/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1199/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1200/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1201/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1202/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1203/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1204/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1205/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1206/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1207/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1208/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1209/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1210/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1211/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1212/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1213/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1214/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1215/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1216/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1217/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1218/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1219/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1220/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1221/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1222/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1223/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1224/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1225/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1226/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1227/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1228/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1229/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1230/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1231/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1232/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1233/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1234/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1235/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1236/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1237/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1238/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1239/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1240/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1241/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1242/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1243/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1244/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1245/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1246/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1247/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1248/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1249/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1250/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1251/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1252/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1253/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1254/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1255/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1256/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1257/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1258/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1259/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1260/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1261/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1262/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1263/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1264/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1265/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1266/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1267/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1268/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1269/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1270/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1271/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1272/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1273/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1274/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1275/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1276/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1277/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1278/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1279/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1280/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1281/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1282/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1283/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1284/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1285/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1286/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1287/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1288/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1289/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1290/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1291/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1292/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1293/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1294/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1295/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1296/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1297/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1298/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1299/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1300/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1301/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1302/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1303/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1304/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1305/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1306/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1307/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1308/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1309/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1310/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1311/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1312/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1313/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1314/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1315/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1316/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1317/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1318/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1319/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1320/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1321/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1322/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1323/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1324/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1325/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1326/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1327/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1328/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1329/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1330/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1331/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1332/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1333/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1334/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1335/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1336/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1337/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1338/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1339/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1340/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1341/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1342/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1343/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1344/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1345/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1346/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1347/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1348/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1349/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1350/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1351/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1352/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1353/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1354/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1355/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1356/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1357/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1358/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1359/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1360/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1361/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1362/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1363/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1364/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1365/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1366/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1367/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1368/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1369/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1370/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1371/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1372/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1373/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1374/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1375/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1376/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1377/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1378/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1379/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1380/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1381/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1382/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1383/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1384/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1385/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1386/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1387/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1388/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1389/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1390/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1391/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1392/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1393/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1394/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1395/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1396/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1397/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1398/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1399/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1400/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1401/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1402/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1403/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1404/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1405/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1406/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1407/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1408/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1409/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1410/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1411/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1412/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1413/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1414/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1415/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1416/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1417/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1418/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1419/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1420/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1421/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1422/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1423/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1424/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1425/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1426/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1427/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1428/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1429/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1430/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1431/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1432/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1433/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1434/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1435/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1436/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1437/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1438/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1439/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1440/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1441/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1442/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1443/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1444/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1445/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1446/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1447/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1448/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1449/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1450/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1451/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1452/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1453/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1454/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1455/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1456/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1457/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1458/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1459/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1460/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1461/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1462/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1463/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1464/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1465/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1466/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1467/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1468/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1469/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1470/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1471/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1472/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1473/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1474/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1475/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1476/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1477/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1478/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1479/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1480/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1481/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1482/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1483/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1484/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1485/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1486/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1487/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1488/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1489/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1490/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1491/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1492/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1493/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1494/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1495/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1496/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1497/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1498/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1499/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1500/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1501/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1502/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1503/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1504/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1505/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1506/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1507/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1508/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1509/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1510/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1511/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1512/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1513/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1514/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1515/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1516/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1517/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1518/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1519/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1520/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1521/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1522/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1523/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1524/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1525/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1526/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1527/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1528/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1529/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1530/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1531/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1532/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1533/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1534/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1535/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1536/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1537/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1538/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1539/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1540/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1541/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1542/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1543/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1544/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1545/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1546/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1547/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1548/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1549/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1550/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1551/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1552/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1553/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1554/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1555/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1556/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1557/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1558/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1559/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1560/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1561/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1562/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1563/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1564/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1565/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1566/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1567/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1568/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1569/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1570/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1571/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1572/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1573/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1574/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1575/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1576/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1577/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1578/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1579/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1580/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1581/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1582/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1583/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1584/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1585/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1586/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1587/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1588/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1589/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1590/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1591/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1592/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1593/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1594/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1595/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1596/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1597/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1598/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1599/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1600/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1601/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1602/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1603/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1604/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1605/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1606/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1607/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1608/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1609/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1610/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1611/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1612/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1613/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1614/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1615/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1616/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1617/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1618/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1619/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1620/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1621/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1622/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1623/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1624/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1625/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1626/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1627/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1628/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1629/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1630/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1631/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1632/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1633/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1634/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1635/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1636/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1637/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1638/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1639/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1640/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1641/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1642/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1643/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1644/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1645/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1646/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1647/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1648/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1649/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1650/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1651/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1652/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1653/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1654/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1655/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1656/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1657/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1658/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1659/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1660/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1661/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1662/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1663/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1664/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1665/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1666/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1667/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1668/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1669/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1670/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1671/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1672/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1673/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1674/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1675/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1676/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1677/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1678/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1679/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1680/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1681/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1682/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1683/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1684/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1685/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1686/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1687/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1688/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1689/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1690/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1691/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1692/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1693/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1694/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1695/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1696/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1697/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1698/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1699/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1700/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1701/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1702/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1703/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1704/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1705/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1706/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1707/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1708/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1709/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1710/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1711/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1712/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1713/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1714/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1715/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1716/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1717/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1718/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1719/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1720/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1721/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1722/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1723/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1724/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1725/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1726/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1727/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1728/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1729/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1730/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1731/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1732/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1733/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1734/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1735/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1736/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1737/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1738/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1739/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1740/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1741/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1742/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1743/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1744/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1745/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1746/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1747/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1748/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1749/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1750/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1751/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1752/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1753/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1754/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1755/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1756/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1757/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1758/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1759/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1760/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1761/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1762/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1763/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1764/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1765/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1766/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1767/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1768/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1769/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1770/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1771/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1772/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1773/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1774/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1775/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1776/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1777/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1778/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1779/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1780/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1781/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1782/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1783/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1784/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1785/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1786/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1787/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1788/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1789/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1790/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1791/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1792/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1793/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1794/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1795/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1796/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1797/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1798/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1799/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1800/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1801/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1802/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1803/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1804/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1805/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1806/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1807/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1808/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1809/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1810/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1811/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1812/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1813/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1814/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1815/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1816/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1817/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1818/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1819/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1820/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1821/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1822/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1823/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1824/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1825/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1826/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1827/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1828/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1829/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1830/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1831/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1832/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1833/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1834/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1835/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1836/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1837/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1838/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1839/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1840/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1841/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1842/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1843/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1844/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1845/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1846/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1847/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1848/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1849/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1850/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1851/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1852/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1853/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1854/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1855/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1856/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1857/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1858/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1859/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1860/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1861/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1862/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1863/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1864/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1865/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1866/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1867/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1868/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1869/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1870/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1871/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1872/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1873/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1874/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1875/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1876/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1877/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1878/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1879/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1880/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1881/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1882/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1883/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1884/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1885/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1886/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1887/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1888/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1889/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1890/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1891/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1892/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1893/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1894/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1895/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1896/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1897/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1898/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1899/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1900/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1901/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1902/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1903/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1904/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1905/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1906/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1907/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1908/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1909/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1910/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1911/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1912/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1913/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1914/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1915/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1916/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1917/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1918/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1919/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1920/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1921/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1922/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1923/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1924/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1925/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1926/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1927/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1928/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1929/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1930/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1931/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1932/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1933/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1934/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1935/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1936/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1937/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1938/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1939/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1940/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1941/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1942/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1943/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1944/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1945/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1946/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1947/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1948/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1949/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1950/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1951/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1952/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1953/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1954/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1955/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1956/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1957/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1958/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1959/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1960/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1961/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1962/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1963/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1964/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1965/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1966/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1967/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1968/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1969/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1970/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1971/10000..  Training Loss: 0.163.. \n",
      "Epoch: 1972/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1973/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1974/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1975/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1976/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1977/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1978/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1979/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1980/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1981/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1982/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1983/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1984/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1985/10000..  Training Loss: 0.163.. \n",
      "Epoch: 1986/10000..  Training Loss: 0.163.. \n",
      "Epoch: 1987/10000..  Training Loss: 0.163.. \n",
      "Epoch: 1988/10000..  Training Loss: 0.163.. \n",
      "Epoch: 1989/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1990/10000..  Training Loss: 0.164.. \n",
      "Epoch: 1991/10000..  Training Loss: 0.165.. \n",
      "Epoch: 1992/10000..  Training Loss: 0.166.. \n",
      "Epoch: 1993/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1994/10000..  Training Loss: 0.168.. \n",
      "Epoch: 1995/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1996/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1997/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1998/10000..  Training Loss: 0.167.. \n",
      "Epoch: 1999/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2000/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2001/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2002/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2003/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2004/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2005/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2006/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2007/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2008/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2009/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2010/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2011/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2012/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2013/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2014/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2015/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2016/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2017/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2018/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2019/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2020/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2021/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2022/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2023/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2024/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2025/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2026/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2027/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2028/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2029/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2030/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2031/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2032/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2033/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2034/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2035/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2036/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2037/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2038/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2039/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2040/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2041/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2042/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2043/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2044/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2045/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2046/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2047/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2048/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2049/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2050/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2051/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2052/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2053/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2054/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2055/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2056/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2057/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2058/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2059/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2060/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2061/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2062/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2063/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2064/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2065/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2066/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2067/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2068/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2069/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2070/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2071/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2072/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2073/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2074/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2075/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2076/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2077/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2078/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2079/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2080/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2081/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2082/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2083/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2084/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2085/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2086/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2087/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2088/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2089/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2090/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2091/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2092/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2093/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2094/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2095/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2096/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2097/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2098/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2099/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2100/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2101/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2102/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2103/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2104/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2105/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2106/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2107/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2108/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2109/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2110/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2111/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2112/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2113/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2114/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2115/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2116/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2117/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2118/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2119/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2120/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2121/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2122/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2123/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2124/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2125/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2126/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2127/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2128/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2129/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2130/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2131/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2132/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2133/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2134/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2135/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2136/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2137/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2138/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2139/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2140/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2141/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2142/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2143/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2144/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2145/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2146/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2147/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2148/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2149/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2150/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2151/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2152/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2153/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2154/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2155/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2156/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2157/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2158/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2159/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2160/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2161/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2162/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2163/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2164/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2165/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2166/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2167/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2168/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2169/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2170/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2171/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2172/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2173/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2174/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2175/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2176/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2177/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2178/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2179/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2180/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2181/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2182/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2183/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2184/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2185/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2186/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2187/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2188/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2189/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2190/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2191/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2192/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2193/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2194/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2195/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2196/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2197/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2198/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2199/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2200/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2201/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2202/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2203/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2204/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2205/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2206/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2207/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2208/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2209/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2210/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2211/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2212/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2213/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2214/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2215/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2216/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2217/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2218/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2219/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2220/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2221/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2222/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2223/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2224/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2225/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2226/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2227/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2228/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2229/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2230/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2231/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2232/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2233/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2234/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2235/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2236/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2237/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2238/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2239/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2240/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2241/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2242/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2243/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2244/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2245/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2246/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2247/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2248/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2249/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2250/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2251/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2252/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2253/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2254/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2255/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2256/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2257/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2258/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2259/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2260/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2261/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2262/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2263/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2264/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2265/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2266/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2267/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2268/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2269/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2270/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2271/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2272/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2273/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2274/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2275/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2276/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2277/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2278/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2279/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2280/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2281/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2282/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2283/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2284/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2285/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2286/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2287/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2288/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2289/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2290/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2291/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2292/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2293/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2294/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2295/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2296/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2297/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2298/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2299/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2300/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2301/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2302/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2303/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2304/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2305/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2306/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2307/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2308/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2309/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2310/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2311/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2312/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2313/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2314/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2315/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2316/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2317/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2318/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2319/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2320/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2321/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2322/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2323/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2324/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2325/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2326/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2327/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2328/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2329/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2330/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2331/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2332/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2333/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2334/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2335/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2336/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2337/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2338/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2339/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2340/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2341/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2342/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2343/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2344/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2345/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2346/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2347/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2348/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2349/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2350/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2351/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2352/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2353/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2354/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2355/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2356/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2357/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2358/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2359/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2360/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2361/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2362/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2363/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2364/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2365/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2366/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2367/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2368/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2369/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2370/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2371/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2372/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2373/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2374/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2375/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2376/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2377/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2378/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2379/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2380/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2381/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2382/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2383/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2384/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2385/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2386/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2387/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2388/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2389/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2390/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2391/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2392/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2393/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2394/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2395/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2396/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2397/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2398/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2399/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2400/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2401/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2402/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2403/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2404/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2405/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2406/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2407/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2408/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2409/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2410/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2411/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2412/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2413/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2414/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2415/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2416/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2417/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2418/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2419/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2420/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2421/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2422/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2423/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2424/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2425/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2426/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2427/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2428/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2429/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2430/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2431/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2432/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2433/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2434/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2435/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2436/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2437/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2438/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2439/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2440/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2441/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2442/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2443/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2444/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2445/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2446/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2447/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2448/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2449/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2450/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2451/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2452/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2453/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2454/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2455/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2456/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2457/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2458/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2459/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2460/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2461/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2462/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2463/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2464/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2465/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2466/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2467/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2468/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2469/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2470/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2471/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2472/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2473/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2474/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2475/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2476/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2477/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2478/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2479/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2480/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2481/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2482/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2483/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2484/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2485/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2486/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2487/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2488/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2489/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2490/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2491/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2492/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2493/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2494/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2495/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2496/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2497/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2498/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2499/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2500/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2501/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2502/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2503/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2504/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2505/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2506/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2507/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2508/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2509/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2510/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2511/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2512/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2513/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2514/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2515/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2516/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2517/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2518/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2519/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2520/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2521/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2522/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2523/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2524/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2525/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2526/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2527/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2528/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2529/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2530/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2531/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2532/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2533/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2534/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2535/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2536/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2537/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2538/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2539/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2540/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2541/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2542/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2543/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2544/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2545/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2546/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2547/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2548/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2549/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2550/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2551/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2552/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2553/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2554/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2555/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2556/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2557/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2558/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2559/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2560/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2561/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2562/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2563/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2564/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2565/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2566/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2567/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2568/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2569/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2570/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2571/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2572/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2573/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2574/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2575/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2576/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2577/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2578/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2579/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2580/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2581/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2582/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2583/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2584/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2585/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2586/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2587/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2588/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2589/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2590/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2591/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2592/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2593/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2594/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2595/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2596/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2597/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2598/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2599/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2600/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2601/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2602/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2603/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2604/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2605/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2606/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2607/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2608/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2609/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2610/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2611/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2612/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2613/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2614/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2615/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2616/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2617/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2618/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2619/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2620/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2621/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2622/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2623/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2624/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2625/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2626/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2627/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2628/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2629/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2630/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2631/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2632/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2633/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2634/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2635/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2636/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2637/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2638/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2639/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2640/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2641/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2642/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2643/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2644/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2645/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2646/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2647/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2648/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2649/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2650/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2651/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2652/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2653/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2654/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2655/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2656/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2657/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2658/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2659/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2660/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2661/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2662/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2663/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2664/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2665/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2666/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2667/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2668/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2669/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2670/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2671/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2672/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2673/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2674/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2675/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2676/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2677/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2678/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2679/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2680/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2681/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2682/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2683/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2684/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2685/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2686/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2687/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2688/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2689/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2690/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2691/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2692/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2693/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2694/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2695/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2696/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2697/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2698/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2699/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2700/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2701/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2702/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2703/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2704/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2705/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2706/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2707/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2708/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2709/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2710/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2711/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2712/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2713/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2714/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2715/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2716/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2717/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2718/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2719/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2720/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2721/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2722/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2723/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2724/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2725/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2726/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2727/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2728/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2729/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2730/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2731/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2732/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2733/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2734/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2735/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2736/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2737/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2738/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2739/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2740/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2741/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2742/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2743/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2744/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2745/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2746/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2747/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2748/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2749/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2750/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2751/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2752/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2753/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2754/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2755/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2756/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2757/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2758/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2759/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2760/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2761/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2762/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2763/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2764/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2765/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2766/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2767/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2768/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2769/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2770/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2771/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2772/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2773/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2774/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2775/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2776/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2777/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2778/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2779/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2780/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2781/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2782/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2783/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2784/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2785/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2786/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2787/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2788/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2789/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2790/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2791/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2792/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2793/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2794/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2795/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2796/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2797/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2798/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2799/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2800/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2801/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2802/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2803/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2804/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2805/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2806/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2807/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2808/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2809/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2810/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2811/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2812/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2813/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2814/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2815/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2816/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2817/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2818/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2819/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2820/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2821/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2822/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2823/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2824/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2825/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2826/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2827/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2828/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2829/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2830/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2831/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2832/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2833/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2834/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2835/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2836/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2837/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2838/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2839/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2840/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2841/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2842/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2843/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2844/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2845/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2846/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2847/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2848/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2849/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2850/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2851/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2852/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2853/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2854/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2855/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2856/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2857/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2858/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2859/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2860/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2861/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2862/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2863/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2864/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2865/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2866/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2867/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2868/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2869/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2870/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2871/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2872/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2873/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2874/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2875/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2876/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2877/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2878/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2879/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2880/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2881/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2882/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2883/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2884/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2885/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2886/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2887/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2888/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2889/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2890/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2891/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2892/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2893/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2894/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2895/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2896/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2897/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2898/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2899/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2900/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2901/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2902/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2903/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2904/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2905/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2906/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2907/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2908/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2909/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2910/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2911/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2912/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2913/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2914/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2915/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2916/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2917/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2918/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2919/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2920/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2921/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2922/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2923/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2924/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2925/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2926/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2927/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2928/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2929/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2930/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2931/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2932/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2933/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2934/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2935/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2936/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2937/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2938/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2939/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2940/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2941/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2942/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2943/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2944/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2945/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2946/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2947/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2948/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2949/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2950/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2951/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2952/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2953/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2954/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2955/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2956/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2957/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2958/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2959/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2960/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2961/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2962/10000..  Training Loss: 0.161.. \n",
      "Epoch: 2963/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2964/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2965/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2966/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2967/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2968/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2969/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2970/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2971/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2972/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2973/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2974/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2975/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2976/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2977/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2978/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2979/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2980/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2981/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2982/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2983/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2984/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2985/10000..  Training Loss: 0.159.. \n",
      "Epoch: 2986/10000..  Training Loss: 0.156.. \n",
      "Epoch: 2987/10000..  Training Loss: 0.154.. \n",
      "Epoch: 2988/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2989/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2990/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2991/10000..  Training Loss: 0.152.. \n",
      "Epoch: 2992/10000..  Training Loss: 0.153.. \n",
      "Epoch: 2993/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2994/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2995/10000..  Training Loss: 0.158.. \n",
      "Epoch: 2996/10000..  Training Loss: 0.160.. \n",
      "Epoch: 2997/10000..  Training Loss: 0.157.. \n",
      "Epoch: 2998/10000..  Training Loss: 0.155.. \n",
      "Epoch: 2999/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3000/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3001/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3002/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3003/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3004/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3005/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3006/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3007/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3008/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3009/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3010/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3011/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3012/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3013/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3014/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3015/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3016/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3017/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3018/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3019/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3020/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3021/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3022/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3023/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3024/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3025/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3026/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3027/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3028/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3029/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3030/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3031/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3032/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3033/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3034/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3035/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3036/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3037/10000..  Training Loss: 0.157.. \n",
      "Epoch: 3038/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3039/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3040/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3041/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3042/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3043/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3044/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3045/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3046/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3047/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3048/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3049/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3050/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3051/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3052/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3053/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3054/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3055/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3056/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3057/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3058/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3059/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3060/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3061/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3062/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3063/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3064/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3065/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3066/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3067/10000..  Training Loss: 0.157.. \n",
      "Epoch: 3068/10000..  Training Loss: 0.159.. \n",
      "Epoch: 3069/10000..  Training Loss: 0.162.. \n",
      "Epoch: 3070/10000..  Training Loss: 0.160.. \n",
      "Epoch: 3071/10000..  Training Loss: 0.158.. \n",
      "Epoch: 3072/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3073/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3074/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3075/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3076/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3077/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3078/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3079/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3080/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3081/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3082/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3083/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3084/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3085/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3086/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3087/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3088/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3089/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3090/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3091/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3092/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3093/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3094/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3095/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3096/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3097/10000..  Training Loss: 0.150.. \n",
      "Epoch: 3098/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3099/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3100/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3101/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3102/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3103/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3104/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3105/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3106/10000..  Training Loss: 0.157.. \n",
      "Epoch: 3107/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3108/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3109/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3110/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3111/10000..  Training Loss: 0.150.. \n",
      "Epoch: 3112/10000..  Training Loss: 0.150.. \n",
      "Epoch: 3113/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3114/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3115/10000..  Training Loss: 0.155.. \n",
      "Epoch: 3116/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3117/10000..  Training Loss: 0.157.. \n",
      "Epoch: 3118/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3119/10000..  Training Loss: 0.156.. \n",
      "Epoch: 3120/10000..  Training Loss: 0.154.. \n",
      "Epoch: 3121/10000..  Training Loss: 0.153.. \n",
      "Epoch: 3122/10000..  Training Loss: 0.152.. \n",
      "Epoch: 3123/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3124/10000..  Training Loss: 0.150.. \n",
      "Epoch: 3125/10000..  Training Loss: 0.150.. \n",
      "Epoch: 3126/10000..  Training Loss: 0.151.. \n",
      "Epoch: 3127/10000..  Training Loss: 0.195.. \n",
      "Epoch: 3128/10000..  Training Loss: 0.605.. \n",
      "Epoch: 3129/10000..  Training Loss: 1.149.. \n",
      "Epoch: 3130/10000..  Training Loss: 1.193.. \n",
      "Epoch: 3131/10000..  Training Loss: 0.614.. \n",
      "Epoch: 3132/10000..  Training Loss: 2.371.. \n",
      "Epoch: 3133/10000..  Training Loss: 0.553.. \n",
      "Epoch: 3134/10000..  Training Loss: 1.387.. \n",
      "Epoch: 3135/10000..  Training Loss: 1.795.. \n",
      "Epoch: 3136/10000..  Training Loss: 1.910.. \n",
      "Epoch: 3137/10000..  Training Loss: 1.571.. \n",
      "Epoch: 3138/10000..  Training Loss: 1.012.. \n",
      "Epoch: 3139/10000..  Training Loss: 0.575.. \n",
      "Epoch: 3140/10000..  Training Loss: 0.423.. \n",
      "Epoch: 3141/10000..  Training Loss: 0.459.. \n",
      "Epoch: 3142/10000..  Training Loss: 0.608.. \n",
      "Epoch: 3143/10000..  Training Loss: 0.431.. \n",
      "Epoch: 3144/10000..  Training Loss: 0.380.. \n",
      "Epoch: 3145/10000..  Training Loss: 0.439.. \n",
      "Epoch: 3146/10000..  Training Loss: 0.430.. \n",
      "Epoch: 3147/10000..  Training Loss: 0.420.. \n",
      "Epoch: 3148/10000..  Training Loss: 0.429.. \n",
      "Epoch: 3149/10000..  Training Loss: 0.423.. \n",
      "Epoch: 3150/10000..  Training Loss: 0.399.. \n",
      "Epoch: 3151/10000..  Training Loss: 0.380.. \n",
      "Epoch: 3152/10000..  Training Loss: 0.298.. \n",
      "Epoch: 3153/10000..  Training Loss: 0.268.. \n",
      "Epoch: 3154/10000..  Training Loss: 0.241.. \n",
      "Epoch: 3155/10000..  Training Loss: 0.255.. \n",
      "Epoch: 3156/10000..  Training Loss: 0.252.. \n",
      "Epoch: 3157/10000..  Training Loss: 0.256.. \n",
      "Epoch: 3158/10000..  Training Loss: 0.263.. \n",
      "Epoch: 3159/10000..  Training Loss: 0.266.. \n",
      "Epoch: 3160/10000..  Training Loss: 0.247.. \n",
      "Epoch: 3161/10000..  Training Loss: 0.200.. \n",
      "Epoch: 3162/10000..  Training Loss: 0.204.. \n",
      "Epoch: 3163/10000..  Training Loss: 0.207.. \n",
      "Epoch: 3164/10000..  Training Loss: 0.204.. \n",
      "Epoch: 3165/10000..  Training Loss: 0.204.. \n",
      "Epoch: 3166/10000..  Training Loss: 0.197.. \n",
      "Epoch: 3167/10000..  Training Loss: 0.208.. \n",
      "Epoch: 3168/10000..  Training Loss: 0.204.. \n",
      "Epoch: 3169/10000..  Training Loss: 0.196.. \n",
      "Epoch: 3170/10000..  Training Loss: 0.193.. \n",
      "Epoch: 3171/10000..  Training Loss: 0.190.. \n",
      "Epoch: 3172/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3173/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3174/10000..  Training Loss: 0.190.. \n",
      "Epoch: 3175/10000..  Training Loss: 0.188.. \n",
      "Epoch: 3176/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3177/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3178/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3179/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3180/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3181/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3182/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3183/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3184/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3185/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3186/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3187/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3188/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3189/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3190/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3191/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3192/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3193/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3194/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3195/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3196/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3197/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3198/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3199/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3200/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3201/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3202/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3203/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3204/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3205/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3206/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3207/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3208/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3209/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3210/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3211/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3212/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3213/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3214/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3215/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3216/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3217/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3218/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3219/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3220/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3221/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3222/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3223/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3224/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3225/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3226/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3227/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3228/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3229/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3230/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3231/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3232/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3233/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3234/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3235/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3236/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3237/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3238/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3239/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3240/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3241/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3242/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3243/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3244/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3245/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3246/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3247/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3248/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3249/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3250/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3251/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3252/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3253/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3254/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3255/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3256/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3257/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3258/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3259/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3260/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3261/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3262/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3263/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3264/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3265/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3266/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3267/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3268/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3269/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3270/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3271/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3272/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3273/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3274/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3275/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3276/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3277/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3278/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3279/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3280/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3281/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3282/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3283/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3284/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3285/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3286/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3287/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3288/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3289/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3290/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3291/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3292/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3293/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3294/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3295/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3296/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3297/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3298/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3299/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3300/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3301/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3302/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3303/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3304/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3305/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3306/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3307/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3308/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3309/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3310/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3311/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3312/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3313/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3314/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3315/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3316/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3317/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3318/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3319/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3320/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3321/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3322/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3323/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3324/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3325/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3326/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3327/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3328/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3329/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3330/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3331/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3332/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3333/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3334/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3335/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3336/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3337/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3338/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3339/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3340/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3341/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3342/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3343/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3344/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3345/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3346/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3347/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3348/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3349/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3350/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3351/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3352/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3353/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3354/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3355/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3356/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3357/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3358/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3359/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3360/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3361/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3362/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3363/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3364/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3365/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3366/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3367/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3368/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3369/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3370/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3371/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3372/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3373/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3374/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3375/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3376/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3377/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3378/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3379/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3380/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3381/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3382/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3383/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3384/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3385/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3386/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3387/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3388/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3389/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3390/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3391/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3392/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3393/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3394/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3395/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3396/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3397/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3398/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3399/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3400/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3401/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3402/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3403/10000..  Training Loss: 0.169.. \n",
      "Epoch: 3404/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3405/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3406/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3407/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3408/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3409/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3410/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3411/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3412/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3413/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3414/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3415/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3416/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3417/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3418/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3419/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3420/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3421/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3422/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3423/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3424/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3425/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3426/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3427/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3428/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3429/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3430/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3431/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3432/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3433/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3434/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3435/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3436/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3437/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3438/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3439/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3440/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3441/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3442/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3443/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3444/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3445/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3446/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3447/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3448/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3449/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3450/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3451/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3452/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3453/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3454/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3455/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3456/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3457/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3458/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3459/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3460/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3461/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3462/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3463/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3464/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3465/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3466/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3467/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3468/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3469/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3470/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3471/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3472/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3473/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3474/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3475/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3476/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3477/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3478/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3479/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3480/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3481/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3482/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3483/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3484/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3485/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3486/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3487/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3488/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3489/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3490/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3491/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3492/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3493/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3494/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3495/10000..  Training Loss: 0.168.. \n",
      "Epoch: 3496/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3497/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3498/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3499/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3500/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3501/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3502/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3503/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3504/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3505/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3506/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3507/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3508/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3509/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3510/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3511/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3512/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3513/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3514/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3515/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3516/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3517/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3518/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3519/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3520/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3521/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3522/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3523/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3524/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3525/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3526/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3527/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3528/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3529/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3530/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3531/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3532/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3533/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3534/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3535/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3536/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3537/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3538/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3539/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3540/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3541/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3542/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3543/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3544/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3545/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3546/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3547/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3548/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3549/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3550/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3551/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3552/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3553/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3554/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3555/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3556/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3557/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3558/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3559/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3560/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3561/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3562/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3563/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3564/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3565/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3566/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3567/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3568/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3569/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3570/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3571/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3572/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3573/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3574/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3575/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3576/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3577/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3578/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3579/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3580/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3581/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3582/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3583/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3584/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3585/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3586/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3587/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3588/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3589/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3590/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3591/10000..  Training Loss: 0.167.. \n",
      "Epoch: 3592/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3593/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3594/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3595/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3596/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3597/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3598/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3599/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3600/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3601/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3602/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3603/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3604/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3605/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3606/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3607/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3608/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3609/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3610/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3611/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3612/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3613/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3614/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3615/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3616/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3617/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3618/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3619/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3620/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3621/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3622/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3623/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3624/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3625/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3626/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3627/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3628/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3629/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3630/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3631/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3632/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3633/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3634/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3635/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3636/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3637/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3638/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3639/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3640/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3641/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3642/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3643/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3644/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3645/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3646/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3647/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3648/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3649/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3650/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3651/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3652/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3653/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3654/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3655/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3656/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3657/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3658/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3659/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3660/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3661/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3662/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3663/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3664/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3665/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3666/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3667/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3668/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3669/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3670/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3671/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3672/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3673/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3674/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3675/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3676/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3677/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3678/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3679/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3680/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3681/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3682/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3683/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3684/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3685/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3686/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3687/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3688/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3689/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3690/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3691/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3692/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3693/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3694/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3695/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3696/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3697/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3698/10000..  Training Loss: 0.166.. \n",
      "Epoch: 3699/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3700/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3701/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3702/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3703/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3704/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3705/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3706/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3707/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3708/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3709/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3710/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3711/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3712/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3713/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3714/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3715/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3716/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3717/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3718/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3719/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3720/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3721/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3722/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3723/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3724/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3725/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3726/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3727/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3728/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3729/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3730/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3731/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3732/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3733/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3734/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3735/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3736/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3737/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3738/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3739/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3740/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3741/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3742/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3743/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3744/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3745/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3746/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3747/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3748/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3749/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3750/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3751/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3752/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3753/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3754/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3755/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3756/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3757/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3758/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3759/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3760/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3761/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3762/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3763/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3764/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3765/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3766/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3767/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3768/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3769/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3770/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3771/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3772/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3773/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3774/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3775/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3776/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3777/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3778/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3779/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3780/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3781/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3782/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3783/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3784/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3785/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3786/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3787/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3788/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3789/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3790/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3791/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3792/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3793/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3794/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3795/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3796/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3797/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3798/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3799/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3800/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3801/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3802/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3803/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3804/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3805/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3806/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3807/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3808/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3809/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3810/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3811/10000..  Training Loss: 0.165.. \n",
      "Epoch: 3812/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3813/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3814/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3815/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3816/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3817/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3818/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3819/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3820/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3821/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3822/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3823/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3824/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3825/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3826/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3827/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3828/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3829/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3830/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3831/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3832/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3833/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3834/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3835/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3836/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3837/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3838/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3839/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3840/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3841/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3842/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3843/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3844/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3845/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3846/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3847/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3848/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3849/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3850/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3851/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3852/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3853/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3854/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3855/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3856/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3857/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3858/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3859/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3860/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3861/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3862/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3863/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3864/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3865/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3866/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3867/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3868/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3869/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3870/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3871/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3872/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3873/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3874/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3875/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3876/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3877/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3878/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3879/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3880/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3881/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3882/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3883/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3884/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3885/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3886/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3887/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3888/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3889/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3890/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3891/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3892/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3893/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3894/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3895/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3896/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3897/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3898/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3899/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3900/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3901/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3902/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3903/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3904/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3905/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3906/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3907/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3908/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3909/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3910/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3911/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3912/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3913/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3914/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3915/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3916/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3917/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3918/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3919/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3920/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3921/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3922/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3923/10000..  Training Loss: 0.164.. \n",
      "Epoch: 3924/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3925/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3926/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3927/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3928/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3929/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3930/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3931/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3932/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3933/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3934/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3935/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3936/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3937/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3938/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3939/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3940/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3941/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3942/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3943/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3944/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3945/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3946/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3947/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3948/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3949/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3950/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3951/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3952/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3953/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3954/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3955/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3956/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3957/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3958/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3959/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3960/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3961/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3962/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3963/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3964/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3965/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3966/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3967/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3968/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3969/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3970/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3971/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3972/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3973/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3974/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3975/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3976/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3977/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3978/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3979/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3980/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3981/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3982/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3983/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3984/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3985/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3986/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3987/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3988/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3989/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3990/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3991/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3992/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3993/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3994/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3995/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3996/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3997/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3998/10000..  Training Loss: 0.163.. \n",
      "Epoch: 3999/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4000/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4001/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4002/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4003/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4004/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4005/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4006/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4007/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4008/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4009/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4010/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4011/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4012/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4013/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4014/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4015/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4016/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4017/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4018/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4019/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4020/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4021/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4022/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4023/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4024/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4025/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4026/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4027/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4028/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4029/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4030/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4031/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4032/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4033/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4034/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4035/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4036/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4037/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4038/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4039/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4040/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4041/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4042/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4043/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4044/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4045/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4046/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4047/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4048/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4049/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4050/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4051/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4052/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4053/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4054/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4055/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4056/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4057/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4058/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4059/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4060/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4061/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4062/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4063/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4064/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4065/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4066/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4067/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4068/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4069/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4070/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4071/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4072/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4073/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4074/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4075/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4076/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4077/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4078/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4079/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4080/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4081/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4082/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4083/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4084/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4085/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4086/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4087/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4088/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4089/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4090/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4091/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4092/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4093/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4094/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4095/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4096/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4097/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4098/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4099/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4100/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4101/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4102/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4103/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4104/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4105/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4106/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4107/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4108/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4109/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4110/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4111/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4112/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4113/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4114/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4115/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4116/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4117/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4118/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4119/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4120/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4121/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4122/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4123/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4124/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4125/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4126/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4127/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4128/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4129/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4130/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4131/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4132/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4133/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4134/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4135/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4136/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4137/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4138/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4139/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4140/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4141/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4142/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4143/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4144/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4145/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4146/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4147/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4148/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4149/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4150/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4151/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4152/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4153/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4154/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4155/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4156/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4157/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4158/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4159/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4160/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4161/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4162/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4163/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4164/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4165/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4166/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4167/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4168/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4169/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4170/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4171/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4172/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4173/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4174/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4175/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4176/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4177/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4178/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4179/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4180/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4181/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4182/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4183/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4184/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4185/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4186/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4187/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4188/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4189/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4190/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4191/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4192/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4193/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4194/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4195/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4196/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4197/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4198/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4199/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4200/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4201/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4202/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4203/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4204/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4205/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4206/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4207/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4208/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4209/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4210/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4211/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4212/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4213/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4214/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4215/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4216/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4217/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4218/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4219/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4220/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4221/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4222/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4223/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4224/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4225/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4226/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4227/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4228/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4229/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4230/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4231/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4232/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4233/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4234/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4235/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4236/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4237/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4238/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4239/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4240/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4241/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4242/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4243/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4244/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4245/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4246/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4247/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4248/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4249/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4250/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4251/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4252/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4253/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4254/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4255/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4256/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4257/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4258/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4259/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4260/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4261/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4262/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4263/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4264/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4265/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4266/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4267/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4268/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4269/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4270/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4271/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4272/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4273/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4274/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4275/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4276/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4277/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4278/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4279/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4280/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4281/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4282/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4283/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4284/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4285/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4286/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4287/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4288/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4289/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4290/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4291/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4292/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4293/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4294/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4295/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4296/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4297/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4298/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4299/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4300/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4301/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4302/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4303/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4304/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4305/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4306/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4307/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4308/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4309/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4310/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4311/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4312/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4313/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4314/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4315/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4316/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4317/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4318/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4319/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4320/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4321/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4322/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4323/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4324/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4325/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4326/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4327/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4328/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4329/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4330/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4331/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4332/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4333/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4334/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4335/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4336/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4337/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4338/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4339/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4340/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4341/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4342/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4343/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4344/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4345/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4346/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4347/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4348/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4349/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4350/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4351/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4352/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4353/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4354/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4355/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4356/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4357/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4358/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4359/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4360/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4361/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4362/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4363/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4364/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4365/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4366/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4367/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4368/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4369/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4370/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4371/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4372/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4373/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4374/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4375/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4376/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4377/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4378/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4379/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4380/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4381/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4382/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4383/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4384/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4385/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4386/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4387/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4388/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4389/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4390/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4391/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4392/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4393/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4394/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4395/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4396/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4397/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4398/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4399/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4400/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4401/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4402/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4403/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4404/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4405/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4406/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4407/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4408/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4409/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4410/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4411/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4412/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4413/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4414/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4415/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4416/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4417/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4418/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4419/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4420/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4421/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4422/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4423/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4424/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4425/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4426/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4427/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4428/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4429/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4430/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4431/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4432/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4433/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4434/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4435/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4436/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4437/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4438/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4439/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4440/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4441/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4442/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4443/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4444/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4445/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4446/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4447/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4448/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4449/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4450/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4451/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4452/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4453/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4454/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4455/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4456/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4457/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4458/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4459/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4460/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4461/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4462/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4463/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4464/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4465/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4466/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4467/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4468/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4469/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4470/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4471/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4472/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4473/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4474/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4475/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4476/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4477/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4478/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4479/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4480/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4481/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4482/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4483/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4484/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4485/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4486/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4487/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4488/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4489/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4490/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4491/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4492/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4493/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4494/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4495/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4496/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4497/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4498/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4499/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4500/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4501/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4502/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4503/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4504/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4505/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4506/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4507/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4508/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4509/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4510/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4511/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4512/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4513/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4514/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4515/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4516/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4517/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4518/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4519/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4520/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4521/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4522/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4523/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4524/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4525/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4526/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4527/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4528/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4529/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4530/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4531/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4532/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4533/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4534/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4535/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4536/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4537/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4538/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4539/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4540/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4541/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4542/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4543/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4544/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4545/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4546/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4547/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4548/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4549/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4550/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4551/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4552/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4553/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4554/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4555/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4556/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4557/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4558/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4559/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4560/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4561/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4562/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4563/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4564/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4565/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4566/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4567/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4568/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4569/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4570/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4571/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4572/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4573/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4574/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4575/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4576/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4577/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4578/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4579/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4580/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4581/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4582/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4583/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4584/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4585/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4586/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4587/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4588/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4589/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4590/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4591/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4592/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4593/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4594/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4595/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4596/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4597/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4598/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4599/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4600/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4601/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4602/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4603/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4604/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4605/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4606/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4607/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4608/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4609/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4610/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4611/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4612/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4613/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4614/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4615/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4616/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4617/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4618/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4619/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4620/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4621/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4622/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4623/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4624/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4625/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4626/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4627/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4628/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4629/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4630/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4631/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4632/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4633/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4634/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4635/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4636/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4637/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4638/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4639/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4640/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4641/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4642/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4643/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4644/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4645/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4646/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4647/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4648/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4649/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4650/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4651/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4652/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4653/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4654/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4655/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4656/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4657/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4658/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4659/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4660/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4661/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4662/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4663/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4664/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4665/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4666/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4667/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4668/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4669/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4670/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4671/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4672/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4673/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4674/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4675/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4676/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4677/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4678/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4679/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4680/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4681/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4682/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4683/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4684/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4685/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4686/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4687/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4688/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4689/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4690/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4691/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4692/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4693/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4694/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4695/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4696/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4697/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4698/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4699/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4700/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4701/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4702/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4703/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4704/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4705/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4706/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4707/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4708/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4709/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4710/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4711/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4712/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4713/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4714/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4715/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4716/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4717/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4718/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4719/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4720/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4721/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4722/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4723/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4724/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4725/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4726/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4727/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4728/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4729/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4730/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4731/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4732/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4733/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4734/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4735/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4736/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4737/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4738/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4739/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4740/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4741/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4742/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4743/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4744/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4745/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4746/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4747/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4748/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4749/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4750/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4751/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4752/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4753/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4754/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4755/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4756/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4757/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4758/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4759/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4760/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4761/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4762/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4763/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4764/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4765/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4766/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4767/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4768/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4769/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4770/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4771/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4772/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4773/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4774/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4775/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4776/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4777/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4778/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4779/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4780/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4781/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4782/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4783/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4784/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4785/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4786/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4787/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4788/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4789/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4790/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4791/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4792/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4793/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4794/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4795/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4796/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4797/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4798/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4799/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4800/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4801/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4802/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4803/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4804/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4805/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4806/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4807/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4808/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4809/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4810/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4811/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4812/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4813/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4814/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4815/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4816/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4817/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4818/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4819/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4820/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4821/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4822/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4823/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4824/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4825/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4826/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4827/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4828/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4829/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4830/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4831/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4832/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4833/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4834/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4835/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4836/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4837/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4838/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4839/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4840/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4841/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4842/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4843/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4844/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4845/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4846/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4847/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4848/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4849/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4850/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4851/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4852/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4853/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4854/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4855/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4856/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4857/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4858/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4859/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4860/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4861/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4862/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4863/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4864/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4865/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4866/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4867/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4868/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4869/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4870/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4871/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4872/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4873/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4874/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4875/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4876/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4877/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4878/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4879/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4880/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4881/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4882/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4883/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4884/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4885/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4886/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4887/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4888/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4889/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4890/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4891/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4892/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4893/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4894/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4895/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4896/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4897/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4898/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4899/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4900/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4901/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4902/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4903/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4904/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4905/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4906/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4907/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4908/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4909/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4910/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4911/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4912/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4913/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4914/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4915/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4916/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4917/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4918/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4919/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4920/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4921/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4922/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4923/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4924/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4925/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4926/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4927/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4928/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4929/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4930/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4931/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4932/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4933/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4934/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4935/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4936/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4937/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4938/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4939/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4940/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4941/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4942/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4943/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4944/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4945/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4946/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4947/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4948/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4949/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4950/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4951/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4952/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4953/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4954/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4955/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4956/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4957/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4958/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4959/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4960/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4961/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4962/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4963/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4964/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4965/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4966/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4967/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4968/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4969/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4970/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4971/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4972/10000..  Training Loss: 0.158.. \n",
      "Epoch: 4973/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4974/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4975/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4976/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4977/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4978/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4979/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4980/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4981/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4982/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4983/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4984/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4985/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4986/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4987/10000..  Training Loss: 0.157.. \n",
      "Epoch: 4988/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4989/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4990/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4991/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4992/10000..  Training Loss: 0.154.. \n",
      "Epoch: 4993/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4994/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4995/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4996/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4997/10000..  Training Loss: 0.155.. \n",
      "Epoch: 4998/10000..  Training Loss: 0.156.. \n",
      "Epoch: 4999/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5000/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5001/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5002/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5003/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5004/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5005/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5006/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5007/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5008/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5009/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5010/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5011/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5012/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5013/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5014/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5015/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5016/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5017/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5018/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5019/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5020/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5021/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5022/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5023/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5024/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5025/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5026/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5027/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5028/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5029/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5030/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5031/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5032/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5033/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5034/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5035/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5036/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5037/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5038/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5039/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5040/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5041/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5042/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5043/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5044/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5045/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5046/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5047/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5048/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5049/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5050/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5051/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5052/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5053/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5054/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5055/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5056/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5057/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5058/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5059/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5060/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5061/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5062/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5063/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5064/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5065/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5066/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5067/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5068/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5069/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5070/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5071/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5072/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5073/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5074/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5075/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5076/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5077/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5078/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5079/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5080/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5081/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5082/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5083/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5084/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5085/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5086/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5087/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5088/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5089/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5090/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5091/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5092/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5093/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5094/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5095/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5096/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5097/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5098/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5099/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5100/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5101/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5102/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5103/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5104/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5105/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5106/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5107/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5108/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5109/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5110/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5111/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5112/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5113/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5114/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5115/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5116/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5117/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5118/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5119/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5120/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5121/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5122/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5123/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5124/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5125/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5126/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5127/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5128/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5129/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5130/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5131/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5132/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5133/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5134/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5135/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5136/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5137/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5138/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5139/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5140/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5141/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5142/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5143/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5144/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5145/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5146/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5147/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5148/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5149/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5150/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5151/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5152/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5153/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5154/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5155/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5156/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5157/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5158/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5159/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5160/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5161/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5162/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5163/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5164/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5165/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5166/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5167/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5168/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5169/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5170/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5171/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5172/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5173/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5174/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5175/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5176/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5177/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5178/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5179/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5180/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5181/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5182/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5183/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5184/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5185/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5186/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5187/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5188/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5189/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5190/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5191/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5192/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5193/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5194/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5195/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5196/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5197/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5198/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5199/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5200/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5201/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5202/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5203/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5204/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5205/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5206/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5207/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5208/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5209/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5210/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5211/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5212/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5213/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5214/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5215/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5216/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5217/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5218/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5219/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5220/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5221/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5222/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5223/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5224/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5225/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5226/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5227/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5228/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5229/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5230/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5231/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5232/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5233/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5234/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5235/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5236/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5237/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5238/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5239/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5240/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5241/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5242/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5243/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5244/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5245/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5246/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5247/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5248/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5249/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5250/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5251/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5252/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5253/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5254/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5255/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5256/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5257/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5258/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5259/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5260/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5261/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5262/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5263/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5264/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5265/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5266/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5267/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5268/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5269/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5270/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5271/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5272/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5273/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5274/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5275/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5276/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5277/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5278/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5279/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5280/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5281/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5282/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5283/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5284/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5285/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5286/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5287/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5288/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5289/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5290/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5291/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5292/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5293/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5294/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5295/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5296/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5297/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5298/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5299/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5300/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5301/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5302/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5303/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5304/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5305/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5306/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5307/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5308/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5309/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5310/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5311/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5312/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5313/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5314/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5315/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5316/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5317/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5318/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5319/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5320/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5321/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5322/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5323/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5324/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5325/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5326/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5327/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5328/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5329/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5330/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5331/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5332/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5333/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5334/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5335/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5336/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5337/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5338/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5339/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5340/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5341/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5342/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5343/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5344/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5345/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5346/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5347/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5348/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5349/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5350/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5351/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5352/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5353/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5354/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5355/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5356/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5357/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5358/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5359/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5360/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5361/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5362/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5363/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5364/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5365/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5366/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5367/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5368/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5369/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5370/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5371/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5372/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5373/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5374/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5375/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5376/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5377/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5378/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5379/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5380/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5381/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5382/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5383/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5384/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5385/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5386/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5387/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5388/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5389/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5390/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5391/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5392/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5393/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5394/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5395/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5396/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5397/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5398/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5399/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5400/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5401/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5402/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5403/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5404/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5405/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5406/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5407/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5408/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5409/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5410/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5411/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5412/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5413/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5414/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5415/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5416/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5417/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5418/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5419/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5420/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5421/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5422/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5423/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5424/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5425/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5426/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5427/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5428/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5429/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5430/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5431/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5432/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5433/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5434/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5435/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5436/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5437/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5438/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5439/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5440/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5441/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5442/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5443/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5444/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5445/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5446/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5447/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5448/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5449/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5450/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5451/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5452/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5453/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5454/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5455/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5456/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5457/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5458/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5459/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5460/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5461/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5462/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5463/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5464/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5465/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5466/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5467/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5468/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5469/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5470/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5471/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5472/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5473/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5474/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5475/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5476/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5477/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5478/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5479/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5480/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5481/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5482/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5483/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5484/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5485/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5486/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5487/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5488/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5489/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5490/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5491/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5492/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5493/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5494/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5495/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5496/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5497/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5498/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5499/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5500/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5501/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5502/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5503/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5504/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5505/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5506/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5507/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5508/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5509/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5510/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5511/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5512/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5513/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5514/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5515/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5516/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5517/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5518/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5519/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5520/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5521/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5522/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5523/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5524/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5525/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5526/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5527/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5528/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5529/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5530/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5531/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5532/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5533/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5534/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5535/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5536/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5537/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5538/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5539/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5540/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5541/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5542/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5543/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5544/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5545/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5546/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5547/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5548/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5549/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5550/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5551/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5552/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5553/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5554/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5555/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5556/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5557/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5558/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5559/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5560/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5561/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5562/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5563/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5564/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5565/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5566/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5567/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5568/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5569/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5570/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5571/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5572/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5573/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5574/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5575/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5576/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5577/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5578/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5579/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5580/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5581/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5582/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5583/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5584/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5585/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5586/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5587/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5588/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5589/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5590/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5591/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5592/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5593/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5594/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5595/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5596/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5597/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5598/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5599/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5600/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5601/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5602/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5603/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5604/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5605/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5606/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5607/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5608/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5609/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5610/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5611/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5612/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5613/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5614/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5615/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5616/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5617/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5618/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5619/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5620/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5621/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5622/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5623/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5624/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5625/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5626/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5627/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5628/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5629/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5630/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5631/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5632/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5633/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5634/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5635/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5636/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5637/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5638/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5639/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5640/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5641/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5642/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5643/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5644/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5645/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5646/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5647/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5648/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5649/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5650/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5651/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5652/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5653/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5654/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5655/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5656/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5657/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5658/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5659/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5660/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5661/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5662/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5663/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5664/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5665/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5666/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5667/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5668/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5669/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5670/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5671/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5672/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5673/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5674/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5675/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5676/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5677/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5678/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5679/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5680/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5681/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5682/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5683/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5684/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5685/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5686/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5687/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5688/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5689/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5690/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5691/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5692/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5693/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5694/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5695/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5696/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5697/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5698/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5699/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5700/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5701/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5702/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5703/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5704/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5705/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5706/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5707/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5708/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5709/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5710/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5711/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5712/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5713/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5714/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5715/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5716/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5717/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5718/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5719/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5720/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5721/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5722/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5723/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5724/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5725/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5726/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5727/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5728/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5729/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5730/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5731/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5732/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5733/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5734/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5735/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5736/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5737/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5738/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5739/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5740/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5741/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5742/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5743/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5744/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5745/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5746/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5747/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5748/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5749/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5750/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5751/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5752/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5753/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5754/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5755/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5756/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5757/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5758/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5759/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5760/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5761/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5762/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5763/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5764/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5765/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5766/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5767/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5768/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5769/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5770/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5771/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5772/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5773/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5774/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5775/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5776/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5777/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5778/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5779/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5780/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5781/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5782/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5783/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5784/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5785/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5786/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5787/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5788/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5789/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5790/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5791/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5792/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5793/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5794/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5795/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5796/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5797/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5798/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5799/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5800/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5801/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5802/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5803/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5804/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5805/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5806/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5807/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5808/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5809/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5810/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5811/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5812/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5813/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5814/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5815/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5816/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5817/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5818/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5819/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5820/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5821/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5822/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5823/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5824/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5825/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5826/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5827/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5828/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5829/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5830/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5831/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5832/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5833/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5834/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5835/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5836/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5837/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5838/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5839/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5840/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5841/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5842/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5843/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5844/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5845/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5846/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5847/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5848/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5849/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5850/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5851/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5852/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5853/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5854/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5855/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5856/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5857/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5858/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5859/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5860/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5861/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5862/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5863/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5864/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5865/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5866/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5867/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5868/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5869/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5870/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5871/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5872/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5873/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5874/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5875/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5876/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5877/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5878/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5879/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5880/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5881/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5882/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5883/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5884/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5885/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5886/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5887/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5888/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5889/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5890/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5891/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5892/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5893/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5894/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5895/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5896/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5897/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5898/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5899/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5900/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5901/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5902/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5903/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5904/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5905/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5906/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5907/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5908/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5909/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5910/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5911/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5912/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5913/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5914/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5915/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5916/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5917/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5918/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5919/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5920/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5921/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5922/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5923/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5924/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5925/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5926/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5927/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5928/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5929/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5930/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5931/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5932/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5933/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5934/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5935/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5936/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5937/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5938/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5939/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5940/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5941/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5942/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5943/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5944/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5945/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5946/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5947/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5948/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5949/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5950/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5951/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5952/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5953/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5954/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5955/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5956/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5957/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5958/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5959/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5960/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5961/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5962/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5963/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5964/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5965/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5966/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5967/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5968/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5969/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5970/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5971/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5972/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5973/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5974/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5975/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5976/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5977/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5978/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5979/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5980/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5981/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5982/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5983/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5984/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5985/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5986/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5987/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5988/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5989/10000..  Training Loss: 0.147.. \n",
      "Epoch: 5990/10000..  Training Loss: 0.147.. \n",
      "Epoch: 5991/10000..  Training Loss: 0.147.. \n",
      "Epoch: 5992/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5993/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5994/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5995/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5996/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5997/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5998/10000..  Training Loss: 0.148.. \n",
      "Epoch: 5999/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6000/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6001/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6002/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6003/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6004/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6005/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6006/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6007/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6008/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6009/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6010/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6011/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6012/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6013/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6014/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6015/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6016/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6017/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6018/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6019/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6020/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6021/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6022/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6023/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6024/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6025/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6026/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6027/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6028/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6029/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6030/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6031/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6032/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6033/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6034/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6035/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6036/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6037/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6038/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6039/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6040/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6041/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6042/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6043/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6044/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6045/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6046/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6047/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6048/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6049/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6050/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6051/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6052/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6053/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6054/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6055/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6056/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6057/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6058/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6059/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6060/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6061/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6062/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6063/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6064/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6065/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6066/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6067/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6068/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6069/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6070/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6071/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6072/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6073/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6074/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6075/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6076/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6077/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6078/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6079/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6080/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6081/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6082/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6083/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6084/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6085/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6086/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6087/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6088/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6089/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6090/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6091/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6092/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6093/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6094/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6095/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6096/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6097/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6098/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6099/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6100/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6101/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6102/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6103/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6104/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6105/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6106/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6107/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6108/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6109/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6110/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6111/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6112/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6113/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6114/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6115/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6116/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6117/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6118/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6119/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6120/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6121/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6122/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6123/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6124/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6125/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6126/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6127/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6128/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6129/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6130/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6131/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6132/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6133/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6134/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6135/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6136/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6137/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6138/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6139/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6140/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6141/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6142/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6143/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6144/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6145/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6146/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6147/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6148/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6149/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6150/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6151/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6152/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6153/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6154/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6155/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6156/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6157/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6158/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6159/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6160/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6161/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6162/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6163/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6164/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6165/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6166/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6167/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6168/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6169/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6170/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6171/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6172/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6173/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6174/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6175/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6176/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6177/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6178/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6179/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6180/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6181/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6182/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6183/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6184/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6185/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6186/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6187/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6188/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6189/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6190/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6191/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6192/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6193/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6194/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6195/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6196/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6197/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6198/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6199/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6200/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6201/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6202/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6203/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6204/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6205/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6206/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6207/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6208/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6209/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6210/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6211/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6212/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6213/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6214/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6215/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6216/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6217/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6218/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6219/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6220/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6221/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6222/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6223/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6224/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6225/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6226/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6227/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6228/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6229/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6230/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6231/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6232/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6233/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6234/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6235/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6236/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6237/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6238/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6239/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6240/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6241/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6242/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6243/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6244/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6245/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6246/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6247/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6248/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6249/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6250/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6251/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6252/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6253/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6254/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6255/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6256/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6257/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6258/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6259/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6260/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6261/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6262/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6263/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6264/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6265/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6266/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6267/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6268/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6269/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6270/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6271/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6272/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6273/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6274/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6275/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6276/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6277/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6278/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6279/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6280/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6281/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6282/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6283/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6284/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6285/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6286/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6287/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6288/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6289/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6290/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6291/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6292/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6293/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6294/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6295/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6296/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6297/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6298/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6299/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6300/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6301/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6302/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6303/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6304/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6305/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6306/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6307/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6308/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6309/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6310/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6311/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6312/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6313/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6314/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6315/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6316/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6317/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6318/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6319/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6320/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6321/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6322/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6323/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6324/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6325/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6326/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6327/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6328/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6329/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6330/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6331/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6332/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6333/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6334/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6335/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6336/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6337/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6338/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6339/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6340/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6341/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6342/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6343/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6344/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6345/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6346/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6347/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6348/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6349/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6350/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6351/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6352/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6353/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6354/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6355/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6356/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6357/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6358/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6359/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6360/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6361/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6362/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6363/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6364/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6365/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6366/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6367/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6368/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6369/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6370/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6371/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6372/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6373/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6374/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6375/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6376/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6377/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6378/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6379/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6380/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6381/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6382/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6383/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6384/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6385/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6386/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6387/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6388/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6389/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6390/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6391/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6392/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6393/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6394/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6395/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6396/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6397/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6398/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6399/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6400/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6401/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6402/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6403/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6404/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6405/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6406/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6407/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6408/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6409/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6410/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6411/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6412/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6413/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6414/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6415/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6416/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6417/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6418/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6419/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6420/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6421/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6422/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6423/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6424/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6425/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6426/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6427/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6428/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6429/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6430/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6431/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6432/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6433/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6434/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6435/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6436/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6437/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6438/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6439/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6440/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6441/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6442/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6443/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6444/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6445/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6446/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6447/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6448/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6449/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6450/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6451/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6452/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6453/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6454/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6455/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6456/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6457/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6458/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6459/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6460/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6461/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6462/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6463/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6464/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6465/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6466/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6467/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6468/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6469/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6470/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6471/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6472/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6473/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6474/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6475/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6476/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6477/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6478/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6479/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6480/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6481/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6482/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6483/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6484/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6485/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6486/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6487/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6488/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6489/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6490/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6491/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6492/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6493/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6494/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6495/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6496/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6497/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6498/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6499/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6500/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6501/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6502/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6503/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6504/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6505/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6506/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6507/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6508/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6509/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6510/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6511/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6512/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6513/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6514/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6515/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6516/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6517/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6518/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6519/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6520/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6521/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6522/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6523/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6524/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6525/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6526/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6527/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6528/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6529/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6530/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6531/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6532/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6533/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6534/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6535/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6536/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6537/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6538/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6539/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6540/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6541/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6542/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6543/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6544/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6545/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6546/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6547/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6548/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6549/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6550/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6551/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6552/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6553/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6554/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6555/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6556/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6557/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6558/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6559/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6560/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6561/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6562/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6563/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6564/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6565/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6566/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6567/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6568/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6569/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6570/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6571/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6572/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6573/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6574/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6575/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6576/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6577/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6578/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6579/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6580/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6581/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6582/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6583/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6584/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6585/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6586/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6587/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6588/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6589/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6590/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6591/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6592/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6593/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6594/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6595/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6596/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6597/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6598/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6599/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6600/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6601/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6602/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6603/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6604/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6605/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6606/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6607/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6608/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6609/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6610/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6611/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6612/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6613/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6614/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6615/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6616/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6617/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6618/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6619/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6620/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6621/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6622/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6623/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6624/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6625/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6626/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6627/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6628/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6629/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6630/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6631/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6632/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6633/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6634/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6635/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6636/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6637/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6638/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6639/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6640/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6641/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6642/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6643/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6644/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6645/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6646/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6647/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6648/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6649/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6650/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6651/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6652/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6653/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6654/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6655/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6656/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6657/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6658/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6659/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6660/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6661/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6662/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6663/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6664/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6665/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6666/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6667/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6668/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6669/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6670/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6671/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6672/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6673/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6674/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6675/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6676/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6677/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6678/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6679/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6680/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6681/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6682/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6683/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6684/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6685/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6686/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6687/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6688/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6689/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6690/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6691/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6692/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6693/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6694/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6695/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6696/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6697/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6698/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6699/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6700/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6701/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6702/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6703/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6704/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6705/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6706/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6707/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6708/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6709/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6710/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6711/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6712/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6713/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6714/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6715/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6716/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6717/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6718/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6719/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6720/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6721/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6722/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6723/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6724/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6725/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6726/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6727/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6728/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6729/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6730/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6731/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6732/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6733/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6734/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6735/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6736/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6737/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6738/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6739/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6740/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6741/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6742/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6743/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6744/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6745/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6746/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6747/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6748/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6749/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6750/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6751/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6752/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6753/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6754/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6755/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6756/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6757/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6758/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6759/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6760/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6761/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6762/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6763/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6764/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6765/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6766/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6767/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6768/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6769/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6770/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6771/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6772/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6773/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6774/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6775/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6776/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6777/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6778/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6779/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6780/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6781/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6782/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6783/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6784/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6785/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6786/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6787/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6788/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6789/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6790/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6791/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6792/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6793/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6794/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6795/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6796/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6797/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6798/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6799/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6800/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6801/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6802/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6803/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6804/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6805/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6806/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6807/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6808/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6809/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6810/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6811/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6812/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6813/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6814/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6815/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6816/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6817/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6818/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6819/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6820/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6821/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6822/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6823/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6824/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6825/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6826/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6827/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6828/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6829/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6830/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6831/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6832/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6833/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6834/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6835/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6836/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6837/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6838/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6839/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6840/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6841/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6842/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6843/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6844/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6845/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6846/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6847/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6848/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6849/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6850/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6851/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6852/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6853/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6854/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6855/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6856/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6857/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6858/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6859/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6860/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6861/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6862/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6863/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6864/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6865/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6866/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6867/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6868/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6869/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6870/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6871/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6872/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6873/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6874/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6875/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6876/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6877/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6878/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6879/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6880/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6881/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6882/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6883/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6884/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6885/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6886/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6887/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6888/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6889/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6890/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6891/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6892/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6893/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6894/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6895/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6896/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6897/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6898/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6899/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6900/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6901/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6902/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6903/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6904/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6905/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6906/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6907/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6908/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6909/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6910/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6911/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6912/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6913/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6914/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6915/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6916/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6917/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6918/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6919/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6920/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6921/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6922/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6923/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6924/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6925/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6926/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6927/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6928/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6929/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6930/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6931/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6932/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6933/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6934/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6935/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6936/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6937/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6938/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6939/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6940/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6941/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6942/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6943/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6944/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6945/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6946/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6947/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6948/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6949/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6950/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6951/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6952/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6953/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6954/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6955/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6956/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6957/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6958/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6959/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6960/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6961/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6962/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6963/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6964/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6965/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6966/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6967/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6968/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6969/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6970/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6971/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6972/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6973/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6974/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6975/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6976/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6977/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6978/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6979/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6980/10000..  Training Loss: 0.141.. \n",
      "Epoch: 6981/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6982/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6983/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6984/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6985/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6986/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6987/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6988/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6989/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6990/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6991/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6992/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6993/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6994/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6995/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6996/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6997/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6998/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6999/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7000/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7001/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7002/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7003/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7004/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7005/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7006/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7007/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7008/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7009/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7010/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7011/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7012/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7013/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7014/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7015/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7016/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7017/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7018/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7019/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7020/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7021/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7022/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7023/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7024/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7025/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7026/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7027/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7028/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7029/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7030/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7031/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7032/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7033/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7034/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7035/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7036/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7037/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7038/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7039/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7040/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7041/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7042/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7043/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7044/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7045/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7046/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7047/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7048/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7049/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7050/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7051/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7052/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7053/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7054/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7055/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7056/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7057/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7058/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7059/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7060/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7061/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7062/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7063/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7064/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7065/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7066/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7067/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7068/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7069/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7070/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7071/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7072/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7073/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7074/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7075/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7076/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7077/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7078/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7079/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7080/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7081/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7082/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7083/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7084/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7085/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7086/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7087/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7088/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7089/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7090/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7091/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7092/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7093/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7094/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7095/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7096/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7097/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7098/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7099/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7100/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7101/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7102/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7103/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7104/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7105/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7106/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7107/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7108/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7109/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7110/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7111/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7112/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7113/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7114/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7115/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7116/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7117/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7118/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7119/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7120/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7121/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7122/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7123/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7124/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7125/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7126/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7127/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7128/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7129/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7130/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7131/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7132/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7133/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7134/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7135/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7136/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7137/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7138/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7139/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7140/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7141/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7142/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7143/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7144/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7145/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7146/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7147/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7148/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7149/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7150/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7151/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7152/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7153/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7154/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7155/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7156/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7157/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7158/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7159/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7160/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7161/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7162/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7163/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7164/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7165/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7166/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7167/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7168/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7169/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7170/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7171/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7172/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7173/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7174/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7175/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7176/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7177/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7178/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7179/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7180/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7181/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7182/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7183/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7184/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7185/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7186/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7187/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7188/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7189/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7190/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7191/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7192/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7193/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7194/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7195/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7196/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7197/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7198/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7199/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7200/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7201/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7202/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7203/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7204/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7205/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7206/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7207/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7208/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7209/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7210/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7211/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7212/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7213/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7214/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7215/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7216/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7217/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7218/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7219/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7220/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7221/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7222/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7223/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7224/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7225/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7226/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7227/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7228/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7229/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7230/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7231/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7232/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7233/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7234/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7235/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7236/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7237/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7238/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7239/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7240/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7241/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7242/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7243/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7244/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7245/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7246/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7247/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7248/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7249/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7250/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7251/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7252/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7253/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7254/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7255/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7256/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7257/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7258/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7259/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7260/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7261/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7262/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7263/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7264/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7265/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7266/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7267/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7268/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7269/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7270/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7271/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7272/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7273/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7274/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7275/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7276/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7277/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7278/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7279/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7280/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7281/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7282/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7283/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7284/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7285/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7286/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7287/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7288/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7289/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7290/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7291/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7292/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7293/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7294/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7295/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7296/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7297/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7298/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7299/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7300/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7301/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7302/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7303/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7304/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7305/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7306/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7307/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7308/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7309/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7310/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7311/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7312/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7313/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7314/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7315/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7316/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7317/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7318/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7319/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7320/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7321/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7322/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7323/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7324/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7325/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7326/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7327/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7328/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7329/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7330/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7331/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7332/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7333/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7334/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7335/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7336/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7337/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7338/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7339/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7340/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7341/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7342/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7343/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7344/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7345/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7346/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7347/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7348/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7349/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7350/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7351/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7352/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7353/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7354/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7355/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7356/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7357/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7358/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7359/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7360/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7361/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7362/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7363/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7364/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7365/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7366/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7367/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7368/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7369/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7370/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7371/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7372/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7373/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7374/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7375/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7376/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7377/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7378/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7379/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7380/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7381/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7382/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7383/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7384/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7385/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7386/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7387/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7388/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7389/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7390/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7391/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7392/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7393/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7394/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7395/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7396/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7397/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7398/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7399/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7400/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7401/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7402/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7403/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7404/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7405/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7406/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7407/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7408/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7409/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7410/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7411/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7412/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7413/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7414/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7415/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7416/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7417/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7418/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7419/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7420/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7421/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7422/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7423/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7424/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7425/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7426/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7427/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7428/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7429/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7430/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7431/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7432/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7433/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7434/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7435/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7436/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7437/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7438/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7439/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7440/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7441/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7442/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7443/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7444/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7445/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7446/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7447/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7448/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7449/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7450/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7451/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7452/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7453/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7454/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7455/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7456/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7457/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7458/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7459/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7460/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7461/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7462/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7463/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7464/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7465/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7466/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7467/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7468/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7469/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7470/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7471/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7472/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7473/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7474/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7475/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7476/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7477/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7478/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7479/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7480/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7481/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7482/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7483/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7484/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7485/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7486/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7487/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7488/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7489/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7490/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7491/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7492/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7493/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7494/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7495/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7496/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7497/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7498/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7499/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7500/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7501/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7502/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7503/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7504/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7505/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7506/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7507/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7508/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7509/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7510/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7511/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7512/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7513/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7514/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7515/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7516/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7517/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7518/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7519/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7520/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7521/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7522/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7523/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7524/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7525/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7526/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7527/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7528/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7529/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7530/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7531/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7532/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7533/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7534/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7535/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7536/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7537/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7538/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7539/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7540/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7541/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7542/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7543/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7544/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7545/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7546/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7547/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7548/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7549/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7550/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7551/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7552/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7553/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7554/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7555/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7556/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7557/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7558/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7559/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7560/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7561/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7562/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7563/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7564/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7565/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7566/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7567/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7568/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7569/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7570/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7571/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7572/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7573/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7574/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7575/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7576/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7577/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7578/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7579/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7580/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7581/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7582/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7583/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7584/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7585/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7586/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7587/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7588/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7589/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7590/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7591/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7592/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7593/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7594/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7595/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7596/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7597/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7598/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7599/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7600/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7601/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7602/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7603/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7604/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7605/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7606/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7607/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7608/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7609/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7610/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7611/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7612/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7613/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7614/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7615/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7616/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7617/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7618/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7619/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7620/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7621/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7622/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7623/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7624/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7625/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7626/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7627/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7628/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7629/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7630/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7631/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7632/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7633/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7634/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7635/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7636/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7637/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7638/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7639/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7640/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7641/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7642/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7643/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7644/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7645/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7646/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7647/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7648/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7649/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7650/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7651/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7652/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7653/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7654/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7655/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7656/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7657/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7658/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7659/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7660/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7661/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7662/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7663/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7664/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7665/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7666/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7667/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7668/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7669/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7670/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7671/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7672/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7673/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7674/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7675/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7676/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7677/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7678/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7679/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7680/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7681/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7682/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7683/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7684/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7685/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7686/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7687/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7688/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7689/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7690/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7691/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7692/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7693/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7694/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7695/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7696/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7697/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7698/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7699/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7700/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7701/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7702/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7703/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7704/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7705/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7706/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7707/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7708/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7709/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7710/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7711/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7712/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7713/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7714/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7715/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7716/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7717/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7718/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7719/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7720/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7721/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7722/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7723/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7724/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7725/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7726/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7727/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7728/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7729/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7730/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7731/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7732/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7733/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7734/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7735/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7736/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7737/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7738/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7739/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7740/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7741/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7742/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7743/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7744/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7745/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7746/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7747/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7748/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7749/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7750/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7751/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7752/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7753/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7754/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7755/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7756/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7757/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7758/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7759/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7760/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7761/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7762/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7763/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7764/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7765/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7766/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7767/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7768/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7769/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7770/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7771/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7772/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7773/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7774/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7775/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7776/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7777/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7778/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7779/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7780/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7781/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7782/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7783/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7784/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7785/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7786/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7787/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7788/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7789/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7790/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7791/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7792/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7793/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7794/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7795/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7796/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7797/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7798/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7799/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7800/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7801/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7802/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7803/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7804/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7805/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7806/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7807/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7808/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7809/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7810/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7811/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7812/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7813/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7814/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7815/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7816/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7817/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7818/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7819/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7820/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7821/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7822/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7823/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7824/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7825/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7826/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7827/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7828/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7829/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7830/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7831/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7832/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7833/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7834/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7835/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7836/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7837/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7838/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7839/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7840/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7841/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7842/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7843/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7844/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7845/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7846/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7847/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7848/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7849/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7850/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7851/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7852/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7853/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7854/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7855/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7856/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7857/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7858/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7859/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7860/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7861/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7862/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7863/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7864/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7865/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7866/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7867/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7868/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7869/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7870/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7871/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7872/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7873/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7874/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7875/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7876/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7877/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7878/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7879/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7880/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7881/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7882/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7883/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7884/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7885/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7886/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7887/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7888/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7889/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7890/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7891/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7892/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7893/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7894/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7895/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7896/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7897/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7898/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7899/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7900/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7901/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7902/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7903/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7904/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7905/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7906/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7907/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7908/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7909/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7910/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7911/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7912/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7913/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7914/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7915/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7916/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7917/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7918/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7919/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7920/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7921/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7922/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7923/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7924/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7925/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7926/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7927/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7928/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7929/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7930/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7931/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7932/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7933/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7934/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7935/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7936/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7937/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7938/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7939/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7940/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7941/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7942/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7943/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7944/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7945/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7946/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7947/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7948/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7949/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7950/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7951/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7952/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7953/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7954/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7955/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7956/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7957/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7958/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7959/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7960/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7961/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7962/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7963/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7964/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7965/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7966/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7967/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7968/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7969/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7970/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7971/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7972/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7973/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7974/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7975/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7976/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7977/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7978/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7979/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7980/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7981/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7982/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7983/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7984/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7985/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7986/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7987/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7988/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7989/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7990/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7991/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7992/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7993/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7994/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7995/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7996/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7997/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7998/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7999/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8000/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8001/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8002/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8003/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8004/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8005/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8006/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8007/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8008/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8009/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8010/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8011/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8012/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8013/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8014/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8015/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8016/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8017/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8018/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8019/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8020/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8021/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8022/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8023/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8024/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8025/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8026/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8027/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8028/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8029/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8030/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8031/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8032/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8033/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8034/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8035/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8036/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8037/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8038/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8039/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8040/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8041/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8042/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8043/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8044/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8045/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8046/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8047/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8048/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8049/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8050/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8051/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8052/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8053/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8054/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8055/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8056/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8057/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8058/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8059/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8060/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8061/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8062/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8063/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8064/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8065/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8066/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8067/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8068/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8069/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8070/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8071/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8072/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8073/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8074/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8075/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8076/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8077/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8078/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8079/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8080/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8081/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8082/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8083/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8084/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8085/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8086/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8087/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8088/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8089/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8090/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8091/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8092/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8093/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8094/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8095/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8096/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8097/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8098/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8099/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8100/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8101/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8102/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8103/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8104/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8105/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8106/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8107/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8108/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8109/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8110/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8111/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8112/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8113/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8114/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8115/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8116/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8117/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8118/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8119/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8120/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8121/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8122/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8123/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8124/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8125/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8126/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8127/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8128/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8129/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8130/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8131/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8132/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8133/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8134/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8135/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8136/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8137/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8138/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8139/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8140/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8141/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8142/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8143/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8144/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8145/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8146/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8147/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8148/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8149/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8150/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8151/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8152/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8153/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8154/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8155/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8156/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8157/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8158/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8159/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8160/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8161/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8162/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8163/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8164/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8165/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8166/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8167/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8168/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8169/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8170/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8171/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8172/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8173/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8174/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8175/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8176/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8177/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8178/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8179/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8180/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8181/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8182/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8183/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8184/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8185/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8186/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8187/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8188/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8189/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8190/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8191/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8192/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8193/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8194/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8195/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8196/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8197/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8198/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8199/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8200/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8201/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8202/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8203/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8204/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8205/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8206/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8207/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8208/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8209/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8210/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8211/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8212/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8213/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8214/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8215/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8216/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8217/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8218/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8219/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8220/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8221/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8222/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8223/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8224/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8225/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8226/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8227/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8228/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8229/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8230/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8231/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8232/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8233/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8234/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8235/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8236/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8237/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8238/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8239/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8240/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8241/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8242/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8243/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8244/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8245/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8246/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8247/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8248/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8249/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8250/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8251/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8252/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8253/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8254/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8255/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8256/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8257/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8258/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8259/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8260/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8261/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8262/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8263/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8264/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8265/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8266/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8267/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8268/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8269/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8270/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8271/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8272/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8273/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8274/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8275/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8276/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8277/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8278/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8279/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8280/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8281/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8282/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8283/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8284/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8285/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8286/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8287/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8288/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8289/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8290/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8291/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8292/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8293/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8294/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8295/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8296/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8297/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8298/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8299/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8300/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8301/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8302/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8303/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8304/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8305/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8306/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8307/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8308/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8309/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8310/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8311/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8312/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8313/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8314/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8315/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8316/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8317/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8318/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8319/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8320/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8321/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8322/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8323/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8324/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8325/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8326/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8327/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8328/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8329/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8330/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8331/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8332/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8333/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8334/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8335/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8336/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8337/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8338/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8339/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8340/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8341/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8342/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8343/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8344/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8345/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8346/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8347/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8348/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8349/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8350/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8351/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8352/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8353/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8354/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8355/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8356/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8357/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8358/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8359/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8360/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8361/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8362/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8363/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8364/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8365/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8366/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8367/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8368/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8369/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8370/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8371/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8372/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8373/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8374/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8375/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8376/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8377/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8378/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8379/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8380/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8381/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8382/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8383/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8384/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8385/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8386/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8387/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8388/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8389/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8390/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8391/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8392/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8393/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8394/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8395/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8396/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8397/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8398/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8399/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8400/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8401/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8402/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8403/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8404/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8405/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8406/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8407/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8408/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8409/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8410/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8411/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8412/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8413/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8414/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8415/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8416/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8417/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8418/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8419/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8420/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8421/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8422/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8423/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8424/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8425/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8426/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8427/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8428/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8429/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8430/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8431/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8432/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8433/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8434/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8435/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8436/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8437/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8438/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8439/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8440/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8441/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8442/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8443/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8444/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8445/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8446/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8447/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8448/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8449/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8450/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8451/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8452/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8453/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8454/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8455/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8456/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8457/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8458/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8459/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8460/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8461/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8462/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8463/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8464/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8465/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8466/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8467/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8468/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8469/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8470/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8471/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8472/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8473/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8474/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8475/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8476/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8477/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8478/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8479/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8480/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8481/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8482/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8483/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8484/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8485/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8486/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8487/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8488/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8489/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8490/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8491/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8492/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8493/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8494/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8495/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8496/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8497/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8498/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8499/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8500/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8501/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8502/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8503/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8504/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8505/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8506/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8507/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8508/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8509/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8510/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8511/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8512/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8513/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8514/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8515/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8516/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8517/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8518/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8519/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8520/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8521/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8522/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8523/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8524/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8525/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8526/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8527/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8528/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8529/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8530/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8531/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8532/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8533/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8534/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8535/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8536/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8537/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8538/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8539/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8540/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8541/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8542/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8543/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8544/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8545/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8546/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8547/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8548/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8549/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8550/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8551/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8552/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8553/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8554/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8555/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8556/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8557/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8558/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8559/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8560/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8561/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8562/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8563/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8564/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8565/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8566/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8567/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8568/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8569/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8570/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8571/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8572/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8573/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8574/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8575/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8576/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8577/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8578/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8579/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8580/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8581/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8582/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8583/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8584/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8585/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8586/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8587/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8588/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8589/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8590/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8591/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8592/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8593/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8594/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8595/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8596/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8597/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8598/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8599/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8600/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8601/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8602/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8603/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8604/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8605/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8606/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8607/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8608/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8609/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8610/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8611/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8612/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8613/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8614/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8615/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8616/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8617/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8618/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8619/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8620/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8621/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8622/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8623/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8624/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8625/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8626/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8627/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8628/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8629/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8630/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8631/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8632/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8633/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8634/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8635/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8636/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8637/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8638/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8639/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8640/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8641/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8642/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8643/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8644/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8645/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8646/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8647/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8648/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8649/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8650/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8651/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8652/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8653/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8654/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8655/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8656/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8657/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8658/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8659/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8660/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8661/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8662/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8663/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8664/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8665/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8666/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8667/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8668/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8669/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8670/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8671/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8672/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8673/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8674/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8675/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8676/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8677/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8678/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8679/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8680/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8681/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8682/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8683/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8684/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8685/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8686/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8687/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8688/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8689/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8690/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8691/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8692/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8693/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8694/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8695/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8696/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8697/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8698/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8699/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8700/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8701/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8702/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8703/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8704/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8705/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8706/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8707/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8708/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8709/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8710/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8711/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8712/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8713/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8714/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8715/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8716/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8717/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8718/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8719/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8720/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8721/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8722/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8723/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8724/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8725/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8726/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8727/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8728/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8729/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8730/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8731/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8732/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8733/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8734/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8735/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8736/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8737/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8738/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8739/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8740/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8741/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8742/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8743/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8744/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8745/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8746/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8747/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8748/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8749/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8750/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8751/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8752/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8753/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8754/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8755/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8756/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8757/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8758/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8759/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8760/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8761/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8762/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8763/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8764/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8765/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8766/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8767/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8768/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8769/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8770/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8771/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8772/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8773/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8774/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8775/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8776/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8777/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8778/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8779/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8780/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8781/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8782/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8783/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8784/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8785/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8786/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8787/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8788/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8789/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8790/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8791/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8792/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8793/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8794/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8795/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8796/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8797/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8798/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8799/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8800/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8801/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8802/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8803/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8804/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8805/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8806/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8807/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8808/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8809/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8810/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8811/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8812/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8813/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8814/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8815/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8816/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8817/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8818/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8819/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8820/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8821/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8822/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8823/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8824/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8825/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8826/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8827/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8828/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8829/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8830/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8831/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8832/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8833/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8834/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8835/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8836/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8837/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8838/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8839/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8840/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8841/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8842/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8843/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8844/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8845/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8846/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8847/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8848/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8849/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8850/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8851/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8852/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8853/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8854/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8855/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8856/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8857/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8858/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8859/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8860/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8861/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8862/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8863/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8864/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8865/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8866/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8867/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8868/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8869/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8870/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8871/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8872/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8873/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8874/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8875/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8876/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8877/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8878/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8879/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8880/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8881/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8882/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8883/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8884/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8885/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8886/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8887/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8888/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8889/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8890/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8891/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8892/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8893/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8894/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8895/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8896/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8897/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8898/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8899/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8900/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8901/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8902/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8903/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8904/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8905/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8906/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8907/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8908/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8909/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8910/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8911/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8912/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8913/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8914/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8915/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8916/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8917/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8918/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8919/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8920/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8921/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8922/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8923/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8924/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8925/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8926/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8927/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8928/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8929/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8930/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8931/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8932/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8933/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8934/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8935/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8936/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8937/10000..  Training Loss: 0.145.. \n",
      "Epoch: 8938/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8939/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8940/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8941/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8942/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8943/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8944/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8945/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8946/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8947/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8948/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8949/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8950/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8951/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8952/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8953/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8954/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8955/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8956/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8957/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8958/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8959/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8960/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8961/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8962/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8963/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8964/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8965/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8966/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8967/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8968/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8969/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8970/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8971/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8972/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8973/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8974/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8975/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8976/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8977/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8978/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8979/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8980/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8981/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8982/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8983/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8984/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8985/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8986/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8987/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8988/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8989/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8990/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8991/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8992/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8993/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8994/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8995/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8996/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8997/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8998/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8999/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9000/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9001/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9002/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9003/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9004/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9005/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9006/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9007/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9008/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9009/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9010/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9011/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9012/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9013/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9014/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9015/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9016/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9017/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9018/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9019/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9020/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9021/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9022/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9023/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9024/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9025/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9026/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9027/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9028/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9029/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9030/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9031/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9032/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9033/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9034/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9035/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9036/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9037/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9038/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9039/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9040/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9041/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9042/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9043/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9044/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9045/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9046/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9047/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9048/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9049/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9050/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9051/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9052/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9053/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9054/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9055/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9056/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9057/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9058/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9059/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9060/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9061/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9062/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9063/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9064/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9065/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9066/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9067/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9068/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9069/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9070/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9071/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9072/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9073/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9074/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9075/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9076/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9077/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9078/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9079/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9080/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9081/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9082/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9083/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9084/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9085/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9086/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9087/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9088/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9089/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9090/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9091/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9092/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9093/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9094/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9095/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9096/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9097/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9098/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9099/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9100/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9101/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9102/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9103/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9104/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9105/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9106/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9107/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9108/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9109/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9110/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9111/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9112/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9113/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9114/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9115/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9116/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9117/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9118/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9119/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9120/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9121/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9122/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9123/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9124/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9125/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9126/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9127/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9128/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9129/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9130/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9131/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9132/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9133/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9134/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9135/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9136/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9137/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9138/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9139/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9140/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9141/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9142/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9143/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9144/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9145/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9146/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9147/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9148/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9149/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9150/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9151/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9152/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9153/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9154/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9155/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9156/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9157/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9158/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9159/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9160/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9161/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9162/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9163/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9164/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9165/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9166/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9167/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9168/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9169/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9170/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9171/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9172/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9173/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9174/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9175/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9176/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9177/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9178/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9179/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9180/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9181/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9182/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9183/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9184/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9185/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9186/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9187/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9188/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9189/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9190/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9191/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9192/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9193/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9194/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9195/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9196/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9197/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9198/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9199/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9200/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9201/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9202/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9203/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9204/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9205/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9206/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9207/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9208/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9209/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9210/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9211/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9212/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9213/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9214/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9215/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9216/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9217/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9218/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9219/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9220/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9221/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9222/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9223/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9224/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9225/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9226/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9227/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9228/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9229/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9230/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9231/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9232/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9233/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9234/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9235/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9236/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9237/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9238/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9239/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9240/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9241/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9242/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9243/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9244/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9245/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9246/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9247/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9248/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9249/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9250/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9251/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9252/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9253/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9254/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9255/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9256/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9257/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9258/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9259/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9260/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9261/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9262/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9263/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9264/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9265/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9266/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9267/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9268/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9269/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9270/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9271/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9272/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9273/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9274/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9275/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9276/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9277/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9278/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9279/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9280/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9281/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9282/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9283/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9284/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9285/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9286/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9287/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9288/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9289/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9290/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9291/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9292/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9293/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9294/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9295/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9296/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9297/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9298/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9299/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9300/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9301/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9302/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9303/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9304/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9305/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9306/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9307/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9308/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9309/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9310/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9311/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9312/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9313/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9314/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9315/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9316/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9317/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9318/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9319/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9320/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9321/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9322/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9323/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9324/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9325/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9326/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9327/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9328/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9329/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9330/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9331/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9332/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9333/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9334/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9335/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9336/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9337/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9338/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9339/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9340/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9341/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9342/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9343/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9344/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9345/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9346/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9347/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9348/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9349/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9350/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9351/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9352/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9353/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9354/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9355/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9356/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9357/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9358/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9359/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9360/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9361/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9362/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9363/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9364/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9365/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9366/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9367/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9368/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9369/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9370/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9371/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9372/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9373/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9374/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9375/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9376/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9377/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9378/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9379/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9380/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9381/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9382/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9383/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9384/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9385/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9386/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9387/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9388/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9389/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9390/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9391/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9392/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9393/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9394/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9395/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9396/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9397/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9398/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9399/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9400/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9401/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9402/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9403/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9404/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9405/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9406/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9407/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9408/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9409/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9410/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9411/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9412/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9413/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9414/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9415/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9416/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9417/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9418/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9419/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9420/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9421/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9422/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9423/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9424/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9425/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9426/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9427/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9428/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9429/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9430/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9431/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9432/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9433/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9434/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9435/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9436/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9437/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9438/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9439/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9440/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9441/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9442/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9443/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9444/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9445/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9446/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9447/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9448/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9449/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9450/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9451/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9452/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9453/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9454/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9455/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9456/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9457/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9458/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9459/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9460/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9461/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9462/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9463/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9464/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9465/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9466/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9467/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9468/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9469/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9470/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9471/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9472/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9473/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9474/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9475/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9476/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9477/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9478/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9479/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9480/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9481/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9482/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9483/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9484/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9485/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9486/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9487/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9488/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9489/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9490/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9491/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9492/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9493/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9494/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9495/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9496/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9497/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9498/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9499/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9500/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9501/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9502/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9503/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9504/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9505/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9506/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9507/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9508/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9509/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9510/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9511/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9512/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9513/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9514/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9515/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9516/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9517/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9518/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9519/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9520/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9521/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9522/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9523/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9524/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9525/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9526/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9527/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9528/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9529/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9530/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9531/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9532/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9533/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9534/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9535/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9536/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9537/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9538/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9539/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9540/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9541/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9542/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9543/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9544/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9545/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9546/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9547/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9548/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9549/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9550/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9551/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9552/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9553/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9554/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9555/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9556/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9557/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9558/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9559/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9560/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9561/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9562/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9563/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9564/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9565/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9566/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9567/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9568/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9569/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9570/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9571/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9572/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9573/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9574/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9575/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9576/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9577/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9578/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9579/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9580/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9581/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9582/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9583/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9584/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9585/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9586/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9587/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9588/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9589/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9590/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9591/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9592/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9593/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9594/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9595/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9596/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9597/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9598/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9599/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9600/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9601/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9602/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9603/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9604/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9605/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9606/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9607/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9608/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9609/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9610/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9611/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9612/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9613/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9614/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9615/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9616/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9617/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9618/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9619/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9620/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9621/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9622/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9623/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9624/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9625/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9626/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9627/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9628/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9629/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9630/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9631/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9632/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9633/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9634/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9635/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9636/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9637/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9638/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9639/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9640/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9641/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9642/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9643/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9644/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9645/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9646/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9647/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9648/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9649/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9650/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9651/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9652/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9653/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9654/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9655/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9656/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9657/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9658/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9659/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9660/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9661/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9662/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9663/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9664/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9665/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9666/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9667/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9668/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9669/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9670/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9671/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9672/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9673/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9674/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9675/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9676/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9677/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9678/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9679/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9680/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9681/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9682/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9683/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9684/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9685/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9686/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9687/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9688/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9689/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9690/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9691/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9692/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9693/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9694/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9695/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9696/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9697/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9698/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9699/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9700/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9701/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9702/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9703/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9704/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9705/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9706/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9707/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9708/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9709/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9710/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9711/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9712/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9713/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9714/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9715/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9716/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9717/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9718/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9719/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9720/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9721/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9722/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9723/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9724/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9725/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9726/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9727/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9728/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9729/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9730/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9731/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9732/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9733/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9734/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9735/10000..  Training Loss: 0.142.. \n",
      "Epoch: 9736/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9737/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9738/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9739/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9740/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9741/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9742/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9743/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9744/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9745/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9746/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9747/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9748/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9749/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9750/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9751/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9752/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9753/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9754/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9755/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9756/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9757/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9758/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9759/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9760/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9761/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9762/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9763/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9764/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9765/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9766/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9767/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9768/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9769/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9770/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9771/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9772/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9773/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9774/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9775/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9776/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9777/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9778/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9779/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9780/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9781/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9782/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9783/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9784/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9785/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9786/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9787/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9788/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9789/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9790/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9791/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9792/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9793/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9794/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9795/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9796/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9797/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9798/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9799/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9800/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9801/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9802/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9803/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9804/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9805/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9806/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9807/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9808/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9809/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9810/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9811/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9812/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9813/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9814/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9815/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9816/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9817/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9818/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9819/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9820/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9821/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9822/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9823/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9824/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9825/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9826/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9827/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9828/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9829/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9830/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9831/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9832/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9833/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9834/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9835/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9836/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9837/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9838/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9839/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9840/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9841/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9842/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9843/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9844/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9845/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9846/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9847/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9848/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9849/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9850/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9851/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9852/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9853/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9854/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9855/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9856/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9857/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9858/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9859/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9860/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9861/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9862/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9863/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9864/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9865/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9866/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9867/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9868/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9869/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9870/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9871/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9872/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9873/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9874/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9875/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9876/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9877/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9878/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9879/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9880/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9881/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9882/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9883/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9884/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9885/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9886/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9887/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9888/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9889/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9890/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9891/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9892/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9893/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9894/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9895/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9896/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9897/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9898/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9899/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9900/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9901/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9902/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9903/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9904/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9905/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9906/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9907/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9908/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9909/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9910/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9911/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9912/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9913/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9914/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9915/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9916/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9917/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9918/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9919/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9920/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9921/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9922/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9923/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9924/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9925/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9926/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9927/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9928/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9929/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9930/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9931/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9932/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9933/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9934/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9935/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9936/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9937/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9938/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9939/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9940/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9941/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9942/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9943/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9944/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9945/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9946/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9947/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9948/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9949/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9950/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9951/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9952/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9953/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9954/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9955/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9956/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9957/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9958/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9959/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9960/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9961/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9962/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9963/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9964/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9965/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9966/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9967/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9968/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9969/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9970/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9971/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9972/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9973/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9974/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9975/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9976/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9977/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9978/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9979/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9980/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9981/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9982/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9983/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9984/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9985/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9986/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9987/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9988/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9989/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9990/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9991/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9992/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9993/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9994/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9995/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9996/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9997/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9998/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9999/10000..  Training Loss: 0.127.. \n",
      "Epoch: 10000/10000..  Training Loss: 0.127.. \n",
      "Epoch: 1/10000..  Training Loss: 0.687.. \n",
      "Epoch: 2/10000..  Training Loss: 0.509.. \n",
      "Epoch: 3/10000..  Training Loss: 0.465.. \n",
      "Epoch: 4/10000..  Training Loss: 0.459.. \n",
      "Epoch: 5/10000..  Training Loss: 0.449.. \n",
      "Epoch: 6/10000..  Training Loss: 0.428.. \n",
      "Epoch: 7/10000..  Training Loss: 0.413.. \n",
      "Epoch: 8/10000..  Training Loss: 0.409.. \n",
      "Epoch: 9/10000..  Training Loss: 0.410.. \n",
      "Epoch: 10/10000..  Training Loss: 0.409.. \n",
      "Epoch: 11/10000..  Training Loss: 0.404.. \n",
      "Epoch: 12/10000..  Training Loss: 0.399.. \n",
      "Epoch: 13/10000..  Training Loss: 0.395.. \n",
      "Epoch: 14/10000..  Training Loss: 0.392.. \n",
      "Epoch: 15/10000..  Training Loss: 0.391.. \n",
      "Epoch: 16/10000..  Training Loss: 0.390.. \n",
      "Epoch: 17/10000..  Training Loss: 0.388.. \n",
      "Epoch: 18/10000..  Training Loss: 0.385.. \n",
      "Epoch: 19/10000..  Training Loss: 0.382.. \n",
      "Epoch: 20/10000..  Training Loss: 0.380.. \n",
      "Epoch: 21/10000..  Training Loss: 0.379.. \n",
      "Epoch: 22/10000..  Training Loss: 0.378.. \n",
      "Epoch: 23/10000..  Training Loss: 0.376.. \n",
      "Epoch: 24/10000..  Training Loss: 0.374.. \n",
      "Epoch: 25/10000..  Training Loss: 0.372.. \n",
      "Epoch: 26/10000..  Training Loss: 0.371.. \n",
      "Epoch: 27/10000..  Training Loss: 0.369.. \n",
      "Epoch: 28/10000..  Training Loss: 0.368.. \n",
      "Epoch: 29/10000..  Training Loss: 0.367.. \n",
      "Epoch: 30/10000..  Training Loss: 0.365.. \n",
      "Epoch: 31/10000..  Training Loss: 0.364.. \n",
      "Epoch: 32/10000..  Training Loss: 0.363.. \n",
      "Epoch: 33/10000..  Training Loss: 0.362.. \n",
      "Epoch: 34/10000..  Training Loss: 0.360.. \n",
      "Epoch: 35/10000..  Training Loss: 0.359.. \n",
      "Epoch: 36/10000..  Training Loss: 0.357.. \n",
      "Epoch: 37/10000..  Training Loss: 0.356.. \n",
      "Epoch: 38/10000..  Training Loss: 0.355.. \n",
      "Epoch: 39/10000..  Training Loss: 0.353.. \n",
      "Epoch: 40/10000..  Training Loss: 0.352.. \n",
      "Epoch: 41/10000..  Training Loss: 0.350.. \n",
      "Epoch: 42/10000..  Training Loss: 0.349.. \n",
      "Epoch: 43/10000..  Training Loss: 0.348.. \n",
      "Epoch: 44/10000..  Training Loss: 0.346.. \n",
      "Epoch: 45/10000..  Training Loss: 0.345.. \n",
      "Epoch: 46/10000..  Training Loss: 0.343.. \n",
      "Epoch: 47/10000..  Training Loss: 0.342.. \n",
      "Epoch: 48/10000..  Training Loss: 0.341.. \n",
      "Epoch: 49/10000..  Training Loss: 0.339.. \n",
      "Epoch: 50/10000..  Training Loss: 0.338.. \n",
      "Epoch: 51/10000..  Training Loss: 0.336.. \n",
      "Epoch: 52/10000..  Training Loss: 0.335.. \n",
      "Epoch: 53/10000..  Training Loss: 0.334.. \n",
      "Epoch: 54/10000..  Training Loss: 0.332.. \n",
      "Epoch: 55/10000..  Training Loss: 0.331.. \n",
      "Epoch: 56/10000..  Training Loss: 0.329.. \n",
      "Epoch: 57/10000..  Training Loss: 0.328.. \n",
      "Epoch: 58/10000..  Training Loss: 0.327.. \n",
      "Epoch: 59/10000..  Training Loss: 0.325.. \n",
      "Epoch: 60/10000..  Training Loss: 0.324.. \n",
      "Epoch: 61/10000..  Training Loss: 0.323.. \n",
      "Epoch: 62/10000..  Training Loss: 0.321.. \n",
      "Epoch: 63/10000..  Training Loss: 0.320.. \n",
      "Epoch: 64/10000..  Training Loss: 0.319.. \n",
      "Epoch: 65/10000..  Training Loss: 0.318.. \n",
      "Epoch: 66/10000..  Training Loss: 0.317.. \n",
      "Epoch: 67/10000..  Training Loss: 0.317.. \n",
      "Epoch: 68/10000..  Training Loss: 0.319.. \n",
      "Epoch: 69/10000..  Training Loss: 0.318.. \n",
      "Epoch: 70/10000..  Training Loss: 0.313.. \n",
      "Epoch: 71/10000..  Training Loss: 0.312.. \n",
      "Epoch: 72/10000..  Training Loss: 0.314.. \n",
      "Epoch: 73/10000..  Training Loss: 0.311.. \n",
      "Epoch: 74/10000..  Training Loss: 0.309.. \n",
      "Epoch: 75/10000..  Training Loss: 0.311.. \n",
      "Epoch: 76/10000..  Training Loss: 0.309.. \n",
      "Epoch: 77/10000..  Training Loss: 0.306.. \n",
      "Epoch: 78/10000..  Training Loss: 0.307.. \n",
      "Epoch: 79/10000..  Training Loss: 0.306.. \n",
      "Epoch: 80/10000..  Training Loss: 0.304.. \n",
      "Epoch: 81/10000..  Training Loss: 0.304.. \n",
      "Epoch: 82/10000..  Training Loss: 0.304.. \n",
      "Epoch: 83/10000..  Training Loss: 0.302.. \n",
      "Epoch: 84/10000..  Training Loss: 0.301.. \n",
      "Epoch: 85/10000..  Training Loss: 0.301.. \n",
      "Epoch: 86/10000..  Training Loss: 0.301.. \n",
      "Epoch: 87/10000..  Training Loss: 0.299.. \n",
      "Epoch: 88/10000..  Training Loss: 0.298.. \n",
      "Epoch: 89/10000..  Training Loss: 0.298.. \n",
      "Epoch: 90/10000..  Training Loss: 0.298.. \n",
      "Epoch: 91/10000..  Training Loss: 0.296.. \n",
      "Epoch: 92/10000..  Training Loss: 0.295.. \n",
      "Epoch: 93/10000..  Training Loss: 0.295.. \n",
      "Epoch: 94/10000..  Training Loss: 0.295.. \n",
      "Epoch: 95/10000..  Training Loss: 0.294.. \n",
      "Epoch: 96/10000..  Training Loss: 0.293.. \n",
      "Epoch: 97/10000..  Training Loss: 0.292.. \n",
      "Epoch: 98/10000..  Training Loss: 0.292.. \n",
      "Epoch: 99/10000..  Training Loss: 0.291.. \n",
      "Epoch: 100/10000..  Training Loss: 0.291.. \n",
      "Epoch: 101/10000..  Training Loss: 0.290.. \n",
      "Epoch: 102/10000..  Training Loss: 0.290.. \n",
      "Epoch: 103/10000..  Training Loss: 0.290.. \n",
      "Epoch: 104/10000..  Training Loss: 0.289.. \n",
      "Epoch: 105/10000..  Training Loss: 0.289.. \n",
      "Epoch: 106/10000..  Training Loss: 0.288.. \n",
      "Epoch: 107/10000..  Training Loss: 0.287.. \n",
      "Epoch: 108/10000..  Training Loss: 0.286.. \n",
      "Epoch: 109/10000..  Training Loss: 0.286.. \n",
      "Epoch: 110/10000..  Training Loss: 0.285.. \n",
      "Epoch: 111/10000..  Training Loss: 0.284.. \n",
      "Epoch: 112/10000..  Training Loss: 0.284.. \n",
      "Epoch: 113/10000..  Training Loss: 0.283.. \n",
      "Epoch: 114/10000..  Training Loss: 0.283.. \n",
      "Epoch: 115/10000..  Training Loss: 0.284.. \n",
      "Epoch: 116/10000..  Training Loss: 0.285.. \n",
      "Epoch: 117/10000..  Training Loss: 0.288.. \n",
      "Epoch: 118/10000..  Training Loss: 0.290.. \n",
      "Epoch: 119/10000..  Training Loss: 0.290.. \n",
      "Epoch: 120/10000..  Training Loss: 0.282.. \n",
      "Epoch: 121/10000..  Training Loss: 0.281.. \n",
      "Epoch: 122/10000..  Training Loss: 0.285.. \n",
      "Epoch: 123/10000..  Training Loss: 0.284.. \n",
      "Epoch: 124/10000..  Training Loss: 0.279.. \n",
      "Epoch: 125/10000..  Training Loss: 0.279.. \n",
      "Epoch: 126/10000..  Training Loss: 0.282.. \n",
      "Epoch: 127/10000..  Training Loss: 0.280.. \n",
      "Epoch: 128/10000..  Training Loss: 0.277.. \n",
      "Epoch: 129/10000..  Training Loss: 0.278.. \n",
      "Epoch: 130/10000..  Training Loss: 0.280.. \n",
      "Epoch: 131/10000..  Training Loss: 0.277.. \n",
      "Epoch: 132/10000..  Training Loss: 0.276.. \n",
      "Epoch: 133/10000..  Training Loss: 0.277.. \n",
      "Epoch: 134/10000..  Training Loss: 0.277.. \n",
      "Epoch: 135/10000..  Training Loss: 0.275.. \n",
      "Epoch: 136/10000..  Training Loss: 0.274.. \n",
      "Epoch: 137/10000..  Training Loss: 0.275.. \n",
      "Epoch: 138/10000..  Training Loss: 0.275.. \n",
      "Epoch: 139/10000..  Training Loss: 0.274.. \n",
      "Epoch: 140/10000..  Training Loss: 0.273.. \n",
      "Epoch: 141/10000..  Training Loss: 0.273.. \n",
      "Epoch: 142/10000..  Training Loss: 0.273.. \n",
      "Epoch: 143/10000..  Training Loss: 0.273.. \n",
      "Epoch: 144/10000..  Training Loss: 0.272.. \n",
      "Epoch: 145/10000..  Training Loss: 0.271.. \n",
      "Epoch: 146/10000..  Training Loss: 0.271.. \n",
      "Epoch: 147/10000..  Training Loss: 0.271.. \n",
      "Epoch: 148/10000..  Training Loss: 0.271.. \n",
      "Epoch: 149/10000..  Training Loss: 0.270.. \n",
      "Epoch: 150/10000..  Training Loss: 0.269.. \n",
      "Epoch: 151/10000..  Training Loss: 0.269.. \n",
      "Epoch: 152/10000..  Training Loss: 0.269.. \n",
      "Epoch: 153/10000..  Training Loss: 0.269.. \n",
      "Epoch: 154/10000..  Training Loss: 0.269.. \n",
      "Epoch: 155/10000..  Training Loss: 0.269.. \n",
      "Epoch: 156/10000..  Training Loss: 0.269.. \n",
      "Epoch: 157/10000..  Training Loss: 0.268.. \n",
      "Epoch: 158/10000..  Training Loss: 0.268.. \n",
      "Epoch: 159/10000..  Training Loss: 0.267.. \n",
      "Epoch: 160/10000..  Training Loss: 0.267.. \n",
      "Epoch: 161/10000..  Training Loss: 0.266.. \n",
      "Epoch: 162/10000..  Training Loss: 0.266.. \n",
      "Epoch: 163/10000..  Training Loss: 0.266.. \n",
      "Epoch: 164/10000..  Training Loss: 0.265.. \n",
      "Epoch: 165/10000..  Training Loss: 0.265.. \n",
      "Epoch: 166/10000..  Training Loss: 0.265.. \n",
      "Epoch: 167/10000..  Training Loss: 0.266.. \n",
      "Epoch: 168/10000..  Training Loss: 0.266.. \n",
      "Epoch: 169/10000..  Training Loss: 0.268.. \n",
      "Epoch: 170/10000..  Training Loss: 0.270.. \n",
      "Epoch: 171/10000..  Training Loss: 0.272.. \n",
      "Epoch: 172/10000..  Training Loss: 0.269.. \n",
      "Epoch: 173/10000..  Training Loss: 0.264.. \n",
      "Epoch: 174/10000..  Training Loss: 0.263.. \n",
      "Epoch: 175/10000..  Training Loss: 0.265.. \n",
      "Epoch: 176/10000..  Training Loss: 0.267.. \n",
      "Epoch: 177/10000..  Training Loss: 0.267.. \n",
      "Epoch: 178/10000..  Training Loss: 0.263.. \n",
      "Epoch: 179/10000..  Training Loss: 0.261.. \n",
      "Epoch: 180/10000..  Training Loss: 0.262.. \n",
      "Epoch: 181/10000..  Training Loss: 0.264.. \n",
      "Epoch: 182/10000..  Training Loss: 0.264.. \n",
      "Epoch: 183/10000..  Training Loss: 0.262.. \n",
      "Epoch: 184/10000..  Training Loss: 0.260.. \n",
      "Epoch: 185/10000..  Training Loss: 0.261.. \n",
      "Epoch: 186/10000..  Training Loss: 0.262.. \n",
      "Epoch: 187/10000..  Training Loss: 0.262.. \n",
      "Epoch: 188/10000..  Training Loss: 0.260.. \n",
      "Epoch: 189/10000..  Training Loss: 0.259.. \n",
      "Epoch: 190/10000..  Training Loss: 0.259.. \n",
      "Epoch: 191/10000..  Training Loss: 0.259.. \n",
      "Epoch: 192/10000..  Training Loss: 0.260.. \n",
      "Epoch: 193/10000..  Training Loss: 0.259.. \n",
      "Epoch: 194/10000..  Training Loss: 0.258.. \n",
      "Epoch: 195/10000..  Training Loss: 0.258.. \n",
      "Epoch: 196/10000..  Training Loss: 0.257.. \n",
      "Epoch: 197/10000..  Training Loss: 0.257.. \n",
      "Epoch: 198/10000..  Training Loss: 0.258.. \n",
      "Epoch: 199/10000..  Training Loss: 0.258.. \n",
      "Epoch: 200/10000..  Training Loss: 0.257.. \n",
      "Epoch: 201/10000..  Training Loss: 0.257.. \n",
      "Epoch: 202/10000..  Training Loss: 0.257.. \n",
      "Epoch: 203/10000..  Training Loss: 0.256.. \n",
      "Epoch: 204/10000..  Training Loss: 0.256.. \n",
      "Epoch: 205/10000..  Training Loss: 0.255.. \n",
      "Epoch: 206/10000..  Training Loss: 0.255.. \n",
      "Epoch: 207/10000..  Training Loss: 0.255.. \n",
      "Epoch: 208/10000..  Training Loss: 0.255.. \n",
      "Epoch: 209/10000..  Training Loss: 0.255.. \n",
      "Epoch: 210/10000..  Training Loss: 0.255.. \n",
      "Epoch: 211/10000..  Training Loss: 0.255.. \n",
      "Epoch: 212/10000..  Training Loss: 0.256.. \n",
      "Epoch: 213/10000..  Training Loss: 0.257.. \n",
      "Epoch: 214/10000..  Training Loss: 0.259.. \n",
      "Epoch: 215/10000..  Training Loss: 0.262.. \n",
      "Epoch: 216/10000..  Training Loss: 0.261.. \n",
      "Epoch: 217/10000..  Training Loss: 0.259.. \n",
      "Epoch: 218/10000..  Training Loss: 0.255.. \n",
      "Epoch: 219/10000..  Training Loss: 0.253.. \n",
      "Epoch: 220/10000..  Training Loss: 0.255.. \n",
      "Epoch: 221/10000..  Training Loss: 0.257.. \n",
      "Epoch: 222/10000..  Training Loss: 0.257.. \n",
      "Epoch: 223/10000..  Training Loss: 0.254.. \n",
      "Epoch: 224/10000..  Training Loss: 0.252.. \n",
      "Epoch: 225/10000..  Training Loss: 0.252.. \n",
      "Epoch: 226/10000..  Training Loss: 0.253.. \n",
      "Epoch: 227/10000..  Training Loss: 0.254.. \n",
      "Epoch: 228/10000..  Training Loss: 0.253.. \n",
      "Epoch: 229/10000..  Training Loss: 0.252.. \n",
      "Epoch: 230/10000..  Training Loss: 0.251.. \n",
      "Epoch: 231/10000..  Training Loss: 0.251.. \n",
      "Epoch: 232/10000..  Training Loss: 0.252.. \n",
      "Epoch: 233/10000..  Training Loss: 0.252.. \n",
      "Epoch: 234/10000..  Training Loss: 0.252.. \n",
      "Epoch: 235/10000..  Training Loss: 0.250.. \n",
      "Epoch: 236/10000..  Training Loss: 0.250.. \n",
      "Epoch: 237/10000..  Training Loss: 0.249.. \n",
      "Epoch: 238/10000..  Training Loss: 0.249.. \n",
      "Epoch: 239/10000..  Training Loss: 0.249.. \n",
      "Epoch: 240/10000..  Training Loss: 0.250.. \n",
      "Epoch: 241/10000..  Training Loss: 0.250.. \n",
      "Epoch: 242/10000..  Training Loss: 0.249.. \n",
      "Epoch: 243/10000..  Training Loss: 0.249.. \n",
      "Epoch: 244/10000..  Training Loss: 0.248.. \n",
      "Epoch: 245/10000..  Training Loss: 0.248.. \n",
      "Epoch: 246/10000..  Training Loss: 0.248.. \n",
      "Epoch: 247/10000..  Training Loss: 0.247.. \n",
      "Epoch: 248/10000..  Training Loss: 0.247.. \n",
      "Epoch: 249/10000..  Training Loss: 0.247.. \n",
      "Epoch: 250/10000..  Training Loss: 0.247.. \n",
      "Epoch: 251/10000..  Training Loss: 0.246.. \n",
      "Epoch: 252/10000..  Training Loss: 0.246.. \n",
      "Epoch: 253/10000..  Training Loss: 0.247.. \n",
      "Epoch: 254/10000..  Training Loss: 0.247.. \n",
      "Epoch: 255/10000..  Training Loss: 0.248.. \n",
      "Epoch: 256/10000..  Training Loss: 0.249.. \n",
      "Epoch: 257/10000..  Training Loss: 0.252.. \n",
      "Epoch: 258/10000..  Training Loss: 0.256.. \n",
      "Epoch: 259/10000..  Training Loss: 0.256.. \n",
      "Epoch: 260/10000..  Training Loss: 0.254.. \n",
      "Epoch: 261/10000..  Training Loss: 0.248.. \n",
      "Epoch: 262/10000..  Training Loss: 0.245.. \n",
      "Epoch: 263/10000..  Training Loss: 0.247.. \n",
      "Epoch: 264/10000..  Training Loss: 0.251.. \n",
      "Epoch: 265/10000..  Training Loss: 0.252.. \n",
      "Epoch: 266/10000..  Training Loss: 0.248.. \n",
      "Epoch: 267/10000..  Training Loss: 0.245.. \n",
      "Epoch: 268/10000..  Training Loss: 0.245.. \n",
      "Epoch: 269/10000..  Training Loss: 0.247.. \n",
      "Epoch: 270/10000..  Training Loss: 0.248.. \n",
      "Epoch: 271/10000..  Training Loss: 0.247.. \n",
      "Epoch: 272/10000..  Training Loss: 0.244.. \n",
      "Epoch: 273/10000..  Training Loss: 0.243.. \n",
      "Epoch: 274/10000..  Training Loss: 0.244.. \n",
      "Epoch: 275/10000..  Training Loss: 0.245.. \n",
      "Epoch: 276/10000..  Training Loss: 0.245.. \n",
      "Epoch: 277/10000..  Training Loss: 0.244.. \n",
      "Epoch: 278/10000..  Training Loss: 0.243.. \n",
      "Epoch: 279/10000..  Training Loss: 0.242.. \n",
      "Epoch: 280/10000..  Training Loss: 0.243.. \n",
      "Epoch: 281/10000..  Training Loss: 0.243.. \n",
      "Epoch: 282/10000..  Training Loss: 0.244.. \n",
      "Epoch: 283/10000..  Training Loss: 0.243.. \n",
      "Epoch: 284/10000..  Training Loss: 0.242.. \n",
      "Epoch: 285/10000..  Training Loss: 0.241.. \n",
      "Epoch: 286/10000..  Training Loss: 0.241.. \n",
      "Epoch: 287/10000..  Training Loss: 0.241.. \n",
      "Epoch: 288/10000..  Training Loss: 0.241.. \n",
      "Epoch: 289/10000..  Training Loss: 0.242.. \n",
      "Epoch: 290/10000..  Training Loss: 0.242.. \n",
      "Epoch: 291/10000..  Training Loss: 0.241.. \n",
      "Epoch: 292/10000..  Training Loss: 0.241.. \n",
      "Epoch: 293/10000..  Training Loss: 0.241.. \n",
      "Epoch: 294/10000..  Training Loss: 0.240.. \n",
      "Epoch: 295/10000..  Training Loss: 0.240.. \n",
      "Epoch: 296/10000..  Training Loss: 0.239.. \n",
      "Epoch: 297/10000..  Training Loss: 0.239.. \n",
      "Epoch: 298/10000..  Training Loss: 0.239.. \n",
      "Epoch: 299/10000..  Training Loss: 0.239.. \n",
      "Epoch: 300/10000..  Training Loss: 0.239.. \n",
      "Epoch: 301/10000..  Training Loss: 0.240.. \n",
      "Epoch: 302/10000..  Training Loss: 0.240.. \n",
      "Epoch: 303/10000..  Training Loss: 0.241.. \n",
      "Epoch: 304/10000..  Training Loss: 0.244.. \n",
      "Epoch: 305/10000..  Training Loss: 0.246.. \n",
      "Epoch: 306/10000..  Training Loss: 0.249.. \n",
      "Epoch: 307/10000..  Training Loss: 0.246.. \n",
      "Epoch: 308/10000..  Training Loss: 0.242.. \n",
      "Epoch: 309/10000..  Training Loss: 0.238.. \n",
      "Epoch: 310/10000..  Training Loss: 0.239.. \n",
      "Epoch: 311/10000..  Training Loss: 0.242.. \n",
      "Epoch: 312/10000..  Training Loss: 0.244.. \n",
      "Epoch: 313/10000..  Training Loss: 0.244.. \n",
      "Epoch: 314/10000..  Training Loss: 0.240.. \n",
      "Epoch: 315/10000..  Training Loss: 0.237.. \n",
      "Epoch: 316/10000..  Training Loss: 0.237.. \n",
      "Epoch: 317/10000..  Training Loss: 0.240.. \n",
      "Epoch: 318/10000..  Training Loss: 0.241.. \n",
      "Epoch: 319/10000..  Training Loss: 0.240.. \n",
      "Epoch: 320/10000..  Training Loss: 0.237.. \n",
      "Epoch: 321/10000..  Training Loss: 0.236.. \n",
      "Epoch: 322/10000..  Training Loss: 0.236.. \n",
      "Epoch: 323/10000..  Training Loss: 0.238.. \n",
      "Epoch: 324/10000..  Training Loss: 0.238.. \n",
      "Epoch: 325/10000..  Training Loss: 0.238.. \n",
      "Epoch: 326/10000..  Training Loss: 0.236.. \n",
      "Epoch: 327/10000..  Training Loss: 0.235.. \n",
      "Epoch: 328/10000..  Training Loss: 0.235.. \n",
      "Epoch: 329/10000..  Training Loss: 0.236.. \n",
      "Epoch: 330/10000..  Training Loss: 0.236.. \n",
      "Epoch: 331/10000..  Training Loss: 0.237.. \n",
      "Epoch: 332/10000..  Training Loss: 0.236.. \n",
      "Epoch: 333/10000..  Training Loss: 0.235.. \n",
      "Epoch: 334/10000..  Training Loss: 0.234.. \n",
      "Epoch: 335/10000..  Training Loss: 0.234.. \n",
      "Epoch: 336/10000..  Training Loss: 0.234.. \n",
      "Epoch: 337/10000..  Training Loss: 0.234.. \n",
      "Epoch: 338/10000..  Training Loss: 0.235.. \n",
      "Epoch: 339/10000..  Training Loss: 0.235.. \n",
      "Epoch: 340/10000..  Training Loss: 0.235.. \n",
      "Epoch: 341/10000..  Training Loss: 0.235.. \n",
      "Epoch: 342/10000..  Training Loss: 0.235.. \n",
      "Epoch: 343/10000..  Training Loss: 0.234.. \n",
      "Epoch: 344/10000..  Training Loss: 0.233.. \n",
      "Epoch: 345/10000..  Training Loss: 0.233.. \n",
      "Epoch: 346/10000..  Training Loss: 0.233.. \n",
      "Epoch: 347/10000..  Training Loss: 0.232.. \n",
      "Epoch: 348/10000..  Training Loss: 0.232.. \n",
      "Epoch: 349/10000..  Training Loss: 0.233.. \n",
      "Epoch: 350/10000..  Training Loss: 0.233.. \n",
      "Epoch: 351/10000..  Training Loss: 0.234.. \n",
      "Epoch: 352/10000..  Training Loss: 0.235.. \n",
      "Epoch: 353/10000..  Training Loss: 0.236.. \n",
      "Epoch: 354/10000..  Training Loss: 0.237.. \n",
      "Epoch: 355/10000..  Training Loss: 0.237.. \n",
      "Epoch: 356/10000..  Training Loss: 0.236.. \n",
      "Epoch: 357/10000..  Training Loss: 0.234.. \n",
      "Epoch: 358/10000..  Training Loss: 0.232.. \n",
      "Epoch: 359/10000..  Training Loss: 0.231.. \n",
      "Epoch: 360/10000..  Training Loss: 0.231.. \n",
      "Epoch: 361/10000..  Training Loss: 0.231.. \n",
      "Epoch: 362/10000..  Training Loss: 0.232.. \n",
      "Epoch: 363/10000..  Training Loss: 0.233.. \n",
      "Epoch: 364/10000..  Training Loss: 0.234.. \n",
      "Epoch: 365/10000..  Training Loss: 0.235.. \n",
      "Epoch: 366/10000..  Training Loss: 0.235.. \n",
      "Epoch: 367/10000..  Training Loss: 0.234.. \n",
      "Epoch: 368/10000..  Training Loss: 0.231.. \n",
      "Epoch: 369/10000..  Training Loss: 0.230.. \n",
      "Epoch: 370/10000..  Training Loss: 0.230.. \n",
      "Epoch: 371/10000..  Training Loss: 0.231.. \n",
      "Epoch: 372/10000..  Training Loss: 0.232.. \n",
      "Epoch: 373/10000..  Training Loss: 0.233.. \n",
      "Epoch: 374/10000..  Training Loss: 0.234.. \n",
      "Epoch: 375/10000..  Training Loss: 0.234.. \n",
      "Epoch: 376/10000..  Training Loss: 0.232.. \n",
      "Epoch: 377/10000..  Training Loss: 0.230.. \n",
      "Epoch: 378/10000..  Training Loss: 0.229.. \n",
      "Epoch: 379/10000..  Training Loss: 0.229.. \n",
      "Epoch: 380/10000..  Training Loss: 0.229.. \n",
      "Epoch: 381/10000..  Training Loss: 0.230.. \n",
      "Epoch: 382/10000..  Training Loss: 0.231.. \n",
      "Epoch: 383/10000..  Training Loss: 0.232.. \n",
      "Epoch: 384/10000..  Training Loss: 0.232.. \n",
      "Epoch: 385/10000..  Training Loss: 0.231.. \n",
      "Epoch: 386/10000..  Training Loss: 0.230.. \n",
      "Epoch: 387/10000..  Training Loss: 0.228.. \n",
      "Epoch: 388/10000..  Training Loss: 0.228.. \n",
      "Epoch: 389/10000..  Training Loss: 0.227.. \n",
      "Epoch: 390/10000..  Training Loss: 0.227.. \n",
      "Epoch: 391/10000..  Training Loss: 0.228.. \n",
      "Epoch: 392/10000..  Training Loss: 0.228.. \n",
      "Epoch: 393/10000..  Training Loss: 0.229.. \n",
      "Epoch: 394/10000..  Training Loss: 0.230.. \n",
      "Epoch: 395/10000..  Training Loss: 0.231.. \n",
      "Epoch: 396/10000..  Training Loss: 0.232.. \n",
      "Epoch: 397/10000..  Training Loss: 0.231.. \n",
      "Epoch: 398/10000..  Training Loss: 0.230.. \n",
      "Epoch: 399/10000..  Training Loss: 0.228.. \n",
      "Epoch: 400/10000..  Training Loss: 0.227.. \n",
      "Epoch: 401/10000..  Training Loss: 0.226.. \n",
      "Epoch: 402/10000..  Training Loss: 0.226.. \n",
      "Epoch: 403/10000..  Training Loss: 0.227.. \n",
      "Epoch: 404/10000..  Training Loss: 0.229.. \n",
      "Epoch: 405/10000..  Training Loss: 0.230.. \n",
      "Epoch: 406/10000..  Training Loss: 0.230.. \n",
      "Epoch: 407/10000..  Training Loss: 0.229.. \n",
      "Epoch: 408/10000..  Training Loss: 0.228.. \n",
      "Epoch: 409/10000..  Training Loss: 0.226.. \n",
      "Epoch: 410/10000..  Training Loss: 0.225.. \n",
      "Epoch: 411/10000..  Training Loss: 0.225.. \n",
      "Epoch: 412/10000..  Training Loss: 0.225.. \n",
      "Epoch: 413/10000..  Training Loss: 0.226.. \n",
      "Epoch: 414/10000..  Training Loss: 0.227.. \n",
      "Epoch: 415/10000..  Training Loss: 0.228.. \n",
      "Epoch: 416/10000..  Training Loss: 0.230.. \n",
      "Epoch: 417/10000..  Training Loss: 0.230.. \n",
      "Epoch: 418/10000..  Training Loss: 0.230.. \n",
      "Epoch: 419/10000..  Training Loss: 0.227.. \n",
      "Epoch: 420/10000..  Training Loss: 0.225.. \n",
      "Epoch: 421/10000..  Training Loss: 0.224.. \n",
      "Epoch: 422/10000..  Training Loss: 0.224.. \n",
      "Epoch: 423/10000..  Training Loss: 0.226.. \n",
      "Epoch: 424/10000..  Training Loss: 0.227.. \n",
      "Epoch: 425/10000..  Training Loss: 0.228.. \n",
      "Epoch: 426/10000..  Training Loss: 0.227.. \n",
      "Epoch: 427/10000..  Training Loss: 0.226.. \n",
      "Epoch: 428/10000..  Training Loss: 0.225.. \n",
      "Epoch: 429/10000..  Training Loss: 0.223.. \n",
      "Epoch: 430/10000..  Training Loss: 0.223.. \n",
      "Epoch: 431/10000..  Training Loss: 0.223.. \n",
      "Epoch: 432/10000..  Training Loss: 0.224.. \n",
      "Epoch: 433/10000..  Training Loss: 0.225.. \n",
      "Epoch: 434/10000..  Training Loss: 0.226.. \n",
      "Epoch: 435/10000..  Training Loss: 0.227.. \n",
      "Epoch: 436/10000..  Training Loss: 0.227.. \n",
      "Epoch: 437/10000..  Training Loss: 0.226.. \n",
      "Epoch: 438/10000..  Training Loss: 0.224.. \n",
      "Epoch: 439/10000..  Training Loss: 0.222.. \n",
      "Epoch: 440/10000..  Training Loss: 0.222.. \n",
      "Epoch: 441/10000..  Training Loss: 0.222.. \n",
      "Epoch: 442/10000..  Training Loss: 0.223.. \n",
      "Epoch: 443/10000..  Training Loss: 0.225.. \n",
      "Epoch: 444/10000..  Training Loss: 0.226.. \n",
      "Epoch: 445/10000..  Training Loss: 0.226.. \n",
      "Epoch: 446/10000..  Training Loss: 0.225.. \n",
      "Epoch: 447/10000..  Training Loss: 0.223.. \n",
      "Epoch: 448/10000..  Training Loss: 0.222.. \n",
      "Epoch: 449/10000..  Training Loss: 0.221.. \n",
      "Epoch: 450/10000..  Training Loss: 0.221.. \n",
      "Epoch: 451/10000..  Training Loss: 0.222.. \n",
      "Epoch: 452/10000..  Training Loss: 0.223.. \n",
      "Epoch: 453/10000..  Training Loss: 0.224.. \n",
      "Epoch: 454/10000..  Training Loss: 0.225.. \n",
      "Epoch: 455/10000..  Training Loss: 0.224.. \n",
      "Epoch: 456/10000..  Training Loss: 0.223.. \n",
      "Epoch: 457/10000..  Training Loss: 0.222.. \n",
      "Epoch: 458/10000..  Training Loss: 0.221.. \n",
      "Epoch: 459/10000..  Training Loss: 0.220.. \n",
      "Epoch: 460/10000..  Training Loss: 0.220.. \n",
      "Epoch: 461/10000..  Training Loss: 0.221.. \n",
      "Epoch: 462/10000..  Training Loss: 0.222.. \n",
      "Epoch: 463/10000..  Training Loss: 0.223.. \n",
      "Epoch: 464/10000..  Training Loss: 0.223.. \n",
      "Epoch: 465/10000..  Training Loss: 0.223.. \n",
      "Epoch: 466/10000..  Training Loss: 0.223.. \n",
      "Epoch: 467/10000..  Training Loss: 0.222.. \n",
      "Epoch: 468/10000..  Training Loss: 0.221.. \n",
      "Epoch: 469/10000..  Training Loss: 0.219.. \n",
      "Epoch: 470/10000..  Training Loss: 0.219.. \n",
      "Epoch: 471/10000..  Training Loss: 0.219.. \n",
      "Epoch: 472/10000..  Training Loss: 0.220.. \n",
      "Epoch: 473/10000..  Training Loss: 0.221.. \n",
      "Epoch: 474/10000..  Training Loss: 0.222.. \n",
      "Epoch: 475/10000..  Training Loss: 0.223.. \n",
      "Epoch: 476/10000..  Training Loss: 0.224.. \n",
      "Epoch: 477/10000..  Training Loss: 0.223.. \n",
      "Epoch: 478/10000..  Training Loss: 0.222.. \n",
      "Epoch: 479/10000..  Training Loss: 0.220.. \n",
      "Epoch: 480/10000..  Training Loss: 0.218.. \n",
      "Epoch: 481/10000..  Training Loss: 0.218.. \n",
      "Epoch: 482/10000..  Training Loss: 0.218.. \n",
      "Epoch: 483/10000..  Training Loss: 0.219.. \n",
      "Epoch: 484/10000..  Training Loss: 0.220.. \n",
      "Epoch: 485/10000..  Training Loss: 0.222.. \n",
      "Epoch: 486/10000..  Training Loss: 0.222.. \n",
      "Epoch: 487/10000..  Training Loss: 0.222.. \n",
      "Epoch: 488/10000..  Training Loss: 0.220.. \n",
      "Epoch: 489/10000..  Training Loss: 0.219.. \n",
      "Epoch: 490/10000..  Training Loss: 0.217.. \n",
      "Epoch: 491/10000..  Training Loss: 0.217.. \n",
      "Epoch: 492/10000..  Training Loss: 0.217.. \n",
      "Epoch: 493/10000..  Training Loss: 0.218.. \n",
      "Epoch: 494/10000..  Training Loss: 0.220.. \n",
      "Epoch: 495/10000..  Training Loss: 0.221.. \n",
      "Epoch: 496/10000..  Training Loss: 0.223.. \n",
      "Epoch: 497/10000..  Training Loss: 0.222.. \n",
      "Epoch: 498/10000..  Training Loss: 0.220.. \n",
      "Epoch: 499/10000..  Training Loss: 0.218.. \n",
      "Epoch: 500/10000..  Training Loss: 0.216.. \n",
      "Epoch: 501/10000..  Training Loss: 0.216.. \n",
      "Epoch: 502/10000..  Training Loss: 0.217.. \n",
      "Epoch: 503/10000..  Training Loss: 0.219.. \n",
      "Epoch: 504/10000..  Training Loss: 0.220.. \n",
      "Epoch: 505/10000..  Training Loss: 0.221.. \n",
      "Epoch: 506/10000..  Training Loss: 0.220.. \n",
      "Epoch: 507/10000..  Training Loss: 0.219.. \n",
      "Epoch: 508/10000..  Training Loss: 0.217.. \n",
      "Epoch: 509/10000..  Training Loss: 0.216.. \n",
      "Epoch: 510/10000..  Training Loss: 0.215.. \n",
      "Epoch: 511/10000..  Training Loss: 0.216.. \n",
      "Epoch: 512/10000..  Training Loss: 0.217.. \n",
      "Epoch: 513/10000..  Training Loss: 0.218.. \n",
      "Epoch: 514/10000..  Training Loss: 0.220.. \n",
      "Epoch: 515/10000..  Training Loss: 0.220.. \n",
      "Epoch: 516/10000..  Training Loss: 0.221.. \n",
      "Epoch: 517/10000..  Training Loss: 0.218.. \n",
      "Epoch: 518/10000..  Training Loss: 0.216.. \n",
      "Epoch: 519/10000..  Training Loss: 0.215.. \n",
      "Epoch: 520/10000..  Training Loss: 0.215.. \n",
      "Epoch: 521/10000..  Training Loss: 0.216.. \n",
      "Epoch: 522/10000..  Training Loss: 0.217.. \n",
      "Epoch: 523/10000..  Training Loss: 0.219.. \n",
      "Epoch: 524/10000..  Training Loss: 0.219.. \n",
      "Epoch: 525/10000..  Training Loss: 0.218.. \n",
      "Epoch: 526/10000..  Training Loss: 0.216.. \n",
      "Epoch: 527/10000..  Training Loss: 0.215.. \n",
      "Epoch: 528/10000..  Training Loss: 0.214.. \n",
      "Epoch: 529/10000..  Training Loss: 0.214.. \n",
      "Epoch: 530/10000..  Training Loss: 0.214.. \n",
      "Epoch: 531/10000..  Training Loss: 0.215.. \n",
      "Epoch: 532/10000..  Training Loss: 0.216.. \n",
      "Epoch: 533/10000..  Training Loss: 0.217.. \n",
      "Epoch: 534/10000..  Training Loss: 0.217.. \n",
      "Epoch: 535/10000..  Training Loss: 0.216.. \n",
      "Epoch: 536/10000..  Training Loss: 0.215.. \n",
      "Epoch: 537/10000..  Training Loss: 0.214.. \n",
      "Epoch: 538/10000..  Training Loss: 0.213.. \n",
      "Epoch: 539/10000..  Training Loss: 0.213.. \n",
      "Epoch: 540/10000..  Training Loss: 0.213.. \n",
      "Epoch: 541/10000..  Training Loss: 0.213.. \n",
      "Epoch: 542/10000..  Training Loss: 0.214.. \n",
      "Epoch: 543/10000..  Training Loss: 0.215.. \n",
      "Epoch: 544/10000..  Training Loss: 0.216.. \n",
      "Epoch: 545/10000..  Training Loss: 0.217.. \n",
      "Epoch: 546/10000..  Training Loss: 0.216.. \n",
      "Epoch: 547/10000..  Training Loss: 0.216.. \n",
      "Epoch: 548/10000..  Training Loss: 0.214.. \n",
      "Epoch: 549/10000..  Training Loss: 0.213.. \n",
      "Epoch: 550/10000..  Training Loss: 0.212.. \n",
      "Epoch: 551/10000..  Training Loss: 0.212.. \n",
      "Epoch: 552/10000..  Training Loss: 0.212.. \n",
      "Epoch: 553/10000..  Training Loss: 0.213.. \n",
      "Epoch: 554/10000..  Training Loss: 0.214.. \n",
      "Epoch: 555/10000..  Training Loss: 0.215.. \n",
      "Epoch: 556/10000..  Training Loss: 0.217.. \n",
      "Epoch: 557/10000..  Training Loss: 0.217.. \n",
      "Epoch: 558/10000..  Training Loss: 0.217.. \n",
      "Epoch: 559/10000..  Training Loss: 0.214.. \n",
      "Epoch: 560/10000..  Training Loss: 0.213.. \n",
      "Epoch: 561/10000..  Training Loss: 0.211.. \n",
      "Epoch: 562/10000..  Training Loss: 0.211.. \n",
      "Epoch: 563/10000..  Training Loss: 0.211.. \n",
      "Epoch: 564/10000..  Training Loss: 0.212.. \n",
      "Epoch: 565/10000..  Training Loss: 0.213.. \n",
      "Epoch: 566/10000..  Training Loss: 0.214.. \n",
      "Epoch: 567/10000..  Training Loss: 0.216.. \n",
      "Epoch: 568/10000..  Training Loss: 0.216.. \n",
      "Epoch: 569/10000..  Training Loss: 0.215.. \n",
      "Epoch: 570/10000..  Training Loss: 0.213.. \n",
      "Epoch: 571/10000..  Training Loss: 0.212.. \n",
      "Epoch: 572/10000..  Training Loss: 0.211.. \n",
      "Epoch: 573/10000..  Training Loss: 0.210.. \n",
      "Epoch: 574/10000..  Training Loss: 0.210.. \n",
      "Epoch: 575/10000..  Training Loss: 0.211.. \n",
      "Epoch: 576/10000..  Training Loss: 0.212.. \n",
      "Epoch: 577/10000..  Training Loss: 0.213.. \n",
      "Epoch: 578/10000..  Training Loss: 0.214.. \n",
      "Epoch: 579/10000..  Training Loss: 0.214.. \n",
      "Epoch: 580/10000..  Training Loss: 0.215.. \n",
      "Epoch: 581/10000..  Training Loss: 0.213.. \n",
      "Epoch: 582/10000..  Training Loss: 0.212.. \n",
      "Epoch: 583/10000..  Training Loss: 0.210.. \n",
      "Epoch: 584/10000..  Training Loss: 0.209.. \n",
      "Epoch: 585/10000..  Training Loss: 0.210.. \n",
      "Epoch: 586/10000..  Training Loss: 0.210.. \n",
      "Epoch: 587/10000..  Training Loss: 0.211.. \n",
      "Epoch: 588/10000..  Training Loss: 0.213.. \n",
      "Epoch: 589/10000..  Training Loss: 0.215.. \n",
      "Epoch: 590/10000..  Training Loss: 0.216.. \n",
      "Epoch: 591/10000..  Training Loss: 0.216.. \n",
      "Epoch: 592/10000..  Training Loss: 0.213.. \n",
      "Epoch: 593/10000..  Training Loss: 0.211.. \n",
      "Epoch: 594/10000..  Training Loss: 0.209.. \n",
      "Epoch: 595/10000..  Training Loss: 0.209.. \n",
      "Epoch: 596/10000..  Training Loss: 0.210.. \n",
      "Epoch: 597/10000..  Training Loss: 0.211.. \n",
      "Epoch: 598/10000..  Training Loss: 0.214.. \n",
      "Epoch: 599/10000..  Training Loss: 0.214.. \n",
      "Epoch: 600/10000..  Training Loss: 0.216.. \n",
      "Epoch: 601/10000..  Training Loss: 0.213.. \n",
      "Epoch: 602/10000..  Training Loss: 0.211.. \n",
      "Epoch: 603/10000..  Training Loss: 0.209.. \n",
      "Epoch: 604/10000..  Training Loss: 0.209.. \n",
      "Epoch: 605/10000..  Training Loss: 0.210.. \n",
      "Epoch: 606/10000..  Training Loss: 0.212.. \n",
      "Epoch: 607/10000..  Training Loss: 0.213.. \n",
      "Epoch: 608/10000..  Training Loss: 0.213.. \n",
      "Epoch: 609/10000..  Training Loss: 0.212.. \n",
      "Epoch: 610/10000..  Training Loss: 0.210.. \n",
      "Epoch: 611/10000..  Training Loss: 0.208.. \n",
      "Epoch: 612/10000..  Training Loss: 0.208.. \n",
      "Epoch: 613/10000..  Training Loss: 0.208.. \n",
      "Epoch: 614/10000..  Training Loss: 0.209.. \n",
      "Epoch: 615/10000..  Training Loss: 0.210.. \n",
      "Epoch: 616/10000..  Training Loss: 0.210.. \n",
      "Epoch: 617/10000..  Training Loss: 0.210.. \n",
      "Epoch: 618/10000..  Training Loss: 0.209.. \n",
      "Epoch: 619/10000..  Training Loss: 0.208.. \n",
      "Epoch: 620/10000..  Training Loss: 0.208.. \n",
      "Epoch: 621/10000..  Training Loss: 0.207.. \n",
      "Epoch: 622/10000..  Training Loss: 0.207.. \n",
      "Epoch: 623/10000..  Training Loss: 0.207.. \n",
      "Epoch: 624/10000..  Training Loss: 0.207.. \n",
      "Epoch: 625/10000..  Training Loss: 0.207.. \n",
      "Epoch: 626/10000..  Training Loss: 0.208.. \n",
      "Epoch: 627/10000..  Training Loss: 0.208.. \n",
      "Epoch: 628/10000..  Training Loss: 0.209.. \n",
      "Epoch: 629/10000..  Training Loss: 0.210.. \n",
      "Epoch: 630/10000..  Training Loss: 0.210.. \n",
      "Epoch: 631/10000..  Training Loss: 0.211.. \n",
      "Epoch: 632/10000..  Training Loss: 0.210.. \n",
      "Epoch: 633/10000..  Training Loss: 0.210.. \n",
      "Epoch: 634/10000..  Training Loss: 0.208.. \n",
      "Epoch: 635/10000..  Training Loss: 0.207.. \n",
      "Epoch: 636/10000..  Training Loss: 0.206.. \n",
      "Epoch: 637/10000..  Training Loss: 0.206.. \n",
      "Epoch: 638/10000..  Training Loss: 0.206.. \n",
      "Epoch: 639/10000..  Training Loss: 0.207.. \n",
      "Epoch: 640/10000..  Training Loss: 0.208.. \n",
      "Epoch: 641/10000..  Training Loss: 0.209.. \n",
      "Epoch: 642/10000..  Training Loss: 0.211.. \n",
      "Epoch: 643/10000..  Training Loss: 0.211.. \n",
      "Epoch: 644/10000..  Training Loss: 0.212.. \n",
      "Epoch: 645/10000..  Training Loss: 0.210.. \n",
      "Epoch: 646/10000..  Training Loss: 0.208.. \n",
      "Epoch: 647/10000..  Training Loss: 0.206.. \n",
      "Epoch: 648/10000..  Training Loss: 0.205.. \n",
      "Epoch: 649/10000..  Training Loss: 0.206.. \n",
      "Epoch: 650/10000..  Training Loss: 0.207.. \n",
      "Epoch: 651/10000..  Training Loss: 0.208.. \n",
      "Epoch: 652/10000..  Training Loss: 0.209.. \n",
      "Epoch: 653/10000..  Training Loss: 0.210.. \n",
      "Epoch: 654/10000..  Training Loss: 0.210.. \n",
      "Epoch: 655/10000..  Training Loss: 0.209.. \n",
      "Epoch: 656/10000..  Training Loss: 0.207.. \n",
      "Epoch: 657/10000..  Training Loss: 0.206.. \n",
      "Epoch: 658/10000..  Training Loss: 0.205.. \n",
      "Epoch: 659/10000..  Training Loss: 0.205.. \n",
      "Epoch: 660/10000..  Training Loss: 0.205.. \n",
      "Epoch: 661/10000..  Training Loss: 0.206.. \n",
      "Epoch: 662/10000..  Training Loss: 0.207.. \n",
      "Epoch: 663/10000..  Training Loss: 0.208.. \n",
      "Epoch: 664/10000..  Training Loss: 0.209.. \n",
      "Epoch: 665/10000..  Training Loss: 0.209.. \n",
      "Epoch: 666/10000..  Training Loss: 0.209.. \n",
      "Epoch: 667/10000..  Training Loss: 0.207.. \n",
      "Epoch: 668/10000..  Training Loss: 0.206.. \n",
      "Epoch: 669/10000..  Training Loss: 0.205.. \n",
      "Epoch: 670/10000..  Training Loss: 0.204.. \n",
      "Epoch: 671/10000..  Training Loss: 0.204.. \n",
      "Epoch: 672/10000..  Training Loss: 0.205.. \n",
      "Epoch: 673/10000..  Training Loss: 0.206.. \n",
      "Epoch: 674/10000..  Training Loss: 0.207.. \n",
      "Epoch: 675/10000..  Training Loss: 0.208.. \n",
      "Epoch: 676/10000..  Training Loss: 0.209.. \n",
      "Epoch: 677/10000..  Training Loss: 0.210.. \n",
      "Epoch: 678/10000..  Training Loss: 0.209.. \n",
      "Epoch: 679/10000..  Training Loss: 0.208.. \n",
      "Epoch: 680/10000..  Training Loss: 0.206.. \n",
      "Epoch: 681/10000..  Training Loss: 0.204.. \n",
      "Epoch: 682/10000..  Training Loss: 0.204.. \n",
      "Epoch: 683/10000..  Training Loss: 0.204.. \n",
      "Epoch: 684/10000..  Training Loss: 0.205.. \n",
      "Epoch: 685/10000..  Training Loss: 0.206.. \n",
      "Epoch: 686/10000..  Training Loss: 0.207.. \n",
      "Epoch: 687/10000..  Training Loss: 0.207.. \n",
      "Epoch: 688/10000..  Training Loss: 0.208.. \n",
      "Epoch: 689/10000..  Training Loss: 0.207.. \n",
      "Epoch: 690/10000..  Training Loss: 0.206.. \n",
      "Epoch: 691/10000..  Training Loss: 0.204.. \n",
      "Epoch: 692/10000..  Training Loss: 0.204.. \n",
      "Epoch: 693/10000..  Training Loss: 0.203.. \n",
      "Epoch: 694/10000..  Training Loss: 0.203.. \n",
      "Epoch: 695/10000..  Training Loss: 0.203.. \n",
      "Epoch: 696/10000..  Training Loss: 0.203.. \n",
      "Epoch: 697/10000..  Training Loss: 0.204.. \n",
      "Epoch: 698/10000..  Training Loss: 0.204.. \n",
      "Epoch: 699/10000..  Training Loss: 0.205.. \n",
      "Epoch: 700/10000..  Training Loss: 0.206.. \n",
      "Epoch: 701/10000..  Training Loss: 0.206.. \n",
      "Epoch: 702/10000..  Training Loss: 0.206.. \n",
      "Epoch: 703/10000..  Training Loss: 0.207.. \n",
      "Epoch: 704/10000..  Training Loss: 0.206.. \n",
      "Epoch: 705/10000..  Training Loss: 0.205.. \n",
      "Epoch: 706/10000..  Training Loss: 0.204.. \n",
      "Epoch: 707/10000..  Training Loss: 0.203.. \n",
      "Epoch: 708/10000..  Training Loss: 0.202.. \n",
      "Epoch: 709/10000..  Training Loss: 0.202.. \n",
      "Epoch: 710/10000..  Training Loss: 0.202.. \n",
      "Epoch: 711/10000..  Training Loss: 0.203.. \n",
      "Epoch: 712/10000..  Training Loss: 0.204.. \n",
      "Epoch: 713/10000..  Training Loss: 0.205.. \n",
      "Epoch: 714/10000..  Training Loss: 0.208.. \n",
      "Epoch: 715/10000..  Training Loss: 0.209.. \n",
      "Epoch: 716/10000..  Training Loss: 0.210.. \n",
      "Epoch: 717/10000..  Training Loss: 0.207.. \n",
      "Epoch: 718/10000..  Training Loss: 0.204.. \n",
      "Epoch: 719/10000..  Training Loss: 0.202.. \n",
      "Epoch: 720/10000..  Training Loss: 0.202.. \n",
      "Epoch: 721/10000..  Training Loss: 0.203.. \n",
      "Epoch: 722/10000..  Training Loss: 0.204.. \n",
      "Epoch: 723/10000..  Training Loss: 0.207.. \n",
      "Epoch: 724/10000..  Training Loss: 0.208.. \n",
      "Epoch: 725/10000..  Training Loss: 0.209.. \n",
      "Epoch: 726/10000..  Training Loss: 0.206.. \n",
      "Epoch: 727/10000..  Training Loss: 0.204.. \n",
      "Epoch: 728/10000..  Training Loss: 0.202.. \n",
      "Epoch: 729/10000..  Training Loss: 0.201.. \n",
      "Epoch: 730/10000..  Training Loss: 0.202.. \n",
      "Epoch: 731/10000..  Training Loss: 0.204.. \n",
      "Epoch: 732/10000..  Training Loss: 0.206.. \n",
      "Epoch: 733/10000..  Training Loss: 0.207.. \n",
      "Epoch: 734/10000..  Training Loss: 0.207.. \n",
      "Epoch: 735/10000..  Training Loss: 0.205.. \n",
      "Epoch: 736/10000..  Training Loss: 0.203.. \n",
      "Epoch: 737/10000..  Training Loss: 0.201.. \n",
      "Epoch: 738/10000..  Training Loss: 0.201.. \n",
      "Epoch: 739/10000..  Training Loss: 0.201.. \n",
      "Epoch: 740/10000..  Training Loss: 0.202.. \n",
      "Epoch: 741/10000..  Training Loss: 0.204.. \n",
      "Epoch: 742/10000..  Training Loss: 0.204.. \n",
      "Epoch: 743/10000..  Training Loss: 0.204.. \n",
      "Epoch: 744/10000..  Training Loss: 0.204.. \n",
      "Epoch: 745/10000..  Training Loss: 0.204.. \n",
      "Epoch: 746/10000..  Training Loss: 0.202.. \n",
      "Epoch: 747/10000..  Training Loss: 0.201.. \n",
      "Epoch: 748/10000..  Training Loss: 0.200.. \n",
      "Epoch: 749/10000..  Training Loss: 0.200.. \n",
      "Epoch: 750/10000..  Training Loss: 0.201.. \n",
      "Epoch: 751/10000..  Training Loss: 0.201.. \n",
      "Epoch: 752/10000..  Training Loss: 0.202.. \n",
      "Epoch: 753/10000..  Training Loss: 0.203.. \n",
      "Epoch: 754/10000..  Training Loss: 0.204.. \n",
      "Epoch: 755/10000..  Training Loss: 0.204.. \n",
      "Epoch: 756/10000..  Training Loss: 0.204.. \n",
      "Epoch: 757/10000..  Training Loss: 0.203.. \n",
      "Epoch: 758/10000..  Training Loss: 0.202.. \n",
      "Epoch: 759/10000..  Training Loss: 0.201.. \n",
      "Epoch: 760/10000..  Training Loss: 0.200.. \n",
      "Epoch: 761/10000..  Training Loss: 0.200.. \n",
      "Epoch: 762/10000..  Training Loss: 0.200.. \n",
      "Epoch: 763/10000..  Training Loss: 0.200.. \n",
      "Epoch: 764/10000..  Training Loss: 0.200.. \n",
      "Epoch: 765/10000..  Training Loss: 0.201.. \n",
      "Epoch: 766/10000..  Training Loss: 0.201.. \n",
      "Epoch: 767/10000..  Training Loss: 0.202.. \n",
      "Epoch: 768/10000..  Training Loss: 0.203.. \n",
      "Epoch: 769/10000..  Training Loss: 0.203.. \n",
      "Epoch: 770/10000..  Training Loss: 0.203.. \n",
      "Epoch: 771/10000..  Training Loss: 0.203.. \n",
      "Epoch: 772/10000..  Training Loss: 0.202.. \n",
      "Epoch: 773/10000..  Training Loss: 0.201.. \n",
      "Epoch: 774/10000..  Training Loss: 0.200.. \n",
      "Epoch: 775/10000..  Training Loss: 0.199.. \n",
      "Epoch: 776/10000..  Training Loss: 0.199.. \n",
      "Epoch: 777/10000..  Training Loss: 0.199.. \n",
      "Epoch: 778/10000..  Training Loss: 0.199.. \n",
      "Epoch: 779/10000..  Training Loss: 0.199.. \n",
      "Epoch: 780/10000..  Training Loss: 0.200.. \n",
      "Epoch: 781/10000..  Training Loss: 0.200.. \n",
      "Epoch: 782/10000..  Training Loss: 0.202.. \n",
      "Epoch: 783/10000..  Training Loss: 0.203.. \n",
      "Epoch: 784/10000..  Training Loss: 0.205.. \n",
      "Epoch: 785/10000..  Training Loss: 0.206.. \n",
      "Epoch: 786/10000..  Training Loss: 0.206.. \n",
      "Epoch: 787/10000..  Training Loss: 0.204.. \n",
      "Epoch: 788/10000..  Training Loss: 0.201.. \n",
      "Epoch: 789/10000..  Training Loss: 0.199.. \n",
      "Epoch: 790/10000..  Training Loss: 0.198.. \n",
      "Epoch: 791/10000..  Training Loss: 0.199.. \n",
      "Epoch: 792/10000..  Training Loss: 0.200.. \n",
      "Epoch: 793/10000..  Training Loss: 0.202.. \n",
      "Epoch: 794/10000..  Training Loss: 0.204.. \n",
      "Epoch: 795/10000..  Training Loss: 0.206.. \n",
      "Epoch: 796/10000..  Training Loss: 0.205.. \n",
      "Epoch: 797/10000..  Training Loss: 0.203.. \n",
      "Epoch: 798/10000..  Training Loss: 0.200.. \n",
      "Epoch: 799/10000..  Training Loss: 0.199.. \n",
      "Epoch: 800/10000..  Training Loss: 0.198.. \n",
      "Epoch: 801/10000..  Training Loss: 0.199.. \n",
      "Epoch: 802/10000..  Training Loss: 0.201.. \n",
      "Epoch: 803/10000..  Training Loss: 0.202.. \n",
      "Epoch: 804/10000..  Training Loss: 0.204.. \n",
      "Epoch: 805/10000..  Training Loss: 0.204.. \n",
      "Epoch: 806/10000..  Training Loss: 0.203.. \n",
      "Epoch: 807/10000..  Training Loss: 0.200.. \n",
      "Epoch: 808/10000..  Training Loss: 0.198.. \n",
      "Epoch: 809/10000..  Training Loss: 0.198.. \n",
      "Epoch: 810/10000..  Training Loss: 0.198.. \n",
      "Epoch: 811/10000..  Training Loss: 0.199.. \n",
      "Epoch: 812/10000..  Training Loss: 0.200.. \n",
      "Epoch: 813/10000..  Training Loss: 0.201.. \n",
      "Epoch: 814/10000..  Training Loss: 0.201.. \n",
      "Epoch: 815/10000..  Training Loss: 0.201.. \n",
      "Epoch: 816/10000..  Training Loss: 0.200.. \n",
      "Epoch: 817/10000..  Training Loss: 0.199.. \n",
      "Epoch: 818/10000..  Training Loss: 0.198.. \n",
      "Epoch: 819/10000..  Training Loss: 0.197.. \n",
      "Epoch: 820/10000..  Training Loss: 0.197.. \n",
      "Epoch: 821/10000..  Training Loss: 0.198.. \n",
      "Epoch: 822/10000..  Training Loss: 0.198.. \n",
      "Epoch: 823/10000..  Training Loss: 0.199.. \n",
      "Epoch: 824/10000..  Training Loss: 0.200.. \n",
      "Epoch: 825/10000..  Training Loss: 0.200.. \n",
      "Epoch: 826/10000..  Training Loss: 0.201.. \n",
      "Epoch: 827/10000..  Training Loss: 0.201.. \n",
      "Epoch: 828/10000..  Training Loss: 0.200.. \n",
      "Epoch: 829/10000..  Training Loss: 0.199.. \n",
      "Epoch: 830/10000..  Training Loss: 0.198.. \n",
      "Epoch: 831/10000..  Training Loss: 0.197.. \n",
      "Epoch: 832/10000..  Training Loss: 0.197.. \n",
      "Epoch: 833/10000..  Training Loss: 0.197.. \n",
      "Epoch: 834/10000..  Training Loss: 0.198.. \n",
      "Epoch: 835/10000..  Training Loss: 0.199.. \n",
      "Epoch: 836/10000..  Training Loss: 0.200.. \n",
      "Epoch: 837/10000..  Training Loss: 0.201.. \n",
      "Epoch: 838/10000..  Training Loss: 0.202.. \n",
      "Epoch: 839/10000..  Training Loss: 0.202.. \n",
      "Epoch: 840/10000..  Training Loss: 0.200.. \n",
      "Epoch: 841/10000..  Training Loss: 0.198.. \n",
      "Epoch: 842/10000..  Training Loss: 0.197.. \n",
      "Epoch: 843/10000..  Training Loss: 0.196.. \n",
      "Epoch: 844/10000..  Training Loss: 0.197.. \n",
      "Epoch: 845/10000..  Training Loss: 0.197.. \n",
      "Epoch: 846/10000..  Training Loss: 0.198.. \n",
      "Epoch: 847/10000..  Training Loss: 0.199.. \n",
      "Epoch: 848/10000..  Training Loss: 0.201.. \n",
      "Epoch: 849/10000..  Training Loss: 0.201.. \n",
      "Epoch: 850/10000..  Training Loss: 0.201.. \n",
      "Epoch: 851/10000..  Training Loss: 0.199.. \n",
      "Epoch: 852/10000..  Training Loss: 0.198.. \n",
      "Epoch: 853/10000..  Training Loss: 0.197.. \n",
      "Epoch: 854/10000..  Training Loss: 0.196.. \n",
      "Epoch: 855/10000..  Training Loss: 0.196.. \n",
      "Epoch: 856/10000..  Training Loss: 0.196.. \n",
      "Epoch: 857/10000..  Training Loss: 0.197.. \n",
      "Epoch: 858/10000..  Training Loss: 0.198.. \n",
      "Epoch: 859/10000..  Training Loss: 0.199.. \n",
      "Epoch: 860/10000..  Training Loss: 0.200.. \n",
      "Epoch: 861/10000..  Training Loss: 0.201.. \n",
      "Epoch: 862/10000..  Training Loss: 0.200.. \n",
      "Epoch: 863/10000..  Training Loss: 0.199.. \n",
      "Epoch: 864/10000..  Training Loss: 0.198.. \n",
      "Epoch: 865/10000..  Training Loss: 0.197.. \n",
      "Epoch: 866/10000..  Training Loss: 0.196.. \n",
      "Epoch: 867/10000..  Training Loss: 0.195.. \n",
      "Epoch: 868/10000..  Training Loss: 0.195.. \n",
      "Epoch: 869/10000..  Training Loss: 0.196.. \n",
      "Epoch: 870/10000..  Training Loss: 0.196.. \n",
      "Epoch: 871/10000..  Training Loss: 0.197.. \n",
      "Epoch: 872/10000..  Training Loss: 0.198.. \n",
      "Epoch: 873/10000..  Training Loss: 0.199.. \n",
      "Epoch: 874/10000..  Training Loss: 0.201.. \n",
      "Epoch: 875/10000..  Training Loss: 0.201.. \n",
      "Epoch: 876/10000..  Training Loss: 0.202.. \n",
      "Epoch: 877/10000..  Training Loss: 0.200.. \n",
      "Epoch: 878/10000..  Training Loss: 0.198.. \n",
      "Epoch: 879/10000..  Training Loss: 0.196.. \n",
      "Epoch: 880/10000..  Training Loss: 0.195.. \n",
      "Epoch: 881/10000..  Training Loss: 0.195.. \n",
      "Epoch: 882/10000..  Training Loss: 0.196.. \n",
      "Epoch: 883/10000..  Training Loss: 0.197.. \n",
      "Epoch: 884/10000..  Training Loss: 0.199.. \n",
      "Epoch: 885/10000..  Training Loss: 0.201.. \n",
      "Epoch: 886/10000..  Training Loss: 0.201.. \n",
      "Epoch: 887/10000..  Training Loss: 0.201.. \n",
      "Epoch: 888/10000..  Training Loss: 0.198.. \n",
      "Epoch: 889/10000..  Training Loss: 0.196.. \n",
      "Epoch: 890/10000..  Training Loss: 0.195.. \n",
      "Epoch: 891/10000..  Training Loss: 0.195.. \n",
      "Epoch: 892/10000..  Training Loss: 0.195.. \n",
      "Epoch: 893/10000..  Training Loss: 0.197.. \n",
      "Epoch: 894/10000..  Training Loss: 0.199.. \n",
      "Epoch: 895/10000..  Training Loss: 0.199.. \n",
      "Epoch: 896/10000..  Training Loss: 0.200.. \n",
      "Epoch: 897/10000..  Training Loss: 0.199.. \n",
      "Epoch: 898/10000..  Training Loss: 0.197.. \n",
      "Epoch: 899/10000..  Training Loss: 0.195.. \n",
      "Epoch: 900/10000..  Training Loss: 0.194.. \n",
      "Epoch: 901/10000..  Training Loss: 0.194.. \n",
      "Epoch: 902/10000..  Training Loss: 0.195.. \n",
      "Epoch: 903/10000..  Training Loss: 0.196.. \n",
      "Epoch: 904/10000..  Training Loss: 0.197.. \n",
      "Epoch: 905/10000..  Training Loss: 0.198.. \n",
      "Epoch: 906/10000..  Training Loss: 0.198.. \n",
      "Epoch: 907/10000..  Training Loss: 0.198.. \n",
      "Epoch: 908/10000..  Training Loss: 0.197.. \n",
      "Epoch: 909/10000..  Training Loss: 0.196.. \n",
      "Epoch: 910/10000..  Training Loss: 0.195.. \n",
      "Epoch: 911/10000..  Training Loss: 0.194.. \n",
      "Epoch: 912/10000..  Training Loss: 0.194.. \n",
      "Epoch: 913/10000..  Training Loss: 0.194.. \n",
      "Epoch: 914/10000..  Training Loss: 0.194.. \n",
      "Epoch: 915/10000..  Training Loss: 0.194.. \n",
      "Epoch: 916/10000..  Training Loss: 0.195.. \n",
      "Epoch: 917/10000..  Training Loss: 0.195.. \n",
      "Epoch: 918/10000..  Training Loss: 0.196.. \n",
      "Epoch: 919/10000..  Training Loss: 0.197.. \n",
      "Epoch: 920/10000..  Training Loss: 0.199.. \n",
      "Epoch: 921/10000..  Training Loss: 0.199.. \n",
      "Epoch: 922/10000..  Training Loss: 0.198.. \n",
      "Epoch: 923/10000..  Training Loss: 0.197.. \n",
      "Epoch: 924/10000..  Training Loss: 0.195.. \n",
      "Epoch: 925/10000..  Training Loss: 0.194.. \n",
      "Epoch: 926/10000..  Training Loss: 0.193.. \n",
      "Epoch: 927/10000..  Training Loss: 0.193.. \n",
      "Epoch: 928/10000..  Training Loss: 0.194.. \n",
      "Epoch: 929/10000..  Training Loss: 0.195.. \n",
      "Epoch: 930/10000..  Training Loss: 0.197.. \n",
      "Epoch: 931/10000..  Training Loss: 0.199.. \n",
      "Epoch: 932/10000..  Training Loss: 0.200.. \n",
      "Epoch: 933/10000..  Training Loss: 0.200.. \n",
      "Epoch: 934/10000..  Training Loss: 0.197.. \n",
      "Epoch: 935/10000..  Training Loss: 0.195.. \n",
      "Epoch: 936/10000..  Training Loss: 0.193.. \n",
      "Epoch: 937/10000..  Training Loss: 0.193.. \n",
      "Epoch: 938/10000..  Training Loss: 0.193.. \n",
      "Epoch: 939/10000..  Training Loss: 0.195.. \n",
      "Epoch: 940/10000..  Training Loss: 0.196.. \n",
      "Epoch: 941/10000..  Training Loss: 0.197.. \n",
      "Epoch: 942/10000..  Training Loss: 0.199.. \n",
      "Epoch: 943/10000..  Training Loss: 0.198.. \n",
      "Epoch: 944/10000..  Training Loss: 0.198.. \n",
      "Epoch: 945/10000..  Training Loss: 0.195.. \n",
      "Epoch: 946/10000..  Training Loss: 0.194.. \n",
      "Epoch: 947/10000..  Training Loss: 0.193.. \n",
      "Epoch: 948/10000..  Training Loss: 0.193.. \n",
      "Epoch: 949/10000..  Training Loss: 0.193.. \n",
      "Epoch: 950/10000..  Training Loss: 0.194.. \n",
      "Epoch: 951/10000..  Training Loss: 0.195.. \n",
      "Epoch: 952/10000..  Training Loss: 0.196.. \n",
      "Epoch: 953/10000..  Training Loss: 0.196.. \n",
      "Epoch: 954/10000..  Training Loss: 0.196.. \n",
      "Epoch: 955/10000..  Training Loss: 0.196.. \n",
      "Epoch: 956/10000..  Training Loss: 0.195.. \n",
      "Epoch: 957/10000..  Training Loss: 0.193.. \n",
      "Epoch: 958/10000..  Training Loss: 0.193.. \n",
      "Epoch: 959/10000..  Training Loss: 0.192.. \n",
      "Epoch: 960/10000..  Training Loss: 0.192.. \n",
      "Epoch: 961/10000..  Training Loss: 0.192.. \n",
      "Epoch: 962/10000..  Training Loss: 0.192.. \n",
      "Epoch: 963/10000..  Training Loss: 0.193.. \n",
      "Epoch: 964/10000..  Training Loss: 0.194.. \n",
      "Epoch: 965/10000..  Training Loss: 0.195.. \n",
      "Epoch: 966/10000..  Training Loss: 0.196.. \n",
      "Epoch: 967/10000..  Training Loss: 0.197.. \n",
      "Epoch: 968/10000..  Training Loss: 0.198.. \n",
      "Epoch: 969/10000..  Training Loss: 0.197.. \n",
      "Epoch: 970/10000..  Training Loss: 0.195.. \n",
      "Epoch: 971/10000..  Training Loss: 0.193.. \n",
      "Epoch: 972/10000..  Training Loss: 0.192.. \n",
      "Epoch: 973/10000..  Training Loss: 0.192.. \n",
      "Epoch: 974/10000..  Training Loss: 0.192.. \n",
      "Epoch: 975/10000..  Training Loss: 0.193.. \n",
      "Epoch: 976/10000..  Training Loss: 0.195.. \n",
      "Epoch: 977/10000..  Training Loss: 0.196.. \n",
      "Epoch: 978/10000..  Training Loss: 0.196.. \n",
      "Epoch: 979/10000..  Training Loss: 0.197.. \n",
      "Epoch: 980/10000..  Training Loss: 0.195.. \n",
      "Epoch: 981/10000..  Training Loss: 0.194.. \n",
      "Epoch: 982/10000..  Training Loss: 0.192.. \n",
      "Epoch: 983/10000..  Training Loss: 0.192.. \n",
      "Epoch: 984/10000..  Training Loss: 0.192.. \n",
      "Epoch: 985/10000..  Training Loss: 0.192.. \n",
      "Epoch: 986/10000..  Training Loss: 0.192.. \n",
      "Epoch: 987/10000..  Training Loss: 0.193.. \n",
      "Epoch: 988/10000..  Training Loss: 0.194.. \n",
      "Epoch: 989/10000..  Training Loss: 0.195.. \n",
      "Epoch: 990/10000..  Training Loss: 0.196.. \n",
      "Epoch: 991/10000..  Training Loss: 0.196.. \n",
      "Epoch: 992/10000..  Training Loss: 0.196.. \n",
      "Epoch: 993/10000..  Training Loss: 0.194.. \n",
      "Epoch: 994/10000..  Training Loss: 0.192.. \n",
      "Epoch: 995/10000..  Training Loss: 0.191.. \n",
      "Epoch: 996/10000..  Training Loss: 0.191.. \n",
      "Epoch: 997/10000..  Training Loss: 0.191.. \n",
      "Epoch: 998/10000..  Training Loss: 0.192.. \n",
      "Epoch: 999/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1000/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1001/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1002/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1003/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1004/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1005/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1006/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1007/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1008/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1009/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1010/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1011/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1012/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1013/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1014/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1015/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1016/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1017/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1018/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1019/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1020/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1021/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1022/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1023/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1024/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1025/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1026/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1027/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1028/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1029/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1030/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1031/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1032/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1033/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1034/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1035/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1036/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1037/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1038/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1039/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1040/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1041/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1042/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1043/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1044/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1045/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1046/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1047/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1048/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1049/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1050/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1051/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1052/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1053/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1054/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1055/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1056/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1057/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1058/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1059/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1060/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1061/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1062/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1063/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1064/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1065/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1066/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1067/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1068/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1069/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1070/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1071/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1072/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1073/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1074/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1075/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1076/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1077/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1078/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1079/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1080/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1081/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1082/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1083/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1084/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1085/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1086/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1087/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1088/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1089/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1090/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1091/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1092/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1093/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1094/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1095/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1096/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1097/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1098/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1099/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1100/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1101/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1102/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1103/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1104/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1105/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1106/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1107/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1108/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1109/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1110/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1111/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1112/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1113/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1114/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1115/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1116/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1117/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1118/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1119/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1120/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1121/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1122/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1123/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1124/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1125/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1126/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1127/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1128/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1129/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1130/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1131/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1132/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1133/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1134/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1135/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1136/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1137/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1138/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1139/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1140/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1141/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1142/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1143/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1144/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1145/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1146/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1147/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1148/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1149/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1150/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1151/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1152/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1153/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1154/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1155/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1156/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1157/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1158/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1159/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1160/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1161/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1162/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1163/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1164/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1165/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1166/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1167/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1168/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1169/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1170/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1171/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1172/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1173/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1174/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1175/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1176/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1177/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1178/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1179/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1180/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1181/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1182/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1183/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1184/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1185/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1186/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1187/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1188/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1189/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1190/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1191/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1192/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1193/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1194/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1195/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1196/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1197/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1198/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1199/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1200/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1201/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1202/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1203/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1204/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1205/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1206/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1207/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1208/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1209/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1210/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1211/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1212/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1213/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1214/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1215/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1216/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1217/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1218/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1219/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1220/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1221/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1222/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1223/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1224/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1225/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1226/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1227/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1228/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1229/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1230/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1231/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1232/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1233/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1234/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1235/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1236/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1237/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1238/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1239/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1240/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1241/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1242/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1243/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1244/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1245/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1246/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1247/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1248/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1249/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1250/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1251/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1252/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1253/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1254/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1255/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1256/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1257/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1258/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1259/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1260/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1261/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1262/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1263/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1264/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1265/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1266/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1267/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1268/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1269/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1270/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1271/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1272/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1273/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1274/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1275/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1276/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1277/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1278/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1279/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1280/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1281/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1282/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1283/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1284/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1285/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1286/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1287/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1288/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1289/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1290/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1291/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1292/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1293/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1294/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1295/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1296/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1297/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1298/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1299/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1300/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1301/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1302/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1303/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1304/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1305/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1306/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1307/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1308/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1309/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1310/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1311/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1312/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1313/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1314/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1315/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1316/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1317/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1318/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1319/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1320/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1321/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1322/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1323/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1324/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1325/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1326/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1327/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1328/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1329/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1330/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1331/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1332/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1333/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1334/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1335/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1336/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1337/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1338/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1339/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1340/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1341/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1342/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1343/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1344/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1345/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1346/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1347/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1348/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1349/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1350/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1351/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1352/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1353/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1354/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1355/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1356/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1357/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1358/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1359/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1360/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1361/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1362/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1363/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1364/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1365/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1366/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1367/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1368/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1369/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1370/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1371/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1372/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1373/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1374/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1375/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1376/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1377/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1378/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1379/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1380/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1381/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1382/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1383/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1384/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1385/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1386/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1387/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1388/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1389/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1390/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1391/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1392/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1393/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1394/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1395/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1396/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1397/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1398/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1399/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1400/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1401/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1402/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1403/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1404/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1405/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1406/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1407/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1408/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1409/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1410/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1411/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1412/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1413/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1414/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1415/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1416/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1417/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1418/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1419/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1420/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1421/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1422/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1423/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1424/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1425/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1426/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1427/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1428/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1429/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1430/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1431/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1432/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1433/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1434/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1435/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1436/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1437/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1438/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1439/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1440/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1441/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1442/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1443/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1444/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1445/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1446/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1447/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1448/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1449/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1450/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1451/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1452/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1453/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1454/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1455/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1456/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1457/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1458/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1459/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1460/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1461/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1462/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1463/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1464/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1465/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1466/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1467/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1468/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1469/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1470/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1471/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1472/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1473/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1474/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1475/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1476/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1477/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1478/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1479/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1480/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1481/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1482/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1483/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1484/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1485/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1486/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1487/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1488/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1489/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1490/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1491/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1492/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1493/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1494/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1495/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1496/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1497/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1498/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1499/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1500/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1501/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1502/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1503/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1504/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1505/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1506/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1507/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1508/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1509/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1510/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1511/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1512/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1513/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1514/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1515/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1516/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1517/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1518/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1519/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1520/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1521/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1522/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1523/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1524/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1525/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1526/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1527/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1528/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1529/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1530/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1531/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1532/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1533/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1534/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1535/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1536/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1537/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1538/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1539/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1540/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1541/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1542/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1543/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1544/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1545/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1546/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1547/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1548/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1549/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1550/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1551/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1552/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1553/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1554/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1555/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1556/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1557/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1558/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1559/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1560/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1561/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1562/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1563/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1564/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1565/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1566/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1567/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1568/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1569/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1570/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1571/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1572/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1573/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1574/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1575/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1576/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1577/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1578/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1579/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1580/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1581/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1582/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1583/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1584/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1585/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1586/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1587/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1588/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1589/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1590/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1591/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1592/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1593/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1594/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1595/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1596/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1597/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1598/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1599/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1600/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1601/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1602/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1603/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1604/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1605/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1606/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1607/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1608/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1609/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1610/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1611/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1612/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1613/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1614/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1615/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1616/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1617/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1618/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1619/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1620/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1621/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1622/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1623/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1624/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1625/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1626/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1627/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1628/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1629/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1630/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1631/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1632/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1633/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1634/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1635/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1636/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1637/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1638/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1639/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1640/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1641/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1642/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1643/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1644/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1645/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1646/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1647/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1648/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1649/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1650/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1651/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1652/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1653/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1654/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1655/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1656/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1657/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1658/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1659/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1660/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1661/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1662/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1663/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1664/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1665/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1666/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1667/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1668/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1669/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1670/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1671/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1672/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1673/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1674/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1675/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1676/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1677/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1678/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1679/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1680/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1681/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1682/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1683/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1684/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1685/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1686/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1687/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1688/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1689/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1690/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1691/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1692/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1693/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1694/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1695/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1696/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1697/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1698/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1699/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1700/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1701/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1702/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1703/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1704/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1705/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1706/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1707/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1708/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1709/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1710/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1711/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1712/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1713/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1714/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1715/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1716/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1717/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1718/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1719/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1720/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1721/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1722/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1723/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1724/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1725/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1726/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1727/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1728/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1729/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1730/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1731/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1732/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1733/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1734/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1735/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1736/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1737/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1738/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1739/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1740/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1741/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1742/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1743/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1744/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1745/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1746/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1747/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1748/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1749/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1750/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1751/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1752/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1753/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1754/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1755/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1756/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1757/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1758/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1759/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1760/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1761/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1762/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1763/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1764/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1765/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1766/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1767/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1768/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1769/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1770/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1771/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1772/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1773/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1774/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1775/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1776/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1777/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1778/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1779/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1780/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1781/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1782/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1783/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1784/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1785/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1786/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1787/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1788/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1789/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1790/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1791/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1792/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1793/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1794/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1795/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1796/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1797/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1798/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1799/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1800/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1801/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1802/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1803/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1804/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1805/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1806/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1807/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1808/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1809/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1810/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1811/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1812/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1813/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1814/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1815/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1816/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1817/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1818/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1819/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1820/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1821/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1822/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1823/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1824/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1825/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1826/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1827/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1828/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1829/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1830/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1831/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1832/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1833/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1834/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1835/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1836/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1837/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1838/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1839/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1840/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1841/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1842/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1843/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1844/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1845/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1846/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1847/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1848/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1849/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1850/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1851/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1852/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1853/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1854/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1855/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1856/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1857/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1858/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1859/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1860/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1861/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1862/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1863/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1864/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1865/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1866/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1867/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1868/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1869/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1870/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1871/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1872/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1873/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1874/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1875/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1876/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1877/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1878/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1879/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1880/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1881/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1882/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1883/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1884/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1885/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1886/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1887/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1888/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1889/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1890/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1891/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1892/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1893/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1894/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1895/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1896/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1897/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1898/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1899/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1900/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1901/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1902/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1903/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1904/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1905/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1906/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1907/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1908/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1909/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1910/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1911/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1912/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1913/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1914/10000..  Training Loss: 0.177.. \n",
      "Epoch: 1915/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1916/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1917/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1918/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1919/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1920/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1921/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1922/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1923/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1924/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1925/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1926/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1927/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1928/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1929/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1930/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1931/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1932/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1933/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1934/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1935/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1936/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1937/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1938/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1939/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1940/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1941/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1942/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1943/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1944/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1945/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1946/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1947/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1948/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1949/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1950/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1951/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1952/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1953/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1954/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1955/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1956/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1957/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1958/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1959/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1960/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1961/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1962/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1963/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1964/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1965/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1966/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1967/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1968/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1969/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1970/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1971/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1972/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1973/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1974/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1975/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1976/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1977/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1978/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1979/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1980/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1981/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1982/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1983/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1984/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1985/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1986/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1987/10000..  Training Loss: 0.171.. \n",
      "Epoch: 1988/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1989/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1990/10000..  Training Loss: 0.174.. \n",
      "Epoch: 1991/10000..  Training Loss: 0.176.. \n",
      "Epoch: 1992/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1993/10000..  Training Loss: 0.175.. \n",
      "Epoch: 1994/10000..  Training Loss: 0.173.. \n",
      "Epoch: 1995/10000..  Training Loss: 0.172.. \n",
      "Epoch: 1996/10000..  Training Loss: 0.170.. \n",
      "Epoch: 1997/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1998/10000..  Training Loss: 0.169.. \n",
      "Epoch: 1999/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2000/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2001/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2002/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2003/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2004/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2005/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2006/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2007/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2008/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2009/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2010/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2011/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2012/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2013/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2014/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2015/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2016/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2017/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2018/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2019/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2020/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2021/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2022/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2023/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2024/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2025/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2026/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2027/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2028/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2029/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2030/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2031/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2032/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2033/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2034/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2035/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2036/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2037/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2038/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2039/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2040/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2041/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2042/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2043/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2044/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2045/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2046/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2047/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2048/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2049/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2050/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2051/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2052/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2053/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2054/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2055/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2056/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2057/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2058/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2059/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2060/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2061/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2062/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2063/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2064/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2065/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2066/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2067/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2068/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2069/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2070/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2071/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2072/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2073/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2074/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2075/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2076/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2077/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2078/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2079/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2080/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2081/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2082/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2083/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2084/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2085/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2086/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2087/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2088/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2089/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2090/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2091/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2092/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2093/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2094/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2095/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2096/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2097/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2098/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2099/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2100/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2101/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2102/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2103/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2104/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2105/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2106/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2107/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2108/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2109/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2110/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2111/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2112/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2113/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2114/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2115/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2116/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2117/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2118/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2119/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2120/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2121/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2122/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2123/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2124/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2125/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2126/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2127/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2128/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2129/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2130/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2131/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2132/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2133/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2134/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2135/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2136/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2137/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2138/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2139/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2140/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2141/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2142/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2143/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2144/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2145/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2146/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2147/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2148/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2149/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2150/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2151/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2152/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2153/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2154/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2155/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2156/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2157/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2158/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2159/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2160/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2161/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2162/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2163/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2164/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2165/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2166/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2167/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2168/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2169/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2170/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2171/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2172/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2173/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2174/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2175/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2176/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2177/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2178/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2179/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2180/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2181/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2182/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2183/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2184/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2185/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2186/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2187/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2188/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2189/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2190/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2191/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2192/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2193/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2194/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2195/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2196/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2197/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2198/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2199/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2200/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2201/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2202/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2203/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2204/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2205/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2206/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2207/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2208/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2209/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2210/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2211/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2212/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2213/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2214/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2215/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2216/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2217/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2218/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2219/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2220/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2221/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2222/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2223/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2224/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2225/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2226/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2227/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2228/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2229/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2230/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2231/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2232/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2233/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2234/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2235/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2236/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2237/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2238/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2239/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2240/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2241/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2242/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2243/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2244/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2245/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2246/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2247/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2248/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2249/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2250/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2251/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2252/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2253/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2254/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2255/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2256/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2257/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2258/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2259/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2260/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2261/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2262/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2263/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2264/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2265/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2266/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2267/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2268/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2269/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2270/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2271/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2272/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2273/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2274/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2275/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2276/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2277/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2278/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2279/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2280/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2281/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2282/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2283/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2284/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2285/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2286/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2287/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2288/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2289/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2290/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2291/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2292/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2293/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2294/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2295/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2296/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2297/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2298/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2299/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2300/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2301/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2302/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2303/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2304/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2305/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2306/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2307/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2308/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2309/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2310/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2311/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2312/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2313/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2314/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2315/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2316/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2317/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2318/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2319/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2320/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2321/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2322/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2323/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2324/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2325/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2326/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2327/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2328/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2329/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2330/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2331/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2332/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2333/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2334/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2335/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2336/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2337/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2338/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2339/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2340/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2341/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2342/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2343/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2344/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2345/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2346/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2347/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2348/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2349/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2350/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2351/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2352/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2353/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2354/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2355/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2356/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2357/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2358/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2359/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2360/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2361/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2362/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2363/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2364/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2365/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2366/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2367/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2368/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2369/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2370/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2371/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2372/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2373/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2374/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2375/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2376/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2377/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2378/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2379/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2380/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2381/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2382/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2383/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2384/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2385/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2386/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2387/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2388/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2389/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2390/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2391/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2392/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2393/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2394/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2395/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2396/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2397/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2398/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2399/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2400/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2401/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2402/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2403/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2404/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2405/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2406/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2407/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2408/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2409/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2410/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2411/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2412/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2413/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2414/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2415/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2416/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2417/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2418/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2419/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2420/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2421/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2422/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2423/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2424/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2425/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2426/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2427/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2428/10000..  Training Loss: 0.166.. \n",
      "Epoch: 2429/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2430/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2431/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2432/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2433/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2434/10000..  Training Loss: 0.164.. \n",
      "Epoch: 2435/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2436/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2437/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2438/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2439/10000..  Training Loss: 0.162.. \n",
      "Epoch: 2440/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2441/10000..  Training Loss: 0.163.. \n",
      "Epoch: 2442/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2443/10000..  Training Loss: 0.167.. \n",
      "Epoch: 2444/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2445/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2446/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2447/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2448/10000..  Training Loss: 0.168.. \n",
      "Epoch: 2449/10000..  Training Loss: 0.165.. \n",
      "Epoch: 2450/10000..  Training Loss: 0.227.. \n",
      "Epoch: 2451/10000..  Training Loss: 1.972.. \n",
      "Epoch: 2452/10000..  Training Loss: 2.161.. \n",
      "Epoch: 2453/10000..  Training Loss: 0.959.. \n",
      "Epoch: 2454/10000..  Training Loss: 1.845.. \n",
      "Epoch: 2455/10000..  Training Loss: 1.290.. \n",
      "Epoch: 2456/10000..  Training Loss: 0.850.. \n",
      "Epoch: 2457/10000..  Training Loss: 0.955.. \n",
      "Epoch: 2458/10000..  Training Loss: 1.038.. \n",
      "Epoch: 2459/10000..  Training Loss: 0.500.. \n",
      "Epoch: 2460/10000..  Training Loss: 0.518.. \n",
      "Epoch: 2461/10000..  Training Loss: 0.539.. \n",
      "Epoch: 2462/10000..  Training Loss: 0.497.. \n",
      "Epoch: 2463/10000..  Training Loss: 0.577.. \n",
      "Epoch: 2464/10000..  Training Loss: 0.383.. \n",
      "Epoch: 2465/10000..  Training Loss: 0.374.. \n",
      "Epoch: 2466/10000..  Training Loss: 0.427.. \n",
      "Epoch: 2467/10000..  Training Loss: 0.405.. \n",
      "Epoch: 2468/10000..  Training Loss: 0.325.. \n",
      "Epoch: 2469/10000..  Training Loss: 0.316.. \n",
      "Epoch: 2470/10000..  Training Loss: 0.322.. \n",
      "Epoch: 2471/10000..  Training Loss: 0.285.. \n",
      "Epoch: 2472/10000..  Training Loss: 0.281.. \n",
      "Epoch: 2473/10000..  Training Loss: 0.279.. \n",
      "Epoch: 2474/10000..  Training Loss: 0.247.. \n",
      "Epoch: 2475/10000..  Training Loss: 0.245.. \n",
      "Epoch: 2476/10000..  Training Loss: 0.263.. \n",
      "Epoch: 2477/10000..  Training Loss: 0.263.. \n",
      "Epoch: 2478/10000..  Training Loss: 0.242.. \n",
      "Epoch: 2479/10000..  Training Loss: 0.248.. \n",
      "Epoch: 2480/10000..  Training Loss: 0.239.. \n",
      "Epoch: 2481/10000..  Training Loss: 0.230.. \n",
      "Epoch: 2482/10000..  Training Loss: 0.229.. \n",
      "Epoch: 2483/10000..  Training Loss: 0.227.. \n",
      "Epoch: 2484/10000..  Training Loss: 0.219.. \n",
      "Epoch: 2485/10000..  Training Loss: 0.215.. \n",
      "Epoch: 2486/10000..  Training Loss: 0.217.. \n",
      "Epoch: 2487/10000..  Training Loss: 0.220.. \n",
      "Epoch: 2488/10000..  Training Loss: 0.216.. \n",
      "Epoch: 2489/10000..  Training Loss: 0.211.. \n",
      "Epoch: 2490/10000..  Training Loss: 0.210.. \n",
      "Epoch: 2491/10000..  Training Loss: 0.210.. \n",
      "Epoch: 2492/10000..  Training Loss: 0.208.. \n",
      "Epoch: 2493/10000..  Training Loss: 0.207.. \n",
      "Epoch: 2494/10000..  Training Loss: 0.206.. \n",
      "Epoch: 2495/10000..  Training Loss: 0.203.. \n",
      "Epoch: 2496/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2497/10000..  Training Loss: 0.202.. \n",
      "Epoch: 2498/10000..  Training Loss: 0.203.. \n",
      "Epoch: 2499/10000..  Training Loss: 0.202.. \n",
      "Epoch: 2500/10000..  Training Loss: 0.201.. \n",
      "Epoch: 2501/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2502/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2503/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2504/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2505/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2506/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2507/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2508/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2509/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2510/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2511/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2512/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2513/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2514/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2515/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2516/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2517/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2518/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2519/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2520/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2521/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2522/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2523/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2524/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2525/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2526/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2527/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2528/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2529/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2530/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2531/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2532/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2533/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2534/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2535/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2536/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2537/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2538/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2539/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2540/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2541/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2542/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2543/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2544/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2545/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2546/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2547/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2548/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2549/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2550/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2551/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2552/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2553/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2554/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2555/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2556/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2557/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2558/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2559/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2560/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2561/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2562/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2563/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2564/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2565/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2566/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2567/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2568/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2569/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2570/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2571/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2572/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2573/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2574/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2575/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2576/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2577/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2578/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2579/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2580/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2581/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2582/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2583/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2584/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2585/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2586/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2587/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2588/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2589/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2590/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2591/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2592/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2593/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2594/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2595/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2596/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2597/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2598/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2599/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2600/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2601/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2602/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2603/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2604/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2605/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2606/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2607/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2608/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2609/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2610/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2611/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2612/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2613/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2614/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2615/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2616/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2617/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2618/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2619/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2620/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2621/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2622/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2623/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2624/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2625/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2626/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2627/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2628/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2629/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2630/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2631/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2632/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2633/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2634/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2635/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2636/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2637/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2638/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2639/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2640/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2641/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2642/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2643/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2644/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2645/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2646/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2647/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2648/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2649/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2650/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2651/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2652/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2653/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2654/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2655/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2656/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2657/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2658/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2659/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2660/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2661/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2662/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2663/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2664/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2665/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2666/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2667/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2668/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2669/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2670/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2671/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2672/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2673/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2674/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2675/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2676/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2677/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2678/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2679/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2680/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2681/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2682/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2683/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2684/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2685/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2686/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2687/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2688/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2689/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2690/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2691/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2692/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2693/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2694/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2695/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2696/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2697/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2698/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2699/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2700/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2701/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2702/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2703/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2704/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2705/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2706/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2707/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2708/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2709/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2710/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2711/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2712/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2713/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2714/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2715/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2716/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2717/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2718/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2719/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2720/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2721/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2722/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2723/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2724/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2725/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2726/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2727/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2728/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2729/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2730/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2731/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2732/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2733/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2734/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2735/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2736/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2737/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2738/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2739/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2740/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2741/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2742/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2743/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2744/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2745/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2746/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2747/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2748/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2749/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2750/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2751/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2752/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2753/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2754/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2755/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2756/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2757/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2758/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2759/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2760/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2761/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2762/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2763/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2764/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2765/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2766/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2767/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2768/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2769/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2770/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2771/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2772/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2773/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2774/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2775/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2776/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2777/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2778/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2779/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2780/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2781/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2782/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2783/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2784/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2785/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2786/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2787/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2788/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2789/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2790/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2791/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2792/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2793/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2794/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2795/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2796/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2797/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2798/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2799/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2800/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2801/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2802/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2803/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2804/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2805/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2806/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2807/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2808/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2809/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2810/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2811/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2812/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2813/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2814/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2815/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2816/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2817/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2818/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2819/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2820/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2821/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2822/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2823/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2824/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2825/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2826/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2827/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2828/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2829/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2830/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2831/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2832/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2833/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2834/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2835/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2836/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2837/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2838/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2839/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2840/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2841/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2842/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2843/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2844/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2845/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2846/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2847/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2848/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2849/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2850/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2851/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2852/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2853/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2854/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2855/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2856/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2857/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2858/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2859/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2860/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2861/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2862/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2863/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2864/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2865/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2866/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2867/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2868/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2869/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2870/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2871/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2872/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2873/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2874/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2875/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2876/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2877/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2878/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2879/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2880/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2881/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2882/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2883/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2884/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2885/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2886/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2887/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2888/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2889/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2890/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2891/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2892/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2893/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2894/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2895/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2896/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2897/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2898/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2899/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2900/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2901/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2902/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2903/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2904/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2905/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2906/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2907/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2908/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2909/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2910/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2911/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2912/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2913/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2914/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2915/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2916/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2917/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2918/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2919/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2920/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2921/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2922/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2923/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2924/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2925/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2926/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2927/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2928/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2929/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2930/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2931/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2932/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2933/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2934/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2935/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2936/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2937/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2938/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2939/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2940/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2941/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2942/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2943/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2944/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2945/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2946/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2947/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2948/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2949/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2950/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2951/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2952/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2953/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2954/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2955/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2956/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2957/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2958/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2959/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2960/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2961/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2962/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2963/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2964/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2965/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2966/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2967/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2968/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2969/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2970/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2971/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2972/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2973/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2974/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2975/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2976/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2977/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2978/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2979/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2980/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2981/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2982/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2983/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2984/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2985/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2986/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2987/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2988/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2989/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2990/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2991/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2992/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2993/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2994/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2995/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2996/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2997/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2998/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2999/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3000/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3001/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3002/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3003/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3004/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3005/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3006/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3007/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3008/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3009/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3010/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3011/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3012/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3013/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3014/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3015/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3016/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3017/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3018/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3019/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3020/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3021/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3022/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3023/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3024/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3025/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3026/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3027/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3028/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3029/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3030/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3031/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3032/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3033/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3034/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3035/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3036/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3037/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3038/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3039/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3040/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3041/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3042/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3043/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3044/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3045/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3046/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3047/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3048/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3049/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3050/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3051/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3052/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3053/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3054/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3055/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3056/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3057/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3058/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3059/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3060/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3061/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3062/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3063/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3064/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3065/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3066/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3067/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3068/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3069/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3070/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3071/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3072/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3073/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3074/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3075/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3076/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3077/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3078/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3079/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3080/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3081/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3082/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3083/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3084/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3085/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3086/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3087/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3088/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3089/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3090/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3091/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3092/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3093/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3094/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3095/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3096/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3097/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3098/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3099/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3100/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3101/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3102/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3103/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3104/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3105/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3106/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3107/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3108/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3109/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3110/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3111/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3112/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3113/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3114/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3115/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3116/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3117/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3118/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3119/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3120/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3121/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3122/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3123/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3124/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3125/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3126/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3127/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3128/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3129/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3130/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3131/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3132/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3133/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3134/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3135/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3136/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3137/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3138/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3139/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3140/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3141/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3142/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3143/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3144/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3145/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3146/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3147/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3148/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3149/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3150/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3151/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3152/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3153/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3154/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3155/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3156/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3157/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3158/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3159/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3160/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3161/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3162/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3163/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3164/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3165/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3166/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3167/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3168/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3169/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3170/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3171/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3172/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3173/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3174/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3175/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3176/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3177/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3178/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3179/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3180/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3181/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3182/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3183/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3184/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3185/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3186/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3187/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3188/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3189/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3190/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3191/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3192/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3193/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3194/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3195/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3196/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3197/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3198/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3199/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3200/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3201/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3202/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3203/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3204/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3205/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3206/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3207/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3208/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3209/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3210/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3211/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3212/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3213/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3214/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3215/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3216/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3217/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3218/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3219/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3220/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3221/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3222/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3223/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3224/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3225/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3226/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3227/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3228/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3229/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3230/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3231/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3232/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3233/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3234/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3235/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3236/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3237/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3238/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3239/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3240/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3241/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3242/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3243/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3244/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3245/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3246/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3247/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3248/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3249/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3250/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3251/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3252/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3253/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3254/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3255/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3256/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3257/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3258/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3259/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3260/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3261/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3262/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3263/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3264/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3265/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3266/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3267/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3268/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3269/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3270/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3271/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3272/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3273/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3274/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3275/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3276/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3277/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3278/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3279/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3280/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3281/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3282/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3283/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3284/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3285/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3286/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3287/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3288/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3289/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3290/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3291/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3292/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3293/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3294/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3295/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3296/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3297/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3298/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3299/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3300/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3301/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3302/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3303/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3304/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3305/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3306/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3307/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3308/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3309/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3310/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3311/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3312/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3313/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3314/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3315/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3316/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3317/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3318/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3319/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3320/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3321/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3322/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3323/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3324/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3325/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3326/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3327/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3328/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3329/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3330/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3331/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3332/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3333/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3334/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3335/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3336/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3337/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3338/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3339/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3340/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3341/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3342/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3343/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3344/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3345/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3346/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3347/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3348/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3349/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3350/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3351/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3352/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3353/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3354/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3355/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3356/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3357/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3358/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3359/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3360/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3361/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3362/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3363/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3364/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3365/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3366/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3367/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3368/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3369/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3370/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3371/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3372/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3373/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3374/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3375/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3376/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3377/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3378/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3379/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3380/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3381/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3382/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3383/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3384/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3385/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3386/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3387/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3388/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3389/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3390/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3391/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3392/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3393/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3394/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3395/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3396/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3397/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3398/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3399/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3400/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3401/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3402/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3403/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3404/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3405/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3406/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3407/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3408/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3409/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3410/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3411/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3412/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3413/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3414/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3415/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3416/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3417/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3418/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3419/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3420/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3421/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3422/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3423/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3424/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3425/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3426/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3427/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3428/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3429/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3430/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3431/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3432/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3433/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3434/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3435/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3436/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3437/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3438/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3439/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3440/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3441/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3442/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3443/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3444/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3445/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3446/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3447/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3448/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3449/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3450/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3451/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3452/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3453/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3454/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3455/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3456/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3457/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3458/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3459/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3460/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3461/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3462/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3463/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3464/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3465/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3466/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3467/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3468/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3469/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3470/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3471/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3472/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3473/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3474/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3475/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3476/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3477/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3478/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3479/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3480/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3481/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3482/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3483/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3484/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3485/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3486/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3487/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3488/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3489/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3490/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3491/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3492/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3493/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3494/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3495/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3496/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3497/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3498/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3499/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3500/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3501/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3502/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3503/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3504/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3505/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3506/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3507/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3508/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3509/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3510/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3511/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3512/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3513/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3514/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3515/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3516/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3517/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3518/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3519/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3520/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3521/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3522/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3523/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3524/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3525/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3526/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3527/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3528/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3529/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3530/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3531/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3532/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3533/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3534/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3535/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3536/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3537/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3538/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3539/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3540/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3541/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3542/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3543/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3544/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3545/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3546/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3547/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3548/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3549/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3550/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3551/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3552/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3553/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3554/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3555/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3556/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3557/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3558/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3559/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3560/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3561/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3562/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3563/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3564/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3565/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3566/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3567/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3568/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3569/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3570/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3571/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3572/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3573/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3574/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3575/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3576/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3577/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3578/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3579/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3580/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3581/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3582/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3583/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3584/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3585/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3586/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3587/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3588/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3589/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3590/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3591/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3592/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3593/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3594/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3595/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3596/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3597/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3598/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3599/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3600/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3601/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3602/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3603/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3604/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3605/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3606/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3607/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3608/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3609/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3610/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3611/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3612/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3613/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3614/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3615/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3616/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3617/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3618/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3619/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3620/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3621/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3622/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3623/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3624/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3625/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3626/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3627/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3628/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3629/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3630/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3631/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3632/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3633/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3634/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3635/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3636/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3637/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3638/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3639/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3640/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3641/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3642/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3643/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3644/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3645/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3646/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3647/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3648/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3649/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3650/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3651/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3652/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3653/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3654/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3655/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3656/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3657/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3658/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3659/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3660/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3661/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3662/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3663/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3664/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3665/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3666/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3667/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3668/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3669/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3670/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3671/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3672/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3673/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3674/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3675/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3676/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3677/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3678/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3679/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3680/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3681/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3682/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3683/10000..  Training Loss: 0.174.. \n",
      "Epoch: 3684/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3685/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3686/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3687/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3688/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3689/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3690/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3691/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3692/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3693/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3694/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3695/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3696/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3697/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3698/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3699/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3700/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3701/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3702/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3703/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3704/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3705/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3706/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3707/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3708/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3709/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3710/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3711/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3712/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3713/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3714/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3715/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3716/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3717/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3718/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3719/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3720/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3721/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3722/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3723/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3724/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3725/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3726/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3727/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3728/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3729/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3730/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3731/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3732/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3733/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3734/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3735/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3736/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3737/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3738/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3739/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3740/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3741/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3742/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3743/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3744/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3745/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3746/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3747/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3748/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3749/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3750/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3751/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3752/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3753/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3754/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3755/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3756/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3757/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3758/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3759/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3760/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3761/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3762/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3763/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3764/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3765/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3766/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3767/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3768/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3769/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3770/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3771/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3772/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3773/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3774/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3775/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3776/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3777/10000..  Training Loss: 0.173.. \n",
      "Epoch: 3778/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3779/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3780/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3781/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3782/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3783/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3784/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3785/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3786/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3787/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3788/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3789/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3790/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3791/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3792/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3793/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3794/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3795/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3796/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3797/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3798/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3799/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3800/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3801/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3802/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3803/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3804/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3805/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3806/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3807/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3808/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3809/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3810/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3811/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3812/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3813/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3814/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3815/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3816/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3817/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3818/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3819/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3820/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3821/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3822/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3823/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3824/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3825/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3826/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3827/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3828/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3829/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3830/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3831/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3832/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3833/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3834/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3835/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3836/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3837/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3838/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3839/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3840/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3841/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3842/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3843/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3844/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3845/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3846/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3847/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3848/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3849/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3850/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3851/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3852/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3853/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3854/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3855/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3856/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3857/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3858/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3859/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3860/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3861/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3862/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3863/10000..  Training Loss: 0.172.. \n",
      "Epoch: 3864/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3865/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3866/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3867/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3868/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3869/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3870/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3871/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3872/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3873/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3874/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3875/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3876/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3877/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3878/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3879/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3880/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3881/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3882/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3883/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3884/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3885/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3886/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3887/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3888/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3889/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3890/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3891/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3892/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3893/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3894/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3895/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3896/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3897/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3898/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3899/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3900/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3901/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3902/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3903/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3904/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3905/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3906/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3907/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3908/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3909/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3910/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3911/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3912/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3913/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3914/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3915/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3916/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3917/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3918/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3919/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3920/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3921/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3922/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3923/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3924/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3925/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3926/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3927/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3928/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3929/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3930/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3931/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3932/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3933/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3934/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3935/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3936/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3937/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3938/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3939/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3940/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3941/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3942/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3943/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3944/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3945/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3946/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3947/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3948/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3949/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3950/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3951/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3952/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3953/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3954/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3955/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3956/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3957/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3958/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3959/10000..  Training Loss: 0.171.. \n",
      "Epoch: 3960/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3961/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3962/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3963/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3964/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3965/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3966/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3967/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3968/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3969/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3970/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3971/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3972/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3973/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3974/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3975/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3976/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3977/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3978/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3979/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3980/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3981/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3982/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3983/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3984/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3985/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3986/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3987/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3988/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3989/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3990/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3991/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3992/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3993/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3994/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3995/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3996/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3997/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3998/10000..  Training Loss: 0.170.. \n",
      "Epoch: 3999/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4000/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4001/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4002/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4003/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4004/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4005/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4006/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4007/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4008/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4009/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4010/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4011/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4012/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4013/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4014/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4015/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4016/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4017/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4018/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4019/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4020/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4021/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4022/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4023/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4024/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4025/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4026/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4027/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4028/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4029/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4030/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4031/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4032/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4033/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4034/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4035/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4036/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4037/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4038/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4039/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4040/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4041/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4042/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4043/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4044/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4045/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4046/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4047/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4048/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4049/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4050/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4051/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4052/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4053/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4054/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4055/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4056/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4057/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4058/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4059/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4060/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4061/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4062/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4063/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4064/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4065/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4066/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4067/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4068/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4069/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4070/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4071/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4072/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4073/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4074/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4075/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4076/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4077/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4078/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4079/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4080/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4081/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4082/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4083/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4084/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4085/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4086/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4087/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4088/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4089/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4090/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4091/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4092/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4093/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4094/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4095/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4096/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4097/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4098/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4099/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4100/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4101/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4102/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4103/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4104/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4105/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4106/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4107/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4108/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4109/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4110/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4111/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4112/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4113/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4114/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4115/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4116/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4117/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4118/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4119/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4120/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4121/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4122/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4123/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4124/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4125/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4126/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4127/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4128/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4129/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4130/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4131/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4132/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4133/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4134/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4135/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4136/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4137/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4138/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4139/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4140/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4141/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4142/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4143/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4144/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4145/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4146/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4147/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4148/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4149/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4150/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4151/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4152/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4153/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4154/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4155/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4156/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4157/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4158/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4159/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4160/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4161/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4162/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4163/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4164/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4165/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4166/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4167/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4168/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4169/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4170/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4171/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4172/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4173/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4174/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4175/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4176/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4177/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4178/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4179/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4180/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4181/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4182/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4183/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4184/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4185/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4186/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4187/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4188/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4189/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4190/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4191/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4192/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4193/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4194/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4195/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4196/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4197/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4198/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4199/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4200/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4201/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4202/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4203/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4204/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4205/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4206/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4207/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4208/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4209/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4210/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4211/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4212/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4213/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4214/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4215/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4216/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4217/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4218/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4219/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4220/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4221/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4222/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4223/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4224/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4225/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4226/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4227/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4228/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4229/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4230/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4231/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4232/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4233/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4234/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4235/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4236/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4237/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4238/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4239/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4240/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4241/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4242/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4243/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4244/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4245/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4246/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4247/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4248/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4249/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4250/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4251/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4252/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4253/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4254/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4255/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4256/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4257/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4258/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4259/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4260/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4261/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4262/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4263/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4264/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4265/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4266/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4267/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4268/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4269/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4270/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4271/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4272/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4273/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4274/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4275/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4276/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4277/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4278/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4279/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4280/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4281/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4282/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4283/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4284/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4285/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4286/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4287/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4288/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4289/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4290/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4291/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4292/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4293/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4294/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4295/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4296/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4297/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4298/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4299/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4300/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4301/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4302/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4303/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4304/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4305/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4306/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4307/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4308/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4309/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4310/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4311/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4312/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4313/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4314/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4315/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4316/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4317/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4318/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4319/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4320/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4321/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4322/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4323/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4324/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4325/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4326/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4327/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4328/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4329/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4330/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4331/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4332/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4333/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4334/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4335/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4336/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4337/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4338/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4339/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4340/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4341/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4342/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4343/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4344/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4345/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4346/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4347/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4348/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4349/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4350/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4351/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4352/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4353/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4354/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4355/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4356/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4357/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4358/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4359/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4360/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4361/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4362/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4363/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4364/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4365/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4366/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4367/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4368/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4369/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4370/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4371/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4372/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4373/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4374/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4375/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4376/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4377/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4378/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4379/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4380/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4381/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4382/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4383/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4384/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4385/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4386/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4387/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4388/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4389/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4390/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4391/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4392/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4393/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4394/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4395/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4396/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4397/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4398/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4399/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4400/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4401/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4402/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4403/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4404/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4405/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4406/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4407/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4408/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4409/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4410/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4411/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4412/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4413/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4414/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4415/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4416/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4417/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4418/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4419/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4420/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4421/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4422/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4423/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4424/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4425/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4426/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4427/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4428/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4429/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4430/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4431/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4432/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4433/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4434/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4435/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4436/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4437/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4438/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4439/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4440/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4441/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4442/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4443/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4444/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4445/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4446/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4447/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4448/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4449/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4450/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4451/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4452/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4453/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4454/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4455/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4456/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4457/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4458/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4459/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4460/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4461/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4462/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4463/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4464/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4465/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4466/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4467/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4468/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4469/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4470/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4471/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4472/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4473/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4474/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4475/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4476/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4477/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4478/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4479/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4480/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4481/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4482/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4483/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4484/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4485/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4486/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4487/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4488/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4489/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4490/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4491/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4492/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4493/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4494/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4495/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4496/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4497/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4498/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4499/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4500/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4501/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4502/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4503/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4504/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4505/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4506/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4507/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4508/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4509/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4510/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4511/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4512/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4513/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4514/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4515/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4516/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4517/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4518/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4519/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4520/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4521/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4522/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4523/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4524/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4525/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4526/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4527/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4528/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4529/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4530/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4531/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4532/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4533/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4534/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4535/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4536/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4537/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4538/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4539/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4540/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4541/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4542/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4543/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4544/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4545/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4546/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4547/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4548/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4549/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4550/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4551/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4552/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4553/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4554/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4555/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4556/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4557/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4558/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4559/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4560/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4561/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4562/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4563/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4564/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4565/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4566/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4567/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4568/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4569/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4570/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4571/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4572/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4573/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4574/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4575/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4576/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4577/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4578/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4579/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4580/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4581/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4582/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4583/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4584/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4585/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4586/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4587/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4588/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4589/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4590/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4591/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4592/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4593/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4594/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4595/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4596/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4597/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4598/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4599/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4600/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4601/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4602/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4603/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4604/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4605/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4606/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4607/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4608/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4609/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4610/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4611/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4612/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4613/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4614/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4615/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4616/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4617/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4618/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4619/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4620/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4621/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4622/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4623/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4624/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4625/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4626/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4627/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4628/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4629/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4630/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4631/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4632/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4633/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4634/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4635/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4636/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4637/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4638/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4639/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4640/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4641/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4642/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4643/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4644/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4645/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4646/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4647/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4648/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4649/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4650/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4651/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4652/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4653/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4654/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4655/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4656/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4657/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4658/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4659/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4660/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4661/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4662/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4663/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4664/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4665/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4666/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4667/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4668/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4669/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4670/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4671/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4672/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4673/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4674/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4675/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4676/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4677/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4678/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4679/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4680/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4681/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4682/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4683/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4684/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4685/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4686/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4687/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4688/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4689/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4690/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4691/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4692/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4693/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4694/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4695/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4696/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4697/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4698/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4699/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4700/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4701/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4702/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4703/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4704/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4705/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4706/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4707/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4708/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4709/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4710/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4711/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4712/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4713/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4714/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4715/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4716/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4717/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4718/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4719/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4720/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4721/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4722/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4723/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4724/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4725/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4726/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4727/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4728/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4729/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4730/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4731/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4732/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4733/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4734/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4735/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4736/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4737/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4738/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4739/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4740/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4741/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4742/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4743/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4744/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4745/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4746/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4747/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4748/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4749/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4750/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4751/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4752/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4753/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4754/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4755/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4756/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4757/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4758/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4759/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4760/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4761/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4762/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4763/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4764/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4765/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4766/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4767/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4768/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4769/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4770/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4771/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4772/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4773/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4774/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4775/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4776/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4777/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4778/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4779/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4780/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4781/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4782/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4783/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4784/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4785/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4786/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4787/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4788/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4789/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4790/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4791/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4792/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4793/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4794/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4795/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4796/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4797/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4798/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4799/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4800/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4801/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4802/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4803/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4804/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4805/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4806/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4807/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4808/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4809/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4810/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4811/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4812/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4813/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4814/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4815/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4816/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4817/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4818/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4819/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4820/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4821/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4822/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4823/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4824/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4825/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4826/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4827/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4828/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4829/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4830/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4831/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4832/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4833/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4834/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4835/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4836/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4837/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4838/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4839/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4840/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4841/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4842/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4843/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4844/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4845/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4846/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4847/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4848/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4849/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4850/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4851/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4852/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4853/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4854/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4855/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4856/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4857/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4858/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4859/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4860/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4861/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4862/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4863/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4864/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4865/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4866/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4867/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4868/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4869/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4870/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4871/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4872/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4873/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4874/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4875/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4876/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4877/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4878/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4879/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4880/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4881/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4882/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4883/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4884/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4885/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4886/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4887/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4888/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4889/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4890/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4891/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4892/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4893/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4894/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4895/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4896/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4897/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4898/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4899/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4900/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4901/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4902/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4903/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4904/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4905/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4906/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4907/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4908/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4909/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4910/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4911/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4912/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4913/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4914/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4915/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4916/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4917/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4918/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4919/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4920/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4921/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4922/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4923/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4924/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4925/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4926/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4927/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4928/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4929/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4930/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4931/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4932/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4933/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4934/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4935/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4936/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4937/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4938/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4939/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4940/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4941/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4942/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4943/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4944/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4945/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4946/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4947/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4948/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4949/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4950/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4951/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4952/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4953/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4954/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4955/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4956/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4957/10000..  Training Loss: 0.165.. \n",
      "Epoch: 4958/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4959/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4960/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4961/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4962/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4963/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4964/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4965/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4966/10000..  Training Loss: 0.163.. \n",
      "Epoch: 4967/10000..  Training Loss: 0.164.. \n",
      "Epoch: 4968/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4969/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4970/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4971/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4972/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4973/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4974/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4975/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4976/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4977/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4978/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4979/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4980/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4981/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4982/10000..  Training Loss: 0.162.. \n",
      "Epoch: 4983/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4984/10000..  Training Loss: 0.161.. \n",
      "Epoch: 4985/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4986/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4987/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4988/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4989/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4990/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4991/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4992/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4993/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4994/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4995/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4996/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4997/10000..  Training Loss: 0.159.. \n",
      "Epoch: 4998/10000..  Training Loss: 0.160.. \n",
      "Epoch: 4999/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5000/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5001/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5002/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5003/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5004/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5005/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5006/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5007/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5008/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5009/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5010/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5011/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5012/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5013/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5014/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5015/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5016/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5017/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5018/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5019/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5020/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5021/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5022/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5023/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5024/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5025/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5026/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5027/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5028/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5029/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5030/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5031/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5032/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5033/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5034/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5035/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5036/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5037/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5038/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5039/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5040/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5041/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5042/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5043/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5044/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5045/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5046/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5047/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5048/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5049/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5050/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5051/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5052/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5053/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5054/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5055/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5056/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5057/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5058/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5059/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5060/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5061/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5062/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5063/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5064/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5065/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5066/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5067/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5068/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5069/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5070/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5071/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5072/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5073/10000..  Training Loss: 0.166.. \n",
      "Epoch: 5074/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5075/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5076/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5077/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5078/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5079/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5080/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5081/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5082/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5083/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5084/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5085/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5086/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5087/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5088/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5089/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5090/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5091/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5092/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5093/10000..  Training Loss: 0.165.. \n",
      "Epoch: 5094/10000..  Training Loss: 0.164.. \n",
      "Epoch: 5095/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5096/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5097/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5098/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5099/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5100/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5101/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5102/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5103/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5104/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5105/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5106/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5107/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5108/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5109/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5110/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5111/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5112/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5113/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5114/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5115/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5116/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5117/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5118/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5119/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5120/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5121/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5122/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5123/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5124/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5125/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5126/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5127/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5128/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5129/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5130/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5131/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5132/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5133/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5134/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5135/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5136/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5137/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5138/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5139/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5140/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5141/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5142/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5143/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5144/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5145/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5146/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5147/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5148/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5149/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5150/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5151/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5152/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5153/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5154/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5155/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5156/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5157/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5158/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5159/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5160/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5161/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5162/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5163/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5164/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5165/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5166/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5167/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5168/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5169/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5170/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5171/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5172/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5173/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5174/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5175/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5176/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5177/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5178/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5179/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5180/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5181/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5182/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5183/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5184/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5185/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5186/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5187/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5188/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5189/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5190/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5191/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5192/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5193/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5194/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5195/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5196/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5197/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5198/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5199/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5200/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5201/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5202/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5203/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5204/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5205/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5206/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5207/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5208/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5209/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5210/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5211/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5212/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5213/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5214/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5215/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5216/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5217/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5218/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5219/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5220/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5221/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5222/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5223/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5224/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5225/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5226/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5227/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5228/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5229/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5230/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5231/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5232/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5233/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5234/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5235/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5236/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5237/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5238/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5239/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5240/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5241/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5242/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5243/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5244/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5245/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5246/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5247/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5248/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5249/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5250/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5251/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5252/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5253/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5254/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5255/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5256/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5257/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5258/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5259/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5260/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5261/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5262/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5263/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5264/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5265/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5266/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5267/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5268/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5269/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5270/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5271/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5272/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5273/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5274/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5275/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5276/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5277/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5278/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5279/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5280/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5281/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5282/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5283/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5284/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5285/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5286/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5287/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5288/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5289/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5290/10000..  Training Loss: 0.162.. \n",
      "Epoch: 5291/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5292/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5293/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5294/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5295/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5296/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5297/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5298/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5299/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5300/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5301/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5302/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5303/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5304/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5305/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5306/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5307/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5308/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5309/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5310/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5311/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5312/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5313/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5314/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5315/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5316/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5317/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5318/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5319/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5320/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5321/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5322/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5323/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5324/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5325/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5326/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5327/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5328/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5329/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5330/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5331/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5332/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5333/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5334/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5335/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5336/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5337/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5338/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5339/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5340/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5341/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5342/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5343/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5344/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5345/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5346/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5347/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5348/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5349/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5350/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5351/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5352/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5353/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5354/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5355/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5356/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5357/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5358/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5359/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5360/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5361/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5362/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5363/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5364/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5365/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5366/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5367/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5368/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5369/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5370/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5371/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5372/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5373/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5374/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5375/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5376/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5377/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5378/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5379/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5380/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5381/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5382/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5383/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5384/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5385/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5386/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5387/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5388/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5389/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5390/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5391/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5392/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5393/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5394/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5395/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5396/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5397/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5398/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5399/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5400/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5401/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5402/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5403/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5404/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5405/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5406/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5407/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5408/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5409/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5410/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5411/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5412/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5413/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5414/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5415/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5416/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5417/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5418/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5419/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5420/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5421/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5422/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5423/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5424/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5425/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5426/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5427/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5428/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5429/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5430/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5431/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5432/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5433/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5434/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5435/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5436/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5437/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5438/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5439/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5440/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5441/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5442/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5443/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5444/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5445/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5446/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5447/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5448/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5449/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5450/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5451/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5452/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5453/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5454/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5455/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5456/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5457/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5458/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5459/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5460/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5461/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5462/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5463/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5464/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5465/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5466/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5467/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5468/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5469/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5470/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5471/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5472/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5473/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5474/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5475/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5476/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5477/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5478/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5479/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5480/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5481/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5482/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5483/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5484/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5485/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5486/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5487/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5488/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5489/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5490/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5491/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5492/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5493/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5494/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5495/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5496/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5497/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5498/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5499/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5500/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5501/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5502/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5503/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5504/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5505/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5506/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5507/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5508/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5509/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5510/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5511/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5512/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5513/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5514/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5515/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5516/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5517/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5518/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5519/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5520/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5521/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5522/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5523/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5524/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5525/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5526/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5527/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5528/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5529/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5530/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5531/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5532/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5533/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5534/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5535/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5536/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5537/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5538/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5539/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5540/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5541/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5542/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5543/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5544/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5545/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5546/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5547/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5548/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5549/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5550/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5551/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5552/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5553/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5554/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5555/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5556/10000..  Training Loss: 0.159.. \n",
      "Epoch: 5557/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5558/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5559/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5560/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5561/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5562/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5563/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5564/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5565/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5566/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5567/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5568/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5569/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5570/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5571/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5572/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5573/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5574/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5575/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5576/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5577/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5578/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5579/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5580/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5581/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5582/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5583/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5584/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5585/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5586/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5587/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5588/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5589/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5590/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5591/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5592/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5593/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5594/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5595/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5596/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5597/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5598/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5599/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5600/10000..  Training Loss: 0.161.. \n",
      "Epoch: 5601/10000..  Training Loss: 0.163.. \n",
      "Epoch: 5602/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5603/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5604/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5605/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5606/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5607/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5608/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5609/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5610/10000..  Training Loss: 0.160.. \n",
      "Epoch: 5611/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5612/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5613/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5614/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5615/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5616/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5617/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5618/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5619/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5620/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5621/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5622/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5623/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5624/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5625/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5626/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5627/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5628/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5629/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5630/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5631/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5632/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5633/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5634/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5635/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5636/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5637/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5638/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5639/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5640/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5641/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5642/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5643/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5644/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5645/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5646/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5647/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5648/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5649/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5650/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5651/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5652/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5653/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5654/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5655/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5656/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5657/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5658/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5659/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5660/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5661/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5662/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5663/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5664/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5665/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5666/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5667/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5668/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5669/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5670/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5671/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5672/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5673/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5674/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5675/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5676/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5677/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5678/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5679/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5680/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5681/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5682/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5683/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5684/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5685/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5686/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5687/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5688/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5689/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5690/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5691/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5692/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5693/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5694/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5695/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5696/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5697/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5698/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5699/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5700/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5701/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5702/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5703/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5704/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5705/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5706/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5707/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5708/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5709/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5710/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5711/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5712/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5713/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5714/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5715/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5716/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5717/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5718/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5719/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5720/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5721/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5722/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5723/10000..  Training Loss: 0.158.. \n",
      "Epoch: 5724/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5725/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5726/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5727/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5728/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5729/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5730/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5731/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5732/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5733/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5734/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5735/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5736/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5737/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5738/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5739/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5740/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5741/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5742/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5743/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5744/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5745/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5746/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5747/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5748/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5749/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5750/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5751/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5752/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5753/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5754/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5755/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5756/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5757/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5758/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5759/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5760/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5761/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5762/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5763/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5764/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5765/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5766/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5767/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5768/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5769/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5770/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5771/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5772/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5773/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5774/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5775/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5776/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5777/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5778/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5779/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5780/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5781/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5782/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5783/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5784/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5785/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5786/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5787/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5788/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5789/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5790/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5791/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5792/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5793/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5794/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5795/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5796/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5797/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5798/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5799/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5800/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5801/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5802/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5803/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5804/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5805/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5806/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5807/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5808/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5809/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5810/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5811/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5812/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5813/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5814/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5815/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5816/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5817/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5818/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5819/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5820/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5821/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5822/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5823/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5824/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5825/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5826/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5827/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5828/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5829/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5830/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5831/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5832/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5833/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5834/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5835/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5836/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5837/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5838/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5839/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5840/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5841/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5842/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5843/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5844/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5845/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5846/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5847/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5848/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5849/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5850/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5851/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5852/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5853/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5854/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5855/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5856/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5857/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5858/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5859/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5860/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5861/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5862/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5863/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5864/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5865/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5866/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5867/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5868/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5869/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5870/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5871/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5872/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5873/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5874/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5875/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5876/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5877/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5878/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5879/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5880/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5881/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5882/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5883/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5884/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5885/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5886/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5887/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5888/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5889/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5890/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5891/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5892/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5893/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5894/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5895/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5896/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5897/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5898/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5899/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5900/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5901/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5902/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5903/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5904/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5905/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5906/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5907/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5908/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5909/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5910/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5911/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5912/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5913/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5914/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5915/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5916/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5917/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5918/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5919/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5920/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5921/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5922/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5923/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5924/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5925/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5926/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5927/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5928/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5929/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5930/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5931/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5932/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5933/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5934/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5935/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5936/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5937/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5938/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5939/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5940/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5941/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5942/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5943/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5944/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5945/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5946/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5947/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5948/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5949/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5950/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5951/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5952/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5953/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5954/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5955/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5956/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5957/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5958/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5959/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5960/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5961/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5962/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5963/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5964/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5965/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5966/10000..  Training Loss: 0.157.. \n",
      "Epoch: 5967/10000..  Training Loss: 0.156.. \n",
      "Epoch: 5968/10000..  Training Loss: 0.155.. \n",
      "Epoch: 5969/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5970/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5971/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5972/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5973/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5974/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5975/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5976/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5977/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5978/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5979/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5980/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5981/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5982/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5983/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5984/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5985/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5986/10000..  Training Loss: 0.149.. \n",
      "Epoch: 5987/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5988/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5989/10000..  Training Loss: 0.150.. \n",
      "Epoch: 5990/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5991/10000..  Training Loss: 0.151.. \n",
      "Epoch: 5992/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5993/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5994/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5995/10000..  Training Loss: 0.153.. \n",
      "Epoch: 5996/10000..  Training Loss: 0.154.. \n",
      "Epoch: 5997/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5998/10000..  Training Loss: 0.152.. \n",
      "Epoch: 5999/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6000/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6001/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6002/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6003/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6004/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6005/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6006/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6007/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6008/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6009/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6010/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6011/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6012/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6013/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6014/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6015/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6016/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6017/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6018/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6019/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6020/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6021/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6022/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6023/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6024/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6025/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6026/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6027/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6028/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6029/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6030/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6031/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6032/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6033/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6034/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6035/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6036/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6037/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6038/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6039/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6040/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6041/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6042/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6043/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6044/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6045/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6046/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6047/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6048/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6049/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6050/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6051/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6052/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6053/10000..  Training Loss: 0.157.. \n",
      "Epoch: 6054/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6055/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6056/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6057/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6058/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6059/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6060/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6061/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6062/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6063/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6064/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6065/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6066/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6067/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6068/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6069/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6070/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6071/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6072/10000..  Training Loss: 0.156.. \n",
      "Epoch: 6073/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6074/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6075/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6076/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6077/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6078/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6079/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6080/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6081/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6082/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6083/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6084/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6085/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6086/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6087/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6088/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6089/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6090/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6091/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6092/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6093/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6094/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6095/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6096/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6097/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6098/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6099/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6100/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6101/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6102/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6103/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6104/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6105/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6106/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6107/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6108/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6109/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6110/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6111/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6112/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6113/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6114/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6115/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6116/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6117/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6118/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6119/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6120/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6121/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6122/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6123/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6124/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6125/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6126/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6127/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6128/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6129/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6130/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6131/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6132/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6133/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6134/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6135/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6136/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6137/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6138/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6139/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6140/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6141/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6142/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6143/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6144/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6145/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6146/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6147/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6148/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6149/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6150/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6151/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6152/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6153/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6154/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6155/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6156/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6157/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6158/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6159/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6160/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6161/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6162/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6163/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6164/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6165/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6166/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6167/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6168/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6169/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6170/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6171/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6172/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6173/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6174/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6175/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6176/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6177/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6178/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6179/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6180/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6181/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6182/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6183/10000..  Training Loss: 0.155.. \n",
      "Epoch: 6184/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6185/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6186/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6187/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6188/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6189/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6190/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6191/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6192/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6193/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6194/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6195/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6196/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6197/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6198/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6199/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6200/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6201/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6202/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6203/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6204/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6205/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6206/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6207/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6208/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6209/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6210/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6211/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6212/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6213/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6214/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6215/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6216/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6217/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6218/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6219/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6220/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6221/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6222/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6223/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6224/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6225/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6226/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6227/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6228/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6229/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6230/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6231/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6232/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6233/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6234/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6235/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6236/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6237/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6238/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6239/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6240/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6241/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6242/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6243/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6244/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6245/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6246/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6247/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6248/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6249/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6250/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6251/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6252/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6253/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6254/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6255/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6256/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6257/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6258/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6259/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6260/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6261/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6262/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6263/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6264/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6265/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6266/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6267/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6268/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6269/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6270/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6271/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6272/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6273/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6274/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6275/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6276/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6277/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6278/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6279/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6280/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6281/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6282/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6283/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6284/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6285/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6286/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6287/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6288/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6289/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6290/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6291/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6292/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6293/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6294/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6295/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6296/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6297/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6298/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6299/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6300/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6301/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6302/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6303/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6304/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6305/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6306/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6307/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6308/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6309/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6310/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6311/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6312/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6313/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6314/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6315/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6316/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6317/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6318/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6319/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6320/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6321/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6322/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6323/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6324/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6325/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6326/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6327/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6328/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6329/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6330/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6331/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6332/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6333/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6334/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6335/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6336/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6337/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6338/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6339/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6340/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6341/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6342/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6343/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6344/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6345/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6346/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6347/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6348/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6349/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6350/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6351/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6352/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6353/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6354/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6355/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6356/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6357/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6358/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6359/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6360/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6361/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6362/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6363/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6364/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6365/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6366/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6367/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6368/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6369/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6370/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6371/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6372/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6373/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6374/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6375/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6376/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6377/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6378/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6379/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6380/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6381/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6382/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6383/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6384/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6385/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6386/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6387/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6388/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6389/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6390/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6391/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6392/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6393/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6394/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6395/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6396/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6397/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6398/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6399/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6400/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6401/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6402/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6403/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6404/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6405/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6406/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6407/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6408/10000..  Training Loss: 0.153.. \n",
      "Epoch: 6409/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6410/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6411/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6412/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6413/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6414/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6415/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6416/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6417/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6418/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6419/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6420/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6421/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6422/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6423/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6424/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6425/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6426/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6427/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6428/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6429/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6430/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6431/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6432/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6433/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6434/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6435/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6436/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6437/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6438/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6439/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6440/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6441/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6442/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6443/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6444/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6445/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6446/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6447/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6448/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6449/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6450/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6451/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6452/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6453/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6454/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6455/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6456/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6457/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6458/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6459/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6460/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6461/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6462/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6463/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6464/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6465/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6466/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6467/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6468/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6469/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6470/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6471/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6472/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6473/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6474/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6475/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6476/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6477/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6478/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6479/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6480/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6481/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6482/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6483/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6484/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6485/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6486/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6487/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6488/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6489/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6490/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6491/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6492/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6493/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6494/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6495/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6496/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6497/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6498/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6499/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6500/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6501/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6502/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6503/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6504/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6505/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6506/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6507/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6508/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6509/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6510/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6511/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6512/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6513/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6514/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6515/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6516/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6517/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6518/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6519/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6520/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6521/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6522/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6523/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6524/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6525/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6526/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6527/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6528/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6529/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6530/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6531/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6532/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6533/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6534/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6535/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6536/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6537/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6538/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6539/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6540/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6541/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6542/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6543/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6544/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6545/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6546/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6547/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6548/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6549/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6550/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6551/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6552/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6553/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6554/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6555/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6556/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6557/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6558/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6559/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6560/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6561/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6562/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6563/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6564/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6565/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6566/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6567/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6568/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6569/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6570/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6571/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6572/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6573/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6574/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6575/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6576/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6577/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6578/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6579/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6580/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6581/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6582/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6583/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6584/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6585/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6586/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6587/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6588/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6589/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6590/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6591/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6592/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6593/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6594/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6595/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6596/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6597/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6598/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6599/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6600/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6601/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6602/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6603/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6604/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6605/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6606/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6607/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6608/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6609/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6610/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6611/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6612/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6613/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6614/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6615/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6616/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6617/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6618/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6619/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6620/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6621/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6622/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6623/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6624/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6625/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6626/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6627/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6628/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6629/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6630/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6631/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6632/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6633/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6634/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6635/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6636/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6637/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6638/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6639/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6640/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6641/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6642/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6643/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6644/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6645/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6646/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6647/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6648/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6649/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6650/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6651/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6652/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6653/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6654/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6655/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6656/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6657/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6658/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6659/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6660/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6661/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6662/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6663/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6664/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6665/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6666/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6667/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6668/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6669/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6670/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6671/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6672/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6673/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6674/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6675/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6676/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6677/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6678/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6679/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6680/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6681/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6682/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6683/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6684/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6685/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6686/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6687/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6688/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6689/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6690/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6691/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6692/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6693/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6694/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6695/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6696/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6697/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6698/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6699/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6700/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6701/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6702/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6703/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6704/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6705/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6706/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6707/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6708/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6709/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6710/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6711/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6712/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6713/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6714/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6715/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6716/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6717/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6718/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6719/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6720/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6721/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6722/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6723/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6724/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6725/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6726/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6727/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6728/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6729/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6730/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6731/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6732/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6733/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6734/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6735/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6736/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6737/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6738/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6739/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6740/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6741/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6742/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6743/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6744/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6745/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6746/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6747/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6748/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6749/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6750/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6751/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6752/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6753/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6754/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6755/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6756/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6757/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6758/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6759/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6760/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6761/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6762/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6763/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6764/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6765/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6766/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6767/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6768/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6769/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6770/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6771/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6772/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6773/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6774/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6775/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6776/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6777/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6778/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6779/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6780/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6781/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6782/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6783/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6784/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6785/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6786/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6787/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6788/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6789/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6790/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6791/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6792/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6793/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6794/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6795/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6796/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6797/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6798/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6799/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6800/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6801/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6802/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6803/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6804/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6805/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6806/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6807/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6808/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6809/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6810/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6811/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6812/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6813/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6814/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6815/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6816/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6817/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6818/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6819/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6820/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6821/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6822/10000..  Training Loss: 0.151.. \n",
      "Epoch: 6823/10000..  Training Loss: 0.152.. \n",
      "Epoch: 6824/10000..  Training Loss: 0.154.. \n",
      "Epoch: 6825/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6826/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6827/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6828/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6829/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6830/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6831/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6832/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6833/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6834/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6835/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6836/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6837/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6838/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6839/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6840/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6841/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6842/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6843/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6844/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6845/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6846/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6847/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6848/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6849/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6850/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6851/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6852/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6853/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6854/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6855/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6856/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6857/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6858/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6859/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6860/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6861/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6862/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6863/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6864/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6865/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6866/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6867/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6868/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6869/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6870/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6871/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6872/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6873/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6874/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6875/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6876/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6877/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6878/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6879/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6880/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6881/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6882/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6883/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6884/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6885/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6886/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6887/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6888/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6889/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6890/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6891/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6892/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6893/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6894/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6895/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6896/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6897/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6898/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6899/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6900/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6901/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6902/10000..  Training Loss: 0.149.. \n",
      "Epoch: 6903/10000..  Training Loss: 0.150.. \n",
      "Epoch: 6904/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6905/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6906/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6907/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6908/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6909/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6910/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6911/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6912/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6913/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6914/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6915/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6916/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6917/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6918/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6919/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6920/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6921/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6922/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6923/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6924/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6925/10000..  Training Loss: 0.148.. \n",
      "Epoch: 6926/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6927/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6928/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6929/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6930/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6931/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6932/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6933/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6934/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6935/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6936/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6937/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6938/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6939/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6940/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6941/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6942/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6943/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6944/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6945/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6946/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6947/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6948/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6949/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6950/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6951/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6952/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6953/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6954/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6955/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6956/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6957/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6958/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6959/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6960/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6961/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6962/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6963/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6964/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6965/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6966/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6967/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6968/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6969/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6970/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6971/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6972/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6973/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6974/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6975/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6976/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6977/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6978/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6979/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6980/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6981/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6982/10000..  Training Loss: 0.141.. \n",
      "Epoch: 6983/10000..  Training Loss: 0.141.. \n",
      "Epoch: 6984/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6985/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6986/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6987/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6988/10000..  Training Loss: 0.142.. \n",
      "Epoch: 6989/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6990/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6991/10000..  Training Loss: 0.145.. \n",
      "Epoch: 6992/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6993/10000..  Training Loss: 0.147.. \n",
      "Epoch: 6994/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6995/10000..  Training Loss: 0.146.. \n",
      "Epoch: 6996/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6997/10000..  Training Loss: 0.144.. \n",
      "Epoch: 6998/10000..  Training Loss: 0.143.. \n",
      "Epoch: 6999/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7000/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7001/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7002/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7003/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7004/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7005/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7006/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7007/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7008/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7009/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7010/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7011/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7012/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7013/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7014/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7015/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7016/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7017/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7018/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7019/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7020/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7021/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7022/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7023/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7024/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7025/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7026/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7027/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7028/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7029/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7030/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7031/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7032/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7033/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7034/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7035/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7036/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7037/10000..  Training Loss: 0.149.. \n",
      "Epoch: 7038/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7039/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7040/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7041/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7042/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7043/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7044/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7045/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7046/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7047/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7048/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7049/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7050/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7051/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7052/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7053/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7054/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7055/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7056/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7057/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7058/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7059/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7060/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7061/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7062/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7063/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7064/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7065/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7066/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7067/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7068/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7069/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7070/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7071/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7072/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7073/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7074/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7075/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7076/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7077/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7078/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7079/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7080/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7081/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7082/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7083/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7084/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7085/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7086/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7087/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7088/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7089/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7090/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7091/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7092/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7093/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7094/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7095/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7096/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7097/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7098/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7099/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7100/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7101/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7102/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7103/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7104/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7105/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7106/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7107/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7108/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7109/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7110/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7111/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7112/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7113/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7114/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7115/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7116/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7117/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7118/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7119/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7120/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7121/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7122/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7123/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7124/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7125/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7126/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7127/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7128/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7129/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7130/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7131/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7132/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7133/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7134/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7135/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7136/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7137/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7138/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7139/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7140/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7141/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7142/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7143/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7144/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7145/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7146/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7147/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7148/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7149/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7150/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7151/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7152/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7153/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7154/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7155/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7156/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7157/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7158/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7159/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7160/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7161/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7162/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7163/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7164/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7165/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7166/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7167/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7168/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7169/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7170/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7171/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7172/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7173/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7174/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7175/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7176/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7177/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7178/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7179/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7180/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7181/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7182/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7183/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7184/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7185/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7186/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7187/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7188/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7189/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7190/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7191/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7192/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7193/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7194/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7195/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7196/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7197/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7198/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7199/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7200/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7201/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7202/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7203/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7204/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7205/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7206/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7207/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7208/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7209/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7210/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7211/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7212/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7213/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7214/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7215/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7216/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7217/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7218/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7219/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7220/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7221/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7222/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7223/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7224/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7225/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7226/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7227/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7228/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7229/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7230/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7231/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7232/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7233/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7234/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7235/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7236/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7237/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7238/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7239/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7240/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7241/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7242/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7243/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7244/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7245/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7246/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7247/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7248/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7249/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7250/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7251/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7252/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7253/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7254/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7255/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7256/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7257/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7258/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7259/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7260/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7261/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7262/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7263/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7264/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7265/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7266/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7267/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7268/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7269/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7270/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7271/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7272/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7273/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7274/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7275/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7276/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7277/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7278/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7279/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7280/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7281/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7282/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7283/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7284/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7285/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7286/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7287/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7288/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7289/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7290/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7291/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7292/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7293/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7294/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7295/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7296/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7297/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7298/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7299/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7300/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7301/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7302/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7303/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7304/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7305/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7306/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7307/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7308/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7309/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7310/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7311/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7312/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7313/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7314/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7315/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7316/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7317/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7318/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7319/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7320/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7321/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7322/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7323/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7324/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7325/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7326/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7327/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7328/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7329/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7330/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7331/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7332/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7333/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7334/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7335/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7336/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7337/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7338/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7339/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7340/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7341/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7342/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7343/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7344/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7345/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7346/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7347/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7348/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7349/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7350/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7351/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7352/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7353/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7354/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7355/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7356/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7357/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7358/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7359/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7360/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7361/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7362/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7363/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7364/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7365/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7366/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7367/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7368/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7369/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7370/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7371/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7372/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7373/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7374/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7375/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7376/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7377/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7378/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7379/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7380/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7381/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7382/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7383/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7384/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7385/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7386/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7387/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7388/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7389/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7390/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7391/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7392/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7393/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7394/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7395/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7396/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7397/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7398/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7399/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7400/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7401/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7402/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7403/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7404/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7405/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7406/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7407/10000..  Training Loss: 0.146.. \n",
      "Epoch: 7408/10000..  Training Loss: 0.148.. \n",
      "Epoch: 7409/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7410/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7411/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7412/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7413/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7414/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7415/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7416/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7417/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7418/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7419/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7420/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7421/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7422/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7423/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7424/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7425/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7426/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7427/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7428/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7429/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7430/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7431/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7432/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7433/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7434/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7435/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7436/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7437/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7438/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7439/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7440/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7441/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7442/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7443/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7444/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7445/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7446/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7447/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7448/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7449/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7450/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7451/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7452/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7453/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7454/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7455/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7456/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7457/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7458/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7459/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7460/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7461/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7462/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7463/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7464/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7465/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7466/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7467/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7468/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7469/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7470/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7471/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7472/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7473/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7474/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7475/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7476/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7477/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7478/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7479/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7480/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7481/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7482/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7483/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7484/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7485/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7486/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7487/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7488/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7489/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7490/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7491/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7492/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7493/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7494/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7495/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7496/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7497/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7498/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7499/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7500/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7501/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7502/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7503/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7504/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7505/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7506/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7507/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7508/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7509/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7510/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7511/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7512/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7513/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7514/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7515/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7516/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7517/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7518/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7519/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7520/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7521/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7522/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7523/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7524/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7525/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7526/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7527/10000..  Training Loss: 0.147.. \n",
      "Epoch: 7528/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7529/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7530/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7531/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7532/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7533/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7534/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7535/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7536/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7537/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7538/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7539/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7540/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7541/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7542/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7543/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7544/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7545/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7546/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7547/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7548/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7549/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7550/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7551/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7552/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7553/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7554/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7555/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7556/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7557/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7558/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7559/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7560/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7561/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7562/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7563/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7564/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7565/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7566/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7567/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7568/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7569/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7570/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7571/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7572/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7573/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7574/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7575/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7576/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7577/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7578/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7579/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7580/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7581/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7582/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7583/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7584/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7585/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7586/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7587/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7588/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7589/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7590/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7591/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7592/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7593/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7594/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7595/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7596/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7597/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7598/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7599/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7600/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7601/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7602/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7603/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7604/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7605/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7606/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7607/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7608/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7609/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7610/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7611/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7612/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7613/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7614/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7615/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7616/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7617/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7618/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7619/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7620/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7621/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7622/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7623/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7624/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7625/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7626/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7627/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7628/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7629/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7630/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7631/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7632/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7633/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7634/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7635/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7636/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7637/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7638/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7639/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7640/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7641/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7642/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7643/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7644/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7645/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7646/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7647/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7648/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7649/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7650/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7651/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7652/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7653/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7654/10000..  Training Loss: 0.145.. \n",
      "Epoch: 7655/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7656/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7657/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7658/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7659/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7660/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7661/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7662/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7663/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7664/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7665/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7666/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7667/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7668/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7669/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7670/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7671/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7672/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7673/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7674/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7675/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7676/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7677/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7678/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7679/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7680/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7681/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7682/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7683/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7684/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7685/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7686/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7687/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7688/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7689/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7690/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7691/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7692/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7693/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7694/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7695/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7696/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7697/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7698/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7699/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7700/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7701/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7702/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7703/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7704/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7705/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7706/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7707/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7708/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7709/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7710/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7711/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7712/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7713/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7714/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7715/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7716/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7717/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7718/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7719/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7720/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7721/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7722/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7723/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7724/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7725/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7726/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7727/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7728/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7729/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7730/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7731/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7732/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7733/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7734/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7735/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7736/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7737/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7738/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7739/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7740/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7741/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7742/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7743/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7744/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7745/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7746/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7747/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7748/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7749/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7750/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7751/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7752/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7753/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7754/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7755/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7756/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7757/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7758/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7759/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7760/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7761/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7762/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7763/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7764/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7765/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7766/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7767/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7768/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7769/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7770/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7771/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7772/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7773/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7774/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7775/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7776/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7777/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7778/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7779/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7780/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7781/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7782/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7783/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7784/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7785/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7786/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7787/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7788/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7789/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7790/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7791/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7792/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7793/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7794/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7795/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7796/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7797/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7798/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7799/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7800/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7801/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7802/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7803/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7804/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7805/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7806/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7807/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7808/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7809/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7810/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7811/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7812/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7813/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7814/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7815/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7816/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7817/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7818/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7819/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7820/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7821/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7822/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7823/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7824/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7825/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7826/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7827/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7828/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7829/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7830/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7831/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7832/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7833/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7834/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7835/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7836/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7837/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7838/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7839/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7840/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7841/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7842/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7843/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7844/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7845/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7846/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7847/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7848/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7849/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7850/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7851/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7852/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7853/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7854/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7855/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7856/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7857/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7858/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7859/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7860/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7861/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7862/10000..  Training Loss: 0.144.. \n",
      "Epoch: 7863/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7864/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7865/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7866/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7867/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7868/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7869/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7870/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7871/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7872/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7873/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7874/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7875/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7876/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7877/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7878/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7879/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7880/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7881/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7882/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7883/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7884/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7885/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7886/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7887/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7888/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7889/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7890/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7891/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7892/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7893/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7894/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7895/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7896/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7897/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7898/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7899/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7900/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7901/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7902/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7903/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7904/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7905/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7906/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7907/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7908/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7909/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7910/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7911/10000..  Training Loss: 0.142.. \n",
      "Epoch: 7912/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7913/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7914/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7915/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7916/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7917/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7918/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7919/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7920/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7921/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7922/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7923/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7924/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7925/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7926/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7927/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7928/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7929/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7930/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7931/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7932/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7933/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7934/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7935/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7936/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7937/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7938/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7939/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7940/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7941/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7942/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7943/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7944/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7945/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7946/10000..  Training Loss: 0.143.. \n",
      "Epoch: 7947/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7948/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7949/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7950/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7951/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7952/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7953/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7954/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7955/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7956/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7957/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7958/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7959/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7960/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7961/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7962/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7963/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7964/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7965/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7966/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7967/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7968/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7969/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7970/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7971/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7972/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7973/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7974/10000..  Training Loss: 0.139.. \n",
      "Epoch: 7975/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7976/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7977/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7978/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7979/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7980/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7981/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7982/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7983/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7984/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7985/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7986/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7987/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7988/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7989/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7990/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7991/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7992/10000..  Training Loss: 0.135.. \n",
      "Epoch: 7993/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7994/10000..  Training Loss: 0.136.. \n",
      "Epoch: 7995/10000..  Training Loss: 0.137.. \n",
      "Epoch: 7996/10000..  Training Loss: 0.138.. \n",
      "Epoch: 7997/10000..  Training Loss: 0.141.. \n",
      "Epoch: 7998/10000..  Training Loss: 0.140.. \n",
      "Epoch: 7999/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8000/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8001/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8002/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8003/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8004/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8005/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8006/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8007/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8008/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8009/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8010/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8011/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8012/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8013/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8014/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8015/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8016/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8017/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8018/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8019/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8020/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8021/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8022/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8023/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8024/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8025/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8026/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8027/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8028/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8029/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8030/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8031/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8032/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8033/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8034/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8035/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8036/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8037/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8038/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8039/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8040/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8041/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8042/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8043/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8044/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8045/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8046/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8047/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8048/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8049/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8050/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8051/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8052/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8053/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8054/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8055/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8056/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8057/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8058/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8059/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8060/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8061/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8062/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8063/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8064/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8065/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8066/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8067/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8068/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8069/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8070/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8071/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8072/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8073/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8074/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8075/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8076/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8077/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8078/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8079/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8080/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8081/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8082/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8083/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8084/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8085/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8086/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8087/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8088/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8089/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8090/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8091/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8092/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8093/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8094/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8095/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8096/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8097/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8098/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8099/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8100/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8101/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8102/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8103/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8104/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8105/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8106/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8107/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8108/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8109/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8110/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8111/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8112/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8113/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8114/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8115/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8116/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8117/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8118/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8119/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8120/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8121/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8122/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8123/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8124/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8125/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8126/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8127/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8128/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8129/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8130/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8131/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8132/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8133/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8134/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8135/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8136/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8137/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8138/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8139/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8140/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8141/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8142/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8143/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8144/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8145/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8146/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8147/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8148/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8149/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8150/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8151/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8152/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8153/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8154/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8155/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8156/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8157/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8158/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8159/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8160/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8161/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8162/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8163/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8164/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8165/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8166/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8167/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8168/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8169/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8170/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8171/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8172/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8173/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8174/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8175/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8176/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8177/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8178/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8179/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8180/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8181/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8182/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8183/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8184/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8185/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8186/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8187/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8188/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8189/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8190/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8191/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8192/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8193/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8194/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8195/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8196/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8197/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8198/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8199/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8200/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8201/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8202/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8203/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8204/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8205/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8206/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8207/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8208/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8209/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8210/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8211/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8212/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8213/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8214/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8215/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8216/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8217/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8218/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8219/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8220/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8221/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8222/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8223/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8224/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8225/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8226/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8227/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8228/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8229/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8230/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8231/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8232/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8233/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8234/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8235/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8236/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8237/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8238/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8239/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8240/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8241/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8242/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8243/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8244/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8245/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8246/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8247/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8248/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8249/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8250/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8251/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8252/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8253/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8254/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8255/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8256/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8257/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8258/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8259/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8260/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8261/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8262/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8263/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8264/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8265/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8266/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8267/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8268/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8269/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8270/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8271/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8272/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8273/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8274/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8275/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8276/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8277/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8278/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8279/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8280/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8281/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8282/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8283/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8284/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8285/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8286/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8287/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8288/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8289/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8290/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8291/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8292/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8293/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8294/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8295/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8296/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8297/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8298/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8299/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8300/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8301/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8302/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8303/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8304/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8305/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8306/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8307/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8308/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8309/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8310/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8311/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8312/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8313/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8314/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8315/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8316/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8317/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8318/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8319/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8320/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8321/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8322/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8323/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8324/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8325/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8326/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8327/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8328/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8329/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8330/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8331/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8332/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8333/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8334/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8335/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8336/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8337/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8338/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8339/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8340/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8341/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8342/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8343/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8344/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8345/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8346/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8347/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8348/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8349/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8350/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8351/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8352/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8353/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8354/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8355/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8356/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8357/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8358/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8359/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8360/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8361/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8362/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8363/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8364/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8365/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8366/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8367/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8368/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8369/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8370/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8371/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8372/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8373/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8374/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8375/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8376/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8377/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8378/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8379/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8380/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8381/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8382/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8383/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8384/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8385/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8386/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8387/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8388/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8389/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8390/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8391/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8392/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8393/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8394/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8395/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8396/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8397/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8398/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8399/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8400/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8401/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8402/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8403/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8404/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8405/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8406/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8407/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8408/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8409/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8410/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8411/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8412/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8413/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8414/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8415/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8416/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8417/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8418/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8419/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8420/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8421/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8422/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8423/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8424/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8425/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8426/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8427/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8428/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8429/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8430/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8431/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8432/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8433/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8434/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8435/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8436/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8437/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8438/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8439/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8440/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8441/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8442/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8443/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8444/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8445/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8446/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8447/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8448/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8449/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8450/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8451/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8452/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8453/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8454/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8455/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8456/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8457/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8458/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8459/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8460/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8461/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8462/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8463/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8464/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8465/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8466/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8467/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8468/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8469/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8470/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8471/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8472/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8473/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8474/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8475/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8476/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8477/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8478/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8479/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8480/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8481/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8482/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8483/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8484/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8485/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8486/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8487/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8488/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8489/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8490/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8491/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8492/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8493/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8494/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8495/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8496/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8497/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8498/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8499/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8500/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8501/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8502/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8503/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8504/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8505/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8506/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8507/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8508/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8509/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8510/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8511/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8512/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8513/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8514/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8515/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8516/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8517/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8518/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8519/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8520/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8521/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8522/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8523/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8524/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8525/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8526/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8527/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8528/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8529/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8530/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8531/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8532/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8533/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8534/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8535/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8536/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8537/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8538/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8539/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8540/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8541/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8542/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8543/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8544/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8545/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8546/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8547/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8548/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8549/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8550/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8551/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8552/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8553/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8554/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8555/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8556/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8557/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8558/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8559/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8560/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8561/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8562/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8563/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8564/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8565/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8566/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8567/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8568/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8569/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8570/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8571/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8572/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8573/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8574/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8575/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8576/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8577/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8578/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8579/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8580/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8581/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8582/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8583/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8584/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8585/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8586/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8587/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8588/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8589/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8590/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8591/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8592/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8593/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8594/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8595/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8596/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8597/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8598/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8599/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8600/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8601/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8602/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8603/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8604/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8605/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8606/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8607/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8608/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8609/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8610/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8611/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8612/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8613/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8614/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8615/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8616/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8617/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8618/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8619/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8620/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8621/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8622/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8623/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8624/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8625/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8626/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8627/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8628/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8629/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8630/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8631/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8632/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8633/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8634/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8635/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8636/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8637/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8638/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8639/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8640/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8641/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8642/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8643/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8644/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8645/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8646/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8647/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8648/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8649/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8650/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8651/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8652/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8653/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8654/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8655/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8656/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8657/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8658/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8659/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8660/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8661/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8662/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8663/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8664/10000..  Training Loss: 0.144.. \n",
      "Epoch: 8665/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8666/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8667/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8668/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8669/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8670/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8671/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8672/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8673/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8674/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8675/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8676/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8677/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8678/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8679/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8680/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8681/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8682/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8683/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8684/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8685/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8686/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8687/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8688/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8689/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8690/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8691/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8692/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8693/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8694/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8695/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8696/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8697/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8698/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8699/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8700/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8701/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8702/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8703/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8704/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8705/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8706/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8707/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8708/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8709/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8710/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8711/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8712/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8713/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8714/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8715/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8716/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8717/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8718/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8719/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8720/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8721/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8722/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8723/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8724/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8725/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8726/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8727/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8728/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8729/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8730/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8731/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8732/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8733/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8734/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8735/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8736/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8737/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8738/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8739/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8740/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8741/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8742/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8743/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8744/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8745/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8746/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8747/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8748/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8749/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8750/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8751/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8752/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8753/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8754/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8755/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8756/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8757/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8758/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8759/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8760/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8761/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8762/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8763/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8764/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8765/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8766/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8767/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8768/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8769/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8770/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8771/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8772/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8773/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8774/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8775/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8776/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8777/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8778/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8779/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8780/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8781/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8782/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8783/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8784/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8785/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8786/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8787/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8788/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8789/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8790/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8791/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8792/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8793/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8794/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8795/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8796/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8797/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8798/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8799/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8800/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8801/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8802/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8803/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8804/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8805/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8806/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8807/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8808/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8809/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8810/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8811/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8812/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8813/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8814/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8815/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8816/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8817/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8818/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8819/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8820/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8821/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8822/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8823/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8824/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8825/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8826/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8827/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8828/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8829/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8830/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8831/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8832/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8833/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8834/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8835/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8836/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8837/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8838/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8839/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8840/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8841/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8842/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8843/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8844/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8845/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8846/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8847/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8848/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8849/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8850/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8851/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8852/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8853/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8854/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8855/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8856/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8857/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8858/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8859/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8860/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8861/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8862/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8863/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8864/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8865/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8866/10000..  Training Loss: 0.150.. \n",
      "Epoch: 8867/10000..  Training Loss: 0.141.. \n",
      "Epoch: 8868/10000..  Training Loss: 0.142.. \n",
      "Epoch: 8869/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8870/10000..  Training Loss: 0.138.. \n",
      "Epoch: 8871/10000..  Training Loss: 0.146.. \n",
      "Epoch: 8872/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8873/10000..  Training Loss: 0.139.. \n",
      "Epoch: 8874/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8875/10000..  Training Loss: 0.140.. \n",
      "Epoch: 8876/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8877/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8878/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8879/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8880/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8881/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8882/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8883/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8884/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8885/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8886/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8887/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8888/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8889/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8890/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8891/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8892/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8893/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8894/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8895/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8896/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8897/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8898/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8899/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8900/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8901/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8902/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8903/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8904/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8905/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8906/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8907/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8908/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8909/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8910/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8911/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8912/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8913/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8914/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8915/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8916/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8917/10000..  Training Loss: 0.137.. \n",
      "Epoch: 8918/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8919/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8920/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8921/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8922/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8923/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8924/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8925/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8926/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8927/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8928/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8929/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8930/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8931/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8932/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8933/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8934/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8935/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8936/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8937/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8938/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8939/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8940/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8941/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8942/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8943/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8944/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8945/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8946/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8947/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8948/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8949/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8950/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8951/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8952/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8953/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8954/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8955/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8956/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8957/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8958/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8959/10000..  Training Loss: 0.136.. \n",
      "Epoch: 8960/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8961/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8962/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8963/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8964/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8965/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8966/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8967/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8968/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8969/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8970/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8971/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8972/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8973/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8974/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8975/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8976/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8977/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8978/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8979/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8980/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8981/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8982/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8983/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8984/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8985/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8986/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8987/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8988/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8989/10000..  Training Loss: 0.132.. \n",
      "Epoch: 8990/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8991/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8992/10000..  Training Loss: 0.135.. \n",
      "Epoch: 8993/10000..  Training Loss: 0.134.. \n",
      "Epoch: 8994/10000..  Training Loss: 0.133.. \n",
      "Epoch: 8995/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8996/10000..  Training Loss: 0.131.. \n",
      "Epoch: 8997/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8998/10000..  Training Loss: 0.130.. \n",
      "Epoch: 8999/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9000/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9001/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9002/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9003/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9004/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9005/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9006/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9007/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9008/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9009/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9010/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9011/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9012/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9013/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9014/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9015/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9016/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9017/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9018/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9019/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9020/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9021/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9022/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9023/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9024/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9025/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9026/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9027/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9028/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9029/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9030/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9031/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9032/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9033/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9034/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9035/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9036/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9037/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9038/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9039/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9040/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9041/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9042/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9043/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9044/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9045/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9046/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9047/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9048/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9049/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9050/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9051/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9052/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9053/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9054/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9055/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9056/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9057/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9058/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9059/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9060/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9061/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9062/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9063/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9064/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9065/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9066/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9067/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9068/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9069/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9070/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9071/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9072/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9073/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9074/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9075/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9076/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9077/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9078/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9079/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9080/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9081/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9082/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9083/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9084/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9085/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9086/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9087/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9088/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9089/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9090/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9091/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9092/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9093/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9094/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9095/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9096/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9097/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9098/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9099/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9100/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9101/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9102/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9103/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9104/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9105/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9106/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9107/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9108/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9109/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9110/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9111/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9112/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9113/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9114/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9115/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9116/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9117/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9118/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9119/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9120/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9121/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9122/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9123/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9124/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9125/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9126/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9127/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9128/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9129/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9130/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9131/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9132/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9133/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9134/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9135/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9136/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9137/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9138/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9139/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9140/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9141/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9142/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9143/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9144/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9145/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9146/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9147/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9148/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9149/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9150/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9151/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9152/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9153/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9154/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9155/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9156/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9157/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9158/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9159/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9160/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9161/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9162/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9163/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9164/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9165/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9166/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9167/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9168/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9169/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9170/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9171/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9172/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9173/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9174/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9175/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9176/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9177/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9178/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9179/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9180/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9181/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9182/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9183/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9184/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9185/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9186/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9187/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9188/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9189/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9190/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9191/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9192/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9193/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9194/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9195/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9196/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9197/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9198/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9199/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9200/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9201/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9202/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9203/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9204/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9205/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9206/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9207/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9208/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9209/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9210/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9211/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9212/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9213/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9214/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9215/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9216/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9217/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9218/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9219/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9220/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9221/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9222/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9223/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9224/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9225/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9226/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9227/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9228/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9229/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9230/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9231/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9232/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9233/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9234/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9235/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9236/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9237/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9238/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9239/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9240/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9241/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9242/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9243/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9244/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9245/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9246/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9247/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9248/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9249/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9250/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9251/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9252/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9253/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9254/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9255/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9256/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9257/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9258/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9259/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9260/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9261/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9262/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9263/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9264/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9265/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9266/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9267/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9268/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9269/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9270/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9271/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9272/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9273/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9274/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9275/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9276/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9277/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9278/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9279/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9280/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9281/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9282/10000..  Training Loss: 0.138.. \n",
      "Epoch: 9283/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9284/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9285/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9286/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9287/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9288/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9289/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9290/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9291/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9292/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9293/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9294/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9295/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9296/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9297/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9298/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9299/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9300/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9301/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9302/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9303/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9304/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9305/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9306/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9307/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9308/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9309/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9310/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9311/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9312/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9313/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9314/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9315/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9316/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9317/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9318/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9319/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9320/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9321/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9322/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9323/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9324/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9325/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9326/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9327/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9328/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9329/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9330/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9331/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9332/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9333/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9334/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9335/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9336/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9337/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9338/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9339/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9340/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9341/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9342/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9343/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9344/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9345/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9346/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9347/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9348/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9349/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9350/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9351/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9352/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9353/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9354/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9355/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9356/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9357/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9358/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9359/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9360/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9361/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9362/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9363/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9364/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9365/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9366/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9367/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9368/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9369/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9370/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9371/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9372/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9373/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9374/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9375/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9376/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9377/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9378/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9379/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9380/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9381/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9382/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9383/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9384/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9385/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9386/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9387/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9388/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9389/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9390/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9391/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9392/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9393/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9394/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9395/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9396/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9397/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9398/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9399/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9400/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9401/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9402/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9403/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9404/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9405/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9406/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9407/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9408/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9409/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9410/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9411/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9412/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9413/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9414/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9415/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9416/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9417/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9418/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9419/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9420/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9421/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9422/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9423/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9424/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9425/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9426/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9427/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9428/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9429/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9430/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9431/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9432/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9433/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9434/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9435/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9436/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9437/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9438/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9439/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9440/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9441/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9442/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9443/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9444/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9445/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9446/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9447/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9448/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9449/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9450/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9451/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9452/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9453/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9454/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9455/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9456/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9457/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9458/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9459/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9460/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9461/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9462/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9463/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9464/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9465/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9466/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9467/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9468/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9469/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9470/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9471/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9472/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9473/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9474/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9475/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9476/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9477/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9478/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9479/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9480/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9481/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9482/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9483/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9484/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9485/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9486/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9487/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9488/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9489/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9490/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9491/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9492/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9493/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9494/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9495/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9496/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9497/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9498/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9499/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9500/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9501/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9502/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9503/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9504/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9505/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9506/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9507/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9508/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9509/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9510/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9511/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9512/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9513/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9514/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9515/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9516/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9517/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9518/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9519/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9520/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9521/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9522/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9523/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9524/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9525/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9526/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9527/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9528/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9529/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9530/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9531/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9532/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9533/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9534/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9535/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9536/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9537/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9538/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9539/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9540/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9541/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9542/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9543/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9544/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9545/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9546/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9547/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9548/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9549/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9550/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9551/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9552/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9553/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9554/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9555/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9556/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9557/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9558/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9559/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9560/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9561/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9562/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9563/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9564/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9565/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9566/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9567/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9568/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9569/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9570/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9571/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9572/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9573/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9574/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9575/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9576/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9577/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9578/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9579/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9580/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9581/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9582/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9583/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9584/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9585/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9586/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9587/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9588/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9589/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9590/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9591/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9592/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9593/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9594/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9595/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9596/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9597/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9598/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9599/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9600/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9601/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9602/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9603/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9604/10000..  Training Loss: 0.141.. \n",
      "Epoch: 9605/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9606/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9607/10000..  Training Loss: 0.147.. \n",
      "Epoch: 9608/10000..  Training Loss: 0.145.. \n",
      "Epoch: 9609/10000..  Training Loss: 0.152.. \n",
      "Epoch: 9610/10000..  Training Loss: 0.139.. \n",
      "Epoch: 9611/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9612/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9613/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9614/10000..  Training Loss: 0.143.. \n",
      "Epoch: 9615/10000..  Training Loss: 0.140.. \n",
      "Epoch: 9616/10000..  Training Loss: 0.144.. \n",
      "Epoch: 9617/10000..  Training Loss: 0.137.. \n",
      "Epoch: 9618/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9619/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9620/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9621/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9622/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9623/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9624/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9625/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9626/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9627/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9628/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9629/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9630/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9631/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9632/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9633/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9634/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9635/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9636/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9637/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9638/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9639/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9640/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9641/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9642/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9643/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9644/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9645/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9646/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9647/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9648/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9649/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9650/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9651/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9652/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9653/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9654/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9655/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9656/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9657/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9658/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9659/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9660/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9661/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9662/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9663/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9664/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9665/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9666/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9667/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9668/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9669/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9670/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9671/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9672/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9673/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9674/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9675/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9676/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9677/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9678/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9679/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9680/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9681/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9682/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9683/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9684/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9685/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9686/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9687/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9688/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9689/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9690/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9691/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9692/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9693/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9694/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9695/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9696/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9697/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9698/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9699/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9700/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9701/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9702/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9703/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9704/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9705/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9706/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9707/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9708/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9709/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9710/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9711/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9712/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9713/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9714/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9715/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9716/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9717/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9718/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9719/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9720/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9721/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9722/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9723/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9724/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9725/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9726/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9727/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9728/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9729/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9730/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9731/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9732/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9733/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9734/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9735/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9736/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9737/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9738/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9739/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9740/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9741/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9742/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9743/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9744/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9745/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9746/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9747/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9748/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9749/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9750/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9751/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9752/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9753/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9754/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9755/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9756/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9757/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9758/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9759/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9760/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9761/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9762/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9763/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9764/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9765/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9766/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9767/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9768/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9769/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9770/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9771/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9772/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9773/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9774/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9775/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9776/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9777/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9778/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9779/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9780/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9781/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9782/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9783/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9784/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9785/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9786/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9787/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9788/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9789/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9790/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9791/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9792/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9793/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9794/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9795/10000..  Training Loss: 0.135.. \n",
      "Epoch: 9796/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9797/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9798/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9799/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9800/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9801/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9802/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9803/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9804/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9805/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9806/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9807/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9808/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9809/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9810/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9811/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9812/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9813/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9814/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9815/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9816/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9817/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9818/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9819/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9820/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9821/10000..  Training Loss: 0.136.. \n",
      "Epoch: 9822/10000..  Training Loss: 0.134.. \n",
      "Epoch: 9823/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9824/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9825/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9826/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9827/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9828/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9829/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9830/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9831/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9832/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9833/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9834/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9835/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9836/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9837/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9838/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9839/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9840/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9841/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9842/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9843/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9844/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9845/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9846/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9847/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9848/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9849/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9850/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9851/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9852/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9853/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9854/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9855/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9856/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9857/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9858/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9859/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9860/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9861/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9862/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9863/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9864/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9865/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9866/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9867/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9868/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9869/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9870/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9871/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9872/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9873/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9874/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9875/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9876/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9877/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9878/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9879/10000..  Training Loss: 0.126.. \n",
      "Epoch: 9880/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9881/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9882/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9883/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9884/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9885/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9886/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9887/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9888/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9889/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9890/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9891/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9892/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9893/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9894/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9895/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9896/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9897/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9898/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9899/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9900/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9901/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9902/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9903/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9904/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9905/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9906/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9907/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9908/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9909/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9910/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9911/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9912/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9913/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9914/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9915/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9916/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9917/10000..  Training Loss: 0.126.. \n",
      "Epoch: 9918/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9919/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9920/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9921/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9922/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9923/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9924/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9925/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9926/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9927/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9928/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9929/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9930/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9931/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9932/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9933/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9934/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9935/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9936/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9937/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9938/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9939/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9940/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9941/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9942/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9943/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9944/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9945/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9946/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9947/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9948/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9949/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9950/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9951/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9952/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9953/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9954/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9955/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9956/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9957/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9958/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9959/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9960/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9961/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9962/10000..  Training Loss: 0.132.. \n",
      "Epoch: 9963/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9964/10000..  Training Loss: 0.130.. \n",
      "Epoch: 9965/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9966/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9967/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9968/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9969/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9970/10000..  Training Loss: 0.126.. \n",
      "Epoch: 9971/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9972/10000..  Training Loss: 0.126.. \n",
      "Epoch: 9973/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9974/10000..  Training Loss: 0.126.. \n",
      "Epoch: 9975/10000..  Training Loss: 0.126.. \n",
      "Epoch: 9976/10000..  Training Loss: 0.126.. \n",
      "Epoch: 9977/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9978/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9979/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9980/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9981/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9982/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9983/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9984/10000..  Training Loss: 0.133.. \n",
      "Epoch: 9985/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9986/10000..  Training Loss: 0.131.. \n",
      "Epoch: 9987/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9988/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9989/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9990/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9991/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9992/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9993/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9994/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9995/10000..  Training Loss: 0.127.. \n",
      "Epoch: 9996/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9997/10000..  Training Loss: 0.128.. \n",
      "Epoch: 9998/10000..  Training Loss: 0.129.. \n",
      "Epoch: 9999/10000..  Training Loss: 0.129.. \n",
      "Epoch: 10000/10000..  Training Loss: 0.130.. \n",
      "Epoch: 1/10000..  Training Loss: 0.676.. \n",
      "Epoch: 2/10000..  Training Loss: 0.514.. \n",
      "Epoch: 3/10000..  Training Loss: 0.469.. \n",
      "Epoch: 4/10000..  Training Loss: 0.465.. \n",
      "Epoch: 5/10000..  Training Loss: 0.449.. \n",
      "Epoch: 6/10000..  Training Loss: 0.427.. \n",
      "Epoch: 7/10000..  Training Loss: 0.415.. \n",
      "Epoch: 8/10000..  Training Loss: 0.415.. \n",
      "Epoch: 9/10000..  Training Loss: 0.416.. \n",
      "Epoch: 10/10000..  Training Loss: 0.412.. \n",
      "Epoch: 11/10000..  Training Loss: 0.407.. \n",
      "Epoch: 12/10000..  Training Loss: 0.402.. \n",
      "Epoch: 13/10000..  Training Loss: 0.398.. \n",
      "Epoch: 14/10000..  Training Loss: 0.396.. \n",
      "Epoch: 15/10000..  Training Loss: 0.395.. \n",
      "Epoch: 16/10000..  Training Loss: 0.395.. \n",
      "Epoch: 17/10000..  Training Loss: 0.392.. \n",
      "Epoch: 18/10000..  Training Loss: 0.389.. \n",
      "Epoch: 19/10000..  Training Loss: 0.387.. \n",
      "Epoch: 20/10000..  Training Loss: 0.385.. \n",
      "Epoch: 21/10000..  Training Loss: 0.384.. \n",
      "Epoch: 22/10000..  Training Loss: 0.383.. \n",
      "Epoch: 23/10000..  Training Loss: 0.381.. \n",
      "Epoch: 24/10000..  Training Loss: 0.379.. \n",
      "Epoch: 25/10000..  Training Loss: 0.377.. \n",
      "Epoch: 26/10000..  Training Loss: 0.376.. \n",
      "Epoch: 27/10000..  Training Loss: 0.374.. \n",
      "Epoch: 28/10000..  Training Loss: 0.373.. \n",
      "Epoch: 29/10000..  Training Loss: 0.372.. \n",
      "Epoch: 30/10000..  Training Loss: 0.370.. \n",
      "Epoch: 31/10000..  Training Loss: 0.369.. \n",
      "Epoch: 32/10000..  Training Loss: 0.367.. \n",
      "Epoch: 33/10000..  Training Loss: 0.366.. \n",
      "Epoch: 34/10000..  Training Loss: 0.365.. \n",
      "Epoch: 35/10000..  Training Loss: 0.363.. \n",
      "Epoch: 36/10000..  Training Loss: 0.362.. \n",
      "Epoch: 37/10000..  Training Loss: 0.360.. \n",
      "Epoch: 38/10000..  Training Loss: 0.359.. \n",
      "Epoch: 39/10000..  Training Loss: 0.358.. \n",
      "Epoch: 40/10000..  Training Loss: 0.356.. \n",
      "Epoch: 41/10000..  Training Loss: 0.355.. \n",
      "Epoch: 42/10000..  Training Loss: 0.354.. \n",
      "Epoch: 43/10000..  Training Loss: 0.352.. \n",
      "Epoch: 44/10000..  Training Loss: 0.351.. \n",
      "Epoch: 45/10000..  Training Loss: 0.349.. \n",
      "Epoch: 46/10000..  Training Loss: 0.348.. \n",
      "Epoch: 47/10000..  Training Loss: 0.347.. \n",
      "Epoch: 48/10000..  Training Loss: 0.345.. \n",
      "Epoch: 49/10000..  Training Loss: 0.344.. \n",
      "Epoch: 50/10000..  Training Loss: 0.342.. \n",
      "Epoch: 51/10000..  Training Loss: 0.341.. \n",
      "Epoch: 52/10000..  Training Loss: 0.340.. \n",
      "Epoch: 53/10000..  Training Loss: 0.338.. \n",
      "Epoch: 54/10000..  Training Loss: 0.337.. \n",
      "Epoch: 55/10000..  Training Loss: 0.336.. \n",
      "Epoch: 56/10000..  Training Loss: 0.334.. \n",
      "Epoch: 57/10000..  Training Loss: 0.333.. \n",
      "Epoch: 58/10000..  Training Loss: 0.332.. \n",
      "Epoch: 59/10000..  Training Loss: 0.330.. \n",
      "Epoch: 60/10000..  Training Loss: 0.329.. \n",
      "Epoch: 61/10000..  Training Loss: 0.330.. \n",
      "Epoch: 62/10000..  Training Loss: 0.333.. \n",
      "Epoch: 63/10000..  Training Loss: 0.332.. \n",
      "Epoch: 64/10000..  Training Loss: 0.325.. \n",
      "Epoch: 65/10000..  Training Loss: 0.326.. \n",
      "Epoch: 66/10000..  Training Loss: 0.327.. \n",
      "Epoch: 67/10000..  Training Loss: 0.322.. \n",
      "Epoch: 68/10000..  Training Loss: 0.323.. \n",
      "Epoch: 69/10000..  Training Loss: 0.323.. \n",
      "Epoch: 70/10000..  Training Loss: 0.320.. \n",
      "Epoch: 71/10000..  Training Loss: 0.320.. \n",
      "Epoch: 72/10000..  Training Loss: 0.320.. \n",
      "Epoch: 73/10000..  Training Loss: 0.317.. \n",
      "Epoch: 74/10000..  Training Loss: 0.318.. \n",
      "Epoch: 75/10000..  Training Loss: 0.317.. \n",
      "Epoch: 76/10000..  Training Loss: 0.315.. \n",
      "Epoch: 77/10000..  Training Loss: 0.315.. \n",
      "Epoch: 78/10000..  Training Loss: 0.315.. \n",
      "Epoch: 79/10000..  Training Loss: 0.313.. \n",
      "Epoch: 80/10000..  Training Loss: 0.312.. \n",
      "Epoch: 81/10000..  Training Loss: 0.312.. \n",
      "Epoch: 82/10000..  Training Loss: 0.311.. \n",
      "Epoch: 83/10000..  Training Loss: 0.310.. \n",
      "Epoch: 84/10000..  Training Loss: 0.310.. \n",
      "Epoch: 85/10000..  Training Loss: 0.309.. \n",
      "Epoch: 86/10000..  Training Loss: 0.308.. \n",
      "Epoch: 87/10000..  Training Loss: 0.307.. \n",
      "Epoch: 88/10000..  Training Loss: 0.307.. \n",
      "Epoch: 89/10000..  Training Loss: 0.306.. \n",
      "Epoch: 90/10000..  Training Loss: 0.306.. \n",
      "Epoch: 91/10000..  Training Loss: 0.305.. \n",
      "Epoch: 92/10000..  Training Loss: 0.304.. \n",
      "Epoch: 93/10000..  Training Loss: 0.304.. \n",
      "Epoch: 94/10000..  Training Loss: 0.303.. \n",
      "Epoch: 95/10000..  Training Loss: 0.303.. \n",
      "Epoch: 96/10000..  Training Loss: 0.302.. \n",
      "Epoch: 97/10000..  Training Loss: 0.302.. \n",
      "Epoch: 98/10000..  Training Loss: 0.302.. \n",
      "Epoch: 99/10000..  Training Loss: 0.302.. \n",
      "Epoch: 100/10000..  Training Loss: 0.301.. \n",
      "Epoch: 101/10000..  Training Loss: 0.301.. \n",
      "Epoch: 102/10000..  Training Loss: 0.300.. \n",
      "Epoch: 103/10000..  Training Loss: 0.299.. \n",
      "Epoch: 104/10000..  Training Loss: 0.298.. \n",
      "Epoch: 105/10000..  Training Loss: 0.297.. \n",
      "Epoch: 106/10000..  Training Loss: 0.296.. \n",
      "Epoch: 107/10000..  Training Loss: 0.296.. \n",
      "Epoch: 108/10000..  Training Loss: 0.295.. \n",
      "Epoch: 109/10000..  Training Loss: 0.295.. \n",
      "Epoch: 110/10000..  Training Loss: 0.295.. \n",
      "Epoch: 111/10000..  Training Loss: 0.297.. \n",
      "Epoch: 112/10000..  Training Loss: 0.300.. \n",
      "Epoch: 113/10000..  Training Loss: 0.302.. \n",
      "Epoch: 114/10000..  Training Loss: 0.301.. \n",
      "Epoch: 115/10000..  Training Loss: 0.294.. \n",
      "Epoch: 116/10000..  Training Loss: 0.292.. \n",
      "Epoch: 117/10000..  Training Loss: 0.296.. \n",
      "Epoch: 118/10000..  Training Loss: 0.297.. \n",
      "Epoch: 119/10000..  Training Loss: 0.292.. \n",
      "Epoch: 120/10000..  Training Loss: 0.290.. \n",
      "Epoch: 121/10000..  Training Loss: 0.293.. \n",
      "Epoch: 122/10000..  Training Loss: 0.294.. \n",
      "Epoch: 123/10000..  Training Loss: 0.290.. \n",
      "Epoch: 124/10000..  Training Loss: 0.289.. \n",
      "Epoch: 125/10000..  Training Loss: 0.290.. \n",
      "Epoch: 126/10000..  Training Loss: 0.291.. \n",
      "Epoch: 127/10000..  Training Loss: 0.288.. \n",
      "Epoch: 128/10000..  Training Loss: 0.287.. \n",
      "Epoch: 129/10000..  Training Loss: 0.288.. \n",
      "Epoch: 130/10000..  Training Loss: 0.288.. \n",
      "Epoch: 131/10000..  Training Loss: 0.287.. \n",
      "Epoch: 132/10000..  Training Loss: 0.285.. \n",
      "Epoch: 133/10000..  Training Loss: 0.285.. \n",
      "Epoch: 134/10000..  Training Loss: 0.286.. \n",
      "Epoch: 135/10000..  Training Loss: 0.286.. \n",
      "Epoch: 136/10000..  Training Loss: 0.285.. \n",
      "Epoch: 137/10000..  Training Loss: 0.284.. \n",
      "Epoch: 138/10000..  Training Loss: 0.283.. \n",
      "Epoch: 139/10000..  Training Loss: 0.283.. \n",
      "Epoch: 140/10000..  Training Loss: 0.284.. \n",
      "Epoch: 141/10000..  Training Loss: 0.283.. \n",
      "Epoch: 142/10000..  Training Loss: 0.282.. \n",
      "Epoch: 143/10000..  Training Loss: 0.281.. \n",
      "Epoch: 144/10000..  Training Loss: 0.281.. \n",
      "Epoch: 145/10000..  Training Loss: 0.281.. \n",
      "Epoch: 146/10000..  Training Loss: 0.281.. \n",
      "Epoch: 147/10000..  Training Loss: 0.281.. \n",
      "Epoch: 148/10000..  Training Loss: 0.281.. \n",
      "Epoch: 149/10000..  Training Loss: 0.280.. \n",
      "Epoch: 150/10000..  Training Loss: 0.280.. \n",
      "Epoch: 151/10000..  Training Loss: 0.280.. \n",
      "Epoch: 152/10000..  Training Loss: 0.279.. \n",
      "Epoch: 153/10000..  Training Loss: 0.279.. \n",
      "Epoch: 154/10000..  Training Loss: 0.278.. \n",
      "Epoch: 155/10000..  Training Loss: 0.278.. \n",
      "Epoch: 156/10000..  Training Loss: 0.278.. \n",
      "Epoch: 157/10000..  Training Loss: 0.278.. \n",
      "Epoch: 158/10000..  Training Loss: 0.278.. \n",
      "Epoch: 159/10000..  Training Loss: 0.278.. \n",
      "Epoch: 160/10000..  Training Loss: 0.279.. \n",
      "Epoch: 161/10000..  Training Loss: 0.280.. \n",
      "Epoch: 162/10000..  Training Loss: 0.281.. \n",
      "Epoch: 163/10000..  Training Loss: 0.281.. \n",
      "Epoch: 164/10000..  Training Loss: 0.279.. \n",
      "Epoch: 165/10000..  Training Loss: 0.276.. \n",
      "Epoch: 166/10000..  Training Loss: 0.274.. \n",
      "Epoch: 167/10000..  Training Loss: 0.274.. \n",
      "Epoch: 168/10000..  Training Loss: 0.275.. \n",
      "Epoch: 169/10000..  Training Loss: 0.277.. \n",
      "Epoch: 170/10000..  Training Loss: 0.277.. \n",
      "Epoch: 171/10000..  Training Loss: 0.276.. \n",
      "Epoch: 172/10000..  Training Loss: 0.274.. \n",
      "Epoch: 173/10000..  Training Loss: 0.272.. \n",
      "Epoch: 174/10000..  Training Loss: 0.272.. \n",
      "Epoch: 175/10000..  Training Loss: 0.272.. \n",
      "Epoch: 176/10000..  Training Loss: 0.273.. \n",
      "Epoch: 177/10000..  Training Loss: 0.274.. \n",
      "Epoch: 178/10000..  Training Loss: 0.274.. \n",
      "Epoch: 179/10000..  Training Loss: 0.273.. \n",
      "Epoch: 180/10000..  Training Loss: 0.271.. \n",
      "Epoch: 181/10000..  Training Loss: 0.270.. \n",
      "Epoch: 182/10000..  Training Loss: 0.270.. \n",
      "Epoch: 183/10000..  Training Loss: 0.270.. \n",
      "Epoch: 184/10000..  Training Loss: 0.270.. \n",
      "Epoch: 185/10000..  Training Loss: 0.271.. \n",
      "Epoch: 186/10000..  Training Loss: 0.272.. \n",
      "Epoch: 187/10000..  Training Loss: 0.272.. \n",
      "Epoch: 188/10000..  Training Loss: 0.271.. \n",
      "Epoch: 189/10000..  Training Loss: 0.270.. \n",
      "Epoch: 190/10000..  Training Loss: 0.268.. \n",
      "Epoch: 191/10000..  Training Loss: 0.267.. \n",
      "Epoch: 192/10000..  Training Loss: 0.267.. \n",
      "Epoch: 193/10000..  Training Loss: 0.267.. \n",
      "Epoch: 194/10000..  Training Loss: 0.268.. \n",
      "Epoch: 195/10000..  Training Loss: 0.269.. \n",
      "Epoch: 196/10000..  Training Loss: 0.269.. \n",
      "Epoch: 197/10000..  Training Loss: 0.270.. \n",
      "Epoch: 198/10000..  Training Loss: 0.270.. \n",
      "Epoch: 199/10000..  Training Loss: 0.269.. \n",
      "Epoch: 200/10000..  Training Loss: 0.267.. \n",
      "Epoch: 201/10000..  Training Loss: 0.266.. \n",
      "Epoch: 202/10000..  Training Loss: 0.265.. \n",
      "Epoch: 203/10000..  Training Loss: 0.265.. \n",
      "Epoch: 204/10000..  Training Loss: 0.266.. \n",
      "Epoch: 205/10000..  Training Loss: 0.267.. \n",
      "Epoch: 206/10000..  Training Loss: 0.268.. \n",
      "Epoch: 207/10000..  Training Loss: 0.267.. \n",
      "Epoch: 208/10000..  Training Loss: 0.266.. \n",
      "Epoch: 209/10000..  Training Loss: 0.265.. \n",
      "Epoch: 210/10000..  Training Loss: 0.263.. \n",
      "Epoch: 211/10000..  Training Loss: 0.263.. \n",
      "Epoch: 212/10000..  Training Loss: 0.263.. \n",
      "Epoch: 213/10000..  Training Loss: 0.264.. \n",
      "Epoch: 214/10000..  Training Loss: 0.264.. \n",
      "Epoch: 215/10000..  Training Loss: 0.266.. \n",
      "Epoch: 216/10000..  Training Loss: 0.266.. \n",
      "Epoch: 217/10000..  Training Loss: 0.266.. \n",
      "Epoch: 218/10000..  Training Loss: 0.264.. \n",
      "Epoch: 219/10000..  Training Loss: 0.262.. \n",
      "Epoch: 220/10000..  Training Loss: 0.261.. \n",
      "Epoch: 221/10000..  Training Loss: 0.261.. \n",
      "Epoch: 222/10000..  Training Loss: 0.261.. \n",
      "Epoch: 223/10000..  Training Loss: 0.262.. \n",
      "Epoch: 224/10000..  Training Loss: 0.263.. \n",
      "Epoch: 225/10000..  Training Loss: 0.264.. \n",
      "Epoch: 226/10000..  Training Loss: 0.265.. \n",
      "Epoch: 227/10000..  Training Loss: 0.264.. \n",
      "Epoch: 228/10000..  Training Loss: 0.263.. \n",
      "Epoch: 229/10000..  Training Loss: 0.261.. \n",
      "Epoch: 230/10000..  Training Loss: 0.259.. \n",
      "Epoch: 231/10000..  Training Loss: 0.259.. \n",
      "Epoch: 232/10000..  Training Loss: 0.259.. \n",
      "Epoch: 233/10000..  Training Loss: 0.261.. \n",
      "Epoch: 234/10000..  Training Loss: 0.261.. \n",
      "Epoch: 235/10000..  Training Loss: 0.262.. \n",
      "Epoch: 236/10000..  Training Loss: 0.261.. \n",
      "Epoch: 237/10000..  Training Loss: 0.260.. \n",
      "Epoch: 238/10000..  Training Loss: 0.258.. \n",
      "Epoch: 239/10000..  Training Loss: 0.257.. \n",
      "Epoch: 240/10000..  Training Loss: 0.257.. \n",
      "Epoch: 241/10000..  Training Loss: 0.257.. \n",
      "Epoch: 242/10000..  Training Loss: 0.258.. \n",
      "Epoch: 243/10000..  Training Loss: 0.258.. \n",
      "Epoch: 244/10000..  Training Loss: 0.259.. \n",
      "Epoch: 245/10000..  Training Loss: 0.260.. \n",
      "Epoch: 246/10000..  Training Loss: 0.260.. \n",
      "Epoch: 247/10000..  Training Loss: 0.259.. \n",
      "Epoch: 248/10000..  Training Loss: 0.258.. \n",
      "Epoch: 249/10000..  Training Loss: 0.256.. \n",
      "Epoch: 250/10000..  Training Loss: 0.255.. \n",
      "Epoch: 251/10000..  Training Loss: 0.255.. \n",
      "Epoch: 252/10000..  Training Loss: 0.255.. \n",
      "Epoch: 253/10000..  Training Loss: 0.256.. \n",
      "Epoch: 254/10000..  Training Loss: 0.257.. \n",
      "Epoch: 255/10000..  Training Loss: 0.258.. \n",
      "Epoch: 256/10000..  Training Loss: 0.259.. \n",
      "Epoch: 257/10000..  Training Loss: 0.260.. \n",
      "Epoch: 258/10000..  Training Loss: 0.259.. \n",
      "Epoch: 259/10000..  Training Loss: 0.257.. \n",
      "Epoch: 260/10000..  Training Loss: 0.255.. \n",
      "Epoch: 261/10000..  Training Loss: 0.253.. \n",
      "Epoch: 262/10000..  Training Loss: 0.254.. \n",
      "Epoch: 263/10000..  Training Loss: 0.255.. \n",
      "Epoch: 264/10000..  Training Loss: 0.256.. \n",
      "Epoch: 265/10000..  Training Loss: 0.257.. \n",
      "Epoch: 266/10000..  Training Loss: 0.257.. \n",
      "Epoch: 267/10000..  Training Loss: 0.256.. \n",
      "Epoch: 268/10000..  Training Loss: 0.254.. \n",
      "Epoch: 269/10000..  Training Loss: 0.253.. \n",
      "Epoch: 270/10000..  Training Loss: 0.252.. \n",
      "Epoch: 271/10000..  Training Loss: 0.252.. \n",
      "Epoch: 272/10000..  Training Loss: 0.253.. \n",
      "Epoch: 273/10000..  Training Loss: 0.254.. \n",
      "Epoch: 274/10000..  Training Loss: 0.255.. \n",
      "Epoch: 275/10000..  Training Loss: 0.256.. \n",
      "Epoch: 276/10000..  Training Loss: 0.254.. \n",
      "Epoch: 277/10000..  Training Loss: 0.253.. \n",
      "Epoch: 278/10000..  Training Loss: 0.251.. \n",
      "Epoch: 279/10000..  Training Loss: 0.251.. \n",
      "Epoch: 280/10000..  Training Loss: 0.251.. \n",
      "Epoch: 281/10000..  Training Loss: 0.252.. \n",
      "Epoch: 282/10000..  Training Loss: 0.253.. \n",
      "Epoch: 283/10000..  Training Loss: 0.254.. \n",
      "Epoch: 284/10000..  Training Loss: 0.255.. \n",
      "Epoch: 285/10000..  Training Loss: 0.254.. \n",
      "Epoch: 286/10000..  Training Loss: 0.254.. \n",
      "Epoch: 287/10000..  Training Loss: 0.252.. \n",
      "Epoch: 288/10000..  Training Loss: 0.250.. \n",
      "Epoch: 289/10000..  Training Loss: 0.249.. \n",
      "Epoch: 290/10000..  Training Loss: 0.249.. \n",
      "Epoch: 291/10000..  Training Loss: 0.250.. \n",
      "Epoch: 292/10000..  Training Loss: 0.251.. \n",
      "Epoch: 293/10000..  Training Loss: 0.253.. \n",
      "Epoch: 294/10000..  Training Loss: 0.253.. \n",
      "Epoch: 295/10000..  Training Loss: 0.253.. \n",
      "Epoch: 296/10000..  Training Loss: 0.252.. \n",
      "Epoch: 297/10000..  Training Loss: 0.250.. \n",
      "Epoch: 298/10000..  Training Loss: 0.248.. \n",
      "Epoch: 299/10000..  Training Loss: 0.248.. \n",
      "Epoch: 300/10000..  Training Loss: 0.249.. \n",
      "Epoch: 301/10000..  Training Loss: 0.250.. \n",
      "Epoch: 302/10000..  Training Loss: 0.251.. \n",
      "Epoch: 303/10000..  Training Loss: 0.251.. \n",
      "Epoch: 304/10000..  Training Loss: 0.251.. \n",
      "Epoch: 305/10000..  Training Loss: 0.250.. \n",
      "Epoch: 306/10000..  Training Loss: 0.249.. \n",
      "Epoch: 307/10000..  Training Loss: 0.247.. \n",
      "Epoch: 308/10000..  Training Loss: 0.247.. \n",
      "Epoch: 309/10000..  Training Loss: 0.247.. \n",
      "Epoch: 310/10000..  Training Loss: 0.248.. \n",
      "Epoch: 311/10000..  Training Loss: 0.249.. \n",
      "Epoch: 312/10000..  Training Loss: 0.250.. \n",
      "Epoch: 313/10000..  Training Loss: 0.251.. \n",
      "Epoch: 314/10000..  Training Loss: 0.250.. \n",
      "Epoch: 315/10000..  Training Loss: 0.248.. \n",
      "Epoch: 316/10000..  Training Loss: 0.246.. \n",
      "Epoch: 317/10000..  Training Loss: 0.245.. \n",
      "Epoch: 318/10000..  Training Loss: 0.246.. \n",
      "Epoch: 319/10000..  Training Loss: 0.246.. \n",
      "Epoch: 320/10000..  Training Loss: 0.248.. \n",
      "Epoch: 321/10000..  Training Loss: 0.248.. \n",
      "Epoch: 322/10000..  Training Loss: 0.249.. \n",
      "Epoch: 323/10000..  Training Loss: 0.248.. \n",
      "Epoch: 324/10000..  Training Loss: 0.247.. \n",
      "Epoch: 325/10000..  Training Loss: 0.246.. \n",
      "Epoch: 326/10000..  Training Loss: 0.245.. \n",
      "Epoch: 327/10000..  Training Loss: 0.244.. \n",
      "Epoch: 328/10000..  Training Loss: 0.244.. \n",
      "Epoch: 329/10000..  Training Loss: 0.244.. \n",
      "Epoch: 330/10000..  Training Loss: 0.245.. \n",
      "Epoch: 331/10000..  Training Loss: 0.245.. \n",
      "Epoch: 332/10000..  Training Loss: 0.246.. \n",
      "Epoch: 333/10000..  Training Loss: 0.247.. \n",
      "Epoch: 334/10000..  Training Loss: 0.247.. \n",
      "Epoch: 335/10000..  Training Loss: 0.248.. \n",
      "Epoch: 336/10000..  Training Loss: 0.248.. \n",
      "Epoch: 337/10000..  Training Loss: 0.247.. \n",
      "Epoch: 338/10000..  Training Loss: 0.245.. \n",
      "Epoch: 339/10000..  Training Loss: 0.244.. \n",
      "Epoch: 340/10000..  Training Loss: 0.243.. \n",
      "Epoch: 341/10000..  Training Loss: 0.242.. \n",
      "Epoch: 342/10000..  Training Loss: 0.243.. \n",
      "Epoch: 343/10000..  Training Loss: 0.244.. \n",
      "Epoch: 344/10000..  Training Loss: 0.245.. \n",
      "Epoch: 345/10000..  Training Loss: 0.246.. \n",
      "Epoch: 346/10000..  Training Loss: 0.248.. \n",
      "Epoch: 347/10000..  Training Loss: 0.247.. \n",
      "Epoch: 348/10000..  Training Loss: 0.246.. \n",
      "Epoch: 349/10000..  Training Loss: 0.244.. \n",
      "Epoch: 350/10000..  Training Loss: 0.242.. \n",
      "Epoch: 351/10000..  Training Loss: 0.241.. \n",
      "Epoch: 352/10000..  Training Loss: 0.242.. \n",
      "Epoch: 353/10000..  Training Loss: 0.243.. \n",
      "Epoch: 354/10000..  Training Loss: 0.244.. \n",
      "Epoch: 355/10000..  Training Loss: 0.245.. \n",
      "Epoch: 356/10000..  Training Loss: 0.246.. \n",
      "Epoch: 357/10000..  Training Loss: 0.245.. \n",
      "Epoch: 358/10000..  Training Loss: 0.243.. \n",
      "Epoch: 359/10000..  Training Loss: 0.241.. \n",
      "Epoch: 360/10000..  Training Loss: 0.240.. \n",
      "Epoch: 361/10000..  Training Loss: 0.240.. \n",
      "Epoch: 362/10000..  Training Loss: 0.241.. \n",
      "Epoch: 363/10000..  Training Loss: 0.242.. \n",
      "Epoch: 364/10000..  Training Loss: 0.244.. \n",
      "Epoch: 365/10000..  Training Loss: 0.244.. \n",
      "Epoch: 366/10000..  Training Loss: 0.245.. \n",
      "Epoch: 367/10000..  Training Loss: 0.243.. \n",
      "Epoch: 368/10000..  Training Loss: 0.241.. \n",
      "Epoch: 369/10000..  Training Loss: 0.240.. \n",
      "Epoch: 370/10000..  Training Loss: 0.239.. \n",
      "Epoch: 371/10000..  Training Loss: 0.239.. \n",
      "Epoch: 372/10000..  Training Loss: 0.240.. \n",
      "Epoch: 373/10000..  Training Loss: 0.241.. \n",
      "Epoch: 374/10000..  Training Loss: 0.241.. \n",
      "Epoch: 375/10000..  Training Loss: 0.242.. \n",
      "Epoch: 376/10000..  Training Loss: 0.243.. \n",
      "Epoch: 377/10000..  Training Loss: 0.242.. \n",
      "Epoch: 378/10000..  Training Loss: 0.241.. \n",
      "Epoch: 379/10000..  Training Loss: 0.239.. \n",
      "Epoch: 380/10000..  Training Loss: 0.238.. \n",
      "Epoch: 381/10000..  Training Loss: 0.238.. \n",
      "Epoch: 382/10000..  Training Loss: 0.238.. \n",
      "Epoch: 383/10000..  Training Loss: 0.238.. \n",
      "Epoch: 384/10000..  Training Loss: 0.239.. \n",
      "Epoch: 385/10000..  Training Loss: 0.240.. \n",
      "Epoch: 386/10000..  Training Loss: 0.241.. \n",
      "Epoch: 387/10000..  Training Loss: 0.242.. \n",
      "Epoch: 388/10000..  Training Loss: 0.243.. \n",
      "Epoch: 389/10000..  Training Loss: 0.242.. \n",
      "Epoch: 390/10000..  Training Loss: 0.241.. \n",
      "Epoch: 391/10000..  Training Loss: 0.238.. \n",
      "Epoch: 392/10000..  Training Loss: 0.237.. \n",
      "Epoch: 393/10000..  Training Loss: 0.236.. \n",
      "Epoch: 394/10000..  Training Loss: 0.237.. \n",
      "Epoch: 395/10000..  Training Loss: 0.238.. \n",
      "Epoch: 396/10000..  Training Loss: 0.240.. \n",
      "Epoch: 397/10000..  Training Loss: 0.241.. \n",
      "Epoch: 398/10000..  Training Loss: 0.241.. \n",
      "Epoch: 399/10000..  Training Loss: 0.240.. \n",
      "Epoch: 400/10000..  Training Loss: 0.238.. \n",
      "Epoch: 401/10000..  Training Loss: 0.236.. \n",
      "Epoch: 402/10000..  Training Loss: 0.235.. \n",
      "Epoch: 403/10000..  Training Loss: 0.236.. \n",
      "Epoch: 404/10000..  Training Loss: 0.237.. \n",
      "Epoch: 405/10000..  Training Loss: 0.238.. \n",
      "Epoch: 406/10000..  Training Loss: 0.239.. \n",
      "Epoch: 407/10000..  Training Loss: 0.240.. \n",
      "Epoch: 408/10000..  Training Loss: 0.240.. \n",
      "Epoch: 409/10000..  Training Loss: 0.239.. \n",
      "Epoch: 410/10000..  Training Loss: 0.237.. \n",
      "Epoch: 411/10000..  Training Loss: 0.235.. \n",
      "Epoch: 412/10000..  Training Loss: 0.235.. \n",
      "Epoch: 413/10000..  Training Loss: 0.235.. \n",
      "Epoch: 414/10000..  Training Loss: 0.235.. \n",
      "Epoch: 415/10000..  Training Loss: 0.237.. \n",
      "Epoch: 416/10000..  Training Loss: 0.237.. \n",
      "Epoch: 417/10000..  Training Loss: 0.238.. \n",
      "Epoch: 418/10000..  Training Loss: 0.238.. \n",
      "Epoch: 419/10000..  Training Loss: 0.238.. \n",
      "Epoch: 420/10000..  Training Loss: 0.236.. \n",
      "Epoch: 421/10000..  Training Loss: 0.235.. \n",
      "Epoch: 422/10000..  Training Loss: 0.234.. \n",
      "Epoch: 423/10000..  Training Loss: 0.233.. \n",
      "Epoch: 424/10000..  Training Loss: 0.234.. \n",
      "Epoch: 425/10000..  Training Loss: 0.234.. \n",
      "Epoch: 426/10000..  Training Loss: 0.235.. \n",
      "Epoch: 427/10000..  Training Loss: 0.235.. \n",
      "Epoch: 428/10000..  Training Loss: 0.236.. \n",
      "Epoch: 429/10000..  Training Loss: 0.237.. \n",
      "Epoch: 430/10000..  Training Loss: 0.237.. \n",
      "Epoch: 431/10000..  Training Loss: 0.236.. \n",
      "Epoch: 432/10000..  Training Loss: 0.235.. \n",
      "Epoch: 433/10000..  Training Loss: 0.234.. \n",
      "Epoch: 434/10000..  Training Loss: 0.233.. \n",
      "Epoch: 435/10000..  Training Loss: 0.232.. \n",
      "Epoch: 436/10000..  Training Loss: 0.232.. \n",
      "Epoch: 437/10000..  Training Loss: 0.232.. \n",
      "Epoch: 438/10000..  Training Loss: 0.232.. \n",
      "Epoch: 439/10000..  Training Loss: 0.233.. \n",
      "Epoch: 440/10000..  Training Loss: 0.234.. \n",
      "Epoch: 441/10000..  Training Loss: 0.236.. \n",
      "Epoch: 442/10000..  Training Loss: 0.238.. \n",
      "Epoch: 443/10000..  Training Loss: 0.240.. \n",
      "Epoch: 444/10000..  Training Loss: 0.239.. \n",
      "Epoch: 445/10000..  Training Loss: 0.237.. \n",
      "Epoch: 446/10000..  Training Loss: 0.234.. \n",
      "Epoch: 447/10000..  Training Loss: 0.231.. \n",
      "Epoch: 448/10000..  Training Loss: 0.231.. \n",
      "Epoch: 449/10000..  Training Loss: 0.232.. \n",
      "Epoch: 450/10000..  Training Loss: 0.234.. \n",
      "Epoch: 451/10000..  Training Loss: 0.236.. \n",
      "Epoch: 452/10000..  Training Loss: 0.237.. \n",
      "Epoch: 453/10000..  Training Loss: 0.236.. \n",
      "Epoch: 454/10000..  Training Loss: 0.235.. \n",
      "Epoch: 455/10000..  Training Loss: 0.232.. \n",
      "Epoch: 456/10000..  Training Loss: 0.231.. \n",
      "Epoch: 457/10000..  Training Loss: 0.230.. \n",
      "Epoch: 458/10000..  Training Loss: 0.231.. \n",
      "Epoch: 459/10000..  Training Loss: 0.233.. \n",
      "Epoch: 460/10000..  Training Loss: 0.234.. \n",
      "Epoch: 461/10000..  Training Loss: 0.235.. \n",
      "Epoch: 462/10000..  Training Loss: 0.235.. \n",
      "Epoch: 463/10000..  Training Loss: 0.233.. \n",
      "Epoch: 464/10000..  Training Loss: 0.231.. \n",
      "Epoch: 465/10000..  Training Loss: 0.230.. \n",
      "Epoch: 466/10000..  Training Loss: 0.229.. \n",
      "Epoch: 467/10000..  Training Loss: 0.230.. \n",
      "Epoch: 468/10000..  Training Loss: 0.231.. \n",
      "Epoch: 469/10000..  Training Loss: 0.232.. \n",
      "Epoch: 470/10000..  Training Loss: 0.233.. \n",
      "Epoch: 471/10000..  Training Loss: 0.233.. \n",
      "Epoch: 472/10000..  Training Loss: 0.233.. \n",
      "Epoch: 473/10000..  Training Loss: 0.232.. \n",
      "Epoch: 474/10000..  Training Loss: 0.230.. \n",
      "Epoch: 475/10000..  Training Loss: 0.229.. \n",
      "Epoch: 476/10000..  Training Loss: 0.228.. \n",
      "Epoch: 477/10000..  Training Loss: 0.229.. \n",
      "Epoch: 478/10000..  Training Loss: 0.230.. \n",
      "Epoch: 479/10000..  Training Loss: 0.231.. \n",
      "Epoch: 480/10000..  Training Loss: 0.232.. \n",
      "Epoch: 481/10000..  Training Loss: 0.233.. \n",
      "Epoch: 482/10000..  Training Loss: 0.233.. \n",
      "Epoch: 483/10000..  Training Loss: 0.232.. \n",
      "Epoch: 484/10000..  Training Loss: 0.230.. \n",
      "Epoch: 485/10000..  Training Loss: 0.228.. \n",
      "Epoch: 486/10000..  Training Loss: 0.228.. \n",
      "Epoch: 487/10000..  Training Loss: 0.228.. \n",
      "Epoch: 488/10000..  Training Loss: 0.229.. \n",
      "Epoch: 489/10000..  Training Loss: 0.230.. \n",
      "Epoch: 490/10000..  Training Loss: 0.232.. \n",
      "Epoch: 491/10000..  Training Loss: 0.233.. \n",
      "Epoch: 492/10000..  Training Loss: 0.233.. \n",
      "Epoch: 493/10000..  Training Loss: 0.231.. \n",
      "Epoch: 494/10000..  Training Loss: 0.229.. \n",
      "Epoch: 495/10000..  Training Loss: 0.227.. \n",
      "Epoch: 496/10000..  Training Loss: 0.227.. \n",
      "Epoch: 497/10000..  Training Loss: 0.228.. \n",
      "Epoch: 498/10000..  Training Loss: 0.229.. \n",
      "Epoch: 499/10000..  Training Loss: 0.231.. \n",
      "Epoch: 500/10000..  Training Loss: 0.232.. \n",
      "Epoch: 501/10000..  Training Loss: 0.233.. \n",
      "Epoch: 502/10000..  Training Loss: 0.231.. \n",
      "Epoch: 503/10000..  Training Loss: 0.229.. \n",
      "Epoch: 504/10000..  Training Loss: 0.227.. \n",
      "Epoch: 505/10000..  Training Loss: 0.226.. \n",
      "Epoch: 506/10000..  Training Loss: 0.227.. \n",
      "Epoch: 507/10000..  Training Loss: 0.228.. \n",
      "Epoch: 508/10000..  Training Loss: 0.230.. \n",
      "Epoch: 509/10000..  Training Loss: 0.230.. \n",
      "Epoch: 510/10000..  Training Loss: 0.231.. \n",
      "Epoch: 511/10000..  Training Loss: 0.229.. \n",
      "Epoch: 512/10000..  Training Loss: 0.228.. \n",
      "Epoch: 513/10000..  Training Loss: 0.226.. \n",
      "Epoch: 514/10000..  Training Loss: 0.225.. \n",
      "Epoch: 515/10000..  Training Loss: 0.225.. \n",
      "Epoch: 516/10000..  Training Loss: 0.226.. \n",
      "Epoch: 517/10000..  Training Loss: 0.227.. \n",
      "Epoch: 518/10000..  Training Loss: 0.228.. \n",
      "Epoch: 519/10000..  Training Loss: 0.229.. \n",
      "Epoch: 520/10000..  Training Loss: 0.230.. \n",
      "Epoch: 521/10000..  Training Loss: 0.231.. \n",
      "Epoch: 522/10000..  Training Loss: 0.230.. \n",
      "Epoch: 523/10000..  Training Loss: 0.228.. \n",
      "Epoch: 524/10000..  Training Loss: 0.226.. \n",
      "Epoch: 525/10000..  Training Loss: 0.225.. \n",
      "Epoch: 526/10000..  Training Loss: 0.225.. \n",
      "Epoch: 527/10000..  Training Loss: 0.225.. \n",
      "Epoch: 528/10000..  Training Loss: 0.226.. \n",
      "Epoch: 529/10000..  Training Loss: 0.228.. \n",
      "Epoch: 530/10000..  Training Loss: 0.230.. \n",
      "Epoch: 531/10000..  Training Loss: 0.230.. \n",
      "Epoch: 532/10000..  Training Loss: 0.230.. \n",
      "Epoch: 533/10000..  Training Loss: 0.228.. \n",
      "Epoch: 534/10000..  Training Loss: 0.226.. \n",
      "Epoch: 535/10000..  Training Loss: 0.224.. \n",
      "Epoch: 536/10000..  Training Loss: 0.224.. \n",
      "Epoch: 537/10000..  Training Loss: 0.225.. \n",
      "Epoch: 538/10000..  Training Loss: 0.226.. \n",
      "Epoch: 539/10000..  Training Loss: 0.228.. \n",
      "Epoch: 540/10000..  Training Loss: 0.229.. \n",
      "Epoch: 541/10000..  Training Loss: 0.230.. \n",
      "Epoch: 542/10000..  Training Loss: 0.228.. \n",
      "Epoch: 543/10000..  Training Loss: 0.226.. \n",
      "Epoch: 544/10000..  Training Loss: 0.224.. \n",
      "Epoch: 545/10000..  Training Loss: 0.223.. \n",
      "Epoch: 546/10000..  Training Loss: 0.224.. \n",
      "Epoch: 547/10000..  Training Loss: 0.226.. \n",
      "Epoch: 548/10000..  Training Loss: 0.228.. \n",
      "Epoch: 549/10000..  Training Loss: 0.229.. \n",
      "Epoch: 550/10000..  Training Loss: 0.229.. \n",
      "Epoch: 551/10000..  Training Loss: 0.227.. \n",
      "Epoch: 552/10000..  Training Loss: 0.224.. \n",
      "Epoch: 553/10000..  Training Loss: 0.223.. \n",
      "Epoch: 554/10000..  Training Loss: 0.223.. \n",
      "Epoch: 555/10000..  Training Loss: 0.224.. \n",
      "Epoch: 556/10000..  Training Loss: 0.225.. \n",
      "Epoch: 557/10000..  Training Loss: 0.227.. \n",
      "Epoch: 558/10000..  Training Loss: 0.227.. \n",
      "Epoch: 559/10000..  Training Loss: 0.227.. \n",
      "Epoch: 560/10000..  Training Loss: 0.225.. \n",
      "Epoch: 561/10000..  Training Loss: 0.224.. \n",
      "Epoch: 562/10000..  Training Loss: 0.222.. \n",
      "Epoch: 563/10000..  Training Loss: 0.222.. \n",
      "Epoch: 564/10000..  Training Loss: 0.223.. \n",
      "Epoch: 565/10000..  Training Loss: 0.223.. \n",
      "Epoch: 566/10000..  Training Loss: 0.225.. \n",
      "Epoch: 567/10000..  Training Loss: 0.225.. \n",
      "Epoch: 568/10000..  Training Loss: 0.226.. \n",
      "Epoch: 569/10000..  Training Loss: 0.225.. \n",
      "Epoch: 570/10000..  Training Loss: 0.224.. \n",
      "Epoch: 571/10000..  Training Loss: 0.223.. \n",
      "Epoch: 572/10000..  Training Loss: 0.222.. \n",
      "Epoch: 573/10000..  Training Loss: 0.221.. \n",
      "Epoch: 574/10000..  Training Loss: 0.221.. \n",
      "Epoch: 575/10000..  Training Loss: 0.222.. \n",
      "Epoch: 576/10000..  Training Loss: 0.223.. \n",
      "Epoch: 577/10000..  Training Loss: 0.224.. \n",
      "Epoch: 578/10000..  Training Loss: 0.225.. \n",
      "Epoch: 579/10000..  Training Loss: 0.226.. \n",
      "Epoch: 580/10000..  Training Loss: 0.225.. \n",
      "Epoch: 581/10000..  Training Loss: 0.224.. \n",
      "Epoch: 582/10000..  Training Loss: 0.223.. \n",
      "Epoch: 583/10000..  Training Loss: 0.221.. \n",
      "Epoch: 584/10000..  Training Loss: 0.221.. \n",
      "Epoch: 585/10000..  Training Loss: 0.221.. \n",
      "Epoch: 586/10000..  Training Loss: 0.221.. \n",
      "Epoch: 587/10000..  Training Loss: 0.221.. \n",
      "Epoch: 588/10000..  Training Loss: 0.223.. \n",
      "Epoch: 589/10000..  Training Loss: 0.224.. \n",
      "Epoch: 590/10000..  Training Loss: 0.226.. \n",
      "Epoch: 591/10000..  Training Loss: 0.226.. \n",
      "Epoch: 592/10000..  Training Loss: 0.227.. \n",
      "Epoch: 593/10000..  Training Loss: 0.224.. \n",
      "Epoch: 594/10000..  Training Loss: 0.222.. \n",
      "Epoch: 595/10000..  Training Loss: 0.220.. \n",
      "Epoch: 596/10000..  Training Loss: 0.220.. \n",
      "Epoch: 597/10000..  Training Loss: 0.220.. \n",
      "Epoch: 598/10000..  Training Loss: 0.221.. \n",
      "Epoch: 599/10000..  Training Loss: 0.223.. \n",
      "Epoch: 600/10000..  Training Loss: 0.224.. \n",
      "Epoch: 601/10000..  Training Loss: 0.226.. \n",
      "Epoch: 602/10000..  Training Loss: 0.225.. \n",
      "Epoch: 603/10000..  Training Loss: 0.224.. \n",
      "Epoch: 604/10000..  Training Loss: 0.221.. \n",
      "Epoch: 605/10000..  Training Loss: 0.220.. \n",
      "Epoch: 606/10000..  Training Loss: 0.219.. \n",
      "Epoch: 607/10000..  Training Loss: 0.220.. \n",
      "Epoch: 608/10000..  Training Loss: 0.220.. \n",
      "Epoch: 609/10000..  Training Loss: 0.221.. \n",
      "Epoch: 610/10000..  Training Loss: 0.223.. \n",
      "Epoch: 611/10000..  Training Loss: 0.224.. \n",
      "Epoch: 612/10000..  Training Loss: 0.224.. \n",
      "Epoch: 613/10000..  Training Loss: 0.223.. \n",
      "Epoch: 614/10000..  Training Loss: 0.222.. \n",
      "Epoch: 615/10000..  Training Loss: 0.220.. \n",
      "Epoch: 616/10000..  Training Loss: 0.219.. \n",
      "Epoch: 617/10000..  Training Loss: 0.219.. \n",
      "Epoch: 618/10000..  Training Loss: 0.219.. \n",
      "Epoch: 619/10000..  Training Loss: 0.221.. \n",
      "Epoch: 620/10000..  Training Loss: 0.222.. \n",
      "Epoch: 621/10000..  Training Loss: 0.224.. \n",
      "Epoch: 622/10000..  Training Loss: 0.224.. \n",
      "Epoch: 623/10000..  Training Loss: 0.224.. \n",
      "Epoch: 624/10000..  Training Loss: 0.222.. \n",
      "Epoch: 625/10000..  Training Loss: 0.220.. \n",
      "Epoch: 626/10000..  Training Loss: 0.219.. \n",
      "Epoch: 627/10000..  Training Loss: 0.218.. \n",
      "Epoch: 628/10000..  Training Loss: 0.218.. \n",
      "Epoch: 629/10000..  Training Loss: 0.219.. \n",
      "Epoch: 630/10000..  Training Loss: 0.220.. \n",
      "Epoch: 631/10000..  Training Loss: 0.222.. \n",
      "Epoch: 632/10000..  Training Loss: 0.223.. \n",
      "Epoch: 633/10000..  Training Loss: 0.223.. \n",
      "Epoch: 634/10000..  Training Loss: 0.222.. \n",
      "Epoch: 635/10000..  Training Loss: 0.220.. \n",
      "Epoch: 636/10000..  Training Loss: 0.218.. \n",
      "Epoch: 637/10000..  Training Loss: 0.217.. \n",
      "Epoch: 638/10000..  Training Loss: 0.217.. \n",
      "Epoch: 639/10000..  Training Loss: 0.218.. \n",
      "Epoch: 640/10000..  Training Loss: 0.219.. \n",
      "Epoch: 641/10000..  Training Loss: 0.221.. \n",
      "Epoch: 642/10000..  Training Loss: 0.222.. \n",
      "Epoch: 643/10000..  Training Loss: 0.223.. \n",
      "Epoch: 644/10000..  Training Loss: 0.222.. \n",
      "Epoch: 645/10000..  Training Loss: 0.220.. \n",
      "Epoch: 646/10000..  Training Loss: 0.218.. \n",
      "Epoch: 647/10000..  Training Loss: 0.217.. \n",
      "Epoch: 648/10000..  Training Loss: 0.217.. \n",
      "Epoch: 649/10000..  Training Loss: 0.217.. \n",
      "Epoch: 650/10000..  Training Loss: 0.218.. \n",
      "Epoch: 651/10000..  Training Loss: 0.219.. \n",
      "Epoch: 652/10000..  Training Loss: 0.221.. \n",
      "Epoch: 653/10000..  Training Loss: 0.222.. \n",
      "Epoch: 654/10000..  Training Loss: 0.223.. \n",
      "Epoch: 655/10000..  Training Loss: 0.222.. \n",
      "Epoch: 656/10000..  Training Loss: 0.220.. \n",
      "Epoch: 657/10000..  Training Loss: 0.218.. \n",
      "Epoch: 658/10000..  Training Loss: 0.216.. \n",
      "Epoch: 659/10000..  Training Loss: 0.216.. \n",
      "Epoch: 660/10000..  Training Loss: 0.217.. \n",
      "Epoch: 661/10000..  Training Loss: 0.219.. \n",
      "Epoch: 662/10000..  Training Loss: 0.220.. \n",
      "Epoch: 663/10000..  Training Loss: 0.222.. \n",
      "Epoch: 664/10000..  Training Loss: 0.222.. \n",
      "Epoch: 665/10000..  Training Loss: 0.221.. \n",
      "Epoch: 666/10000..  Training Loss: 0.218.. \n",
      "Epoch: 667/10000..  Training Loss: 0.216.. \n",
      "Epoch: 668/10000..  Training Loss: 0.216.. \n",
      "Epoch: 669/10000..  Training Loss: 0.216.. \n",
      "Epoch: 670/10000..  Training Loss: 0.218.. \n",
      "Epoch: 671/10000..  Training Loss: 0.220.. \n",
      "Epoch: 672/10000..  Training Loss: 0.222.. \n",
      "Epoch: 673/10000..  Training Loss: 0.222.. \n",
      "Epoch: 674/10000..  Training Loss: 0.222.. \n",
      "Epoch: 675/10000..  Training Loss: 0.218.. \n",
      "Epoch: 676/10000..  Training Loss: 0.216.. \n",
      "Epoch: 677/10000..  Training Loss: 0.216.. \n",
      "Epoch: 678/10000..  Training Loss: 0.216.. \n",
      "Epoch: 679/10000..  Training Loss: 0.218.. \n",
      "Epoch: 680/10000..  Training Loss: 0.220.. \n",
      "Epoch: 681/10000..  Training Loss: 0.223.. \n",
      "Epoch: 682/10000..  Training Loss: 0.223.. \n",
      "Epoch: 683/10000..  Training Loss: 0.222.. \n",
      "Epoch: 684/10000..  Training Loss: 0.218.. \n",
      "Epoch: 685/10000..  Training Loss: 0.216.. \n",
      "Epoch: 686/10000..  Training Loss: 0.215.. \n",
      "Epoch: 687/10000..  Training Loss: 0.217.. \n",
      "Epoch: 688/10000..  Training Loss: 0.221.. \n",
      "Epoch: 689/10000..  Training Loss: 0.221.. \n",
      "Epoch: 690/10000..  Training Loss: 0.222.. \n",
      "Epoch: 691/10000..  Training Loss: 0.220.. \n",
      "Epoch: 692/10000..  Training Loss: 0.216.. \n",
      "Epoch: 693/10000..  Training Loss: 0.215.. \n",
      "Epoch: 694/10000..  Training Loss: 0.215.. \n",
      "Epoch: 695/10000..  Training Loss: 0.217.. \n",
      "Epoch: 696/10000..  Training Loss: 0.217.. \n",
      "Epoch: 697/10000..  Training Loss: 0.218.. \n",
      "Epoch: 698/10000..  Training Loss: 0.218.. \n",
      "Epoch: 699/10000..  Training Loss: 0.216.. \n",
      "Epoch: 700/10000..  Training Loss: 0.214.. \n",
      "Epoch: 701/10000..  Training Loss: 0.214.. \n",
      "Epoch: 702/10000..  Training Loss: 0.215.. \n",
      "Epoch: 703/10000..  Training Loss: 0.215.. \n",
      "Epoch: 704/10000..  Training Loss: 0.216.. \n",
      "Epoch: 705/10000..  Training Loss: 0.217.. \n",
      "Epoch: 706/10000..  Training Loss: 0.216.. \n",
      "Epoch: 707/10000..  Training Loss: 0.215.. \n",
      "Epoch: 708/10000..  Training Loss: 0.215.. \n",
      "Epoch: 709/10000..  Training Loss: 0.214.. \n",
      "Epoch: 710/10000..  Training Loss: 0.213.. \n",
      "Epoch: 711/10000..  Training Loss: 0.213.. \n",
      "Epoch: 712/10000..  Training Loss: 0.214.. \n",
      "Epoch: 713/10000..  Training Loss: 0.214.. \n",
      "Epoch: 714/10000..  Training Loss: 0.215.. \n",
      "Epoch: 715/10000..  Training Loss: 0.215.. \n",
      "Epoch: 716/10000..  Training Loss: 0.216.. \n",
      "Epoch: 717/10000..  Training Loss: 0.216.. \n",
      "Epoch: 718/10000..  Training Loss: 0.215.. \n",
      "Epoch: 719/10000..  Training Loss: 0.215.. \n",
      "Epoch: 720/10000..  Training Loss: 0.214.. \n",
      "Epoch: 721/10000..  Training Loss: 0.213.. \n",
      "Epoch: 722/10000..  Training Loss: 0.213.. \n",
      "Epoch: 723/10000..  Training Loss: 0.213.. \n",
      "Epoch: 724/10000..  Training Loss: 0.213.. \n",
      "Epoch: 725/10000..  Training Loss: 0.212.. \n",
      "Epoch: 726/10000..  Training Loss: 0.213.. \n",
      "Epoch: 727/10000..  Training Loss: 0.213.. \n",
      "Epoch: 728/10000..  Training Loss: 0.213.. \n",
      "Epoch: 729/10000..  Training Loss: 0.213.. \n",
      "Epoch: 730/10000..  Training Loss: 0.214.. \n",
      "Epoch: 731/10000..  Training Loss: 0.215.. \n",
      "Epoch: 732/10000..  Training Loss: 0.217.. \n",
      "Epoch: 733/10000..  Training Loss: 0.218.. \n",
      "Epoch: 734/10000..  Training Loss: 0.221.. \n",
      "Epoch: 735/10000..  Training Loss: 0.220.. \n",
      "Epoch: 736/10000..  Training Loss: 0.218.. \n",
      "Epoch: 737/10000..  Training Loss: 0.215.. \n",
      "Epoch: 738/10000..  Training Loss: 0.213.. \n",
      "Epoch: 739/10000..  Training Loss: 0.212.. \n",
      "Epoch: 740/10000..  Training Loss: 0.213.. \n",
      "Epoch: 741/10000..  Training Loss: 0.215.. \n",
      "Epoch: 742/10000..  Training Loss: 0.216.. \n",
      "Epoch: 743/10000..  Training Loss: 0.219.. \n",
      "Epoch: 744/10000..  Training Loss: 0.220.. \n",
      "Epoch: 745/10000..  Training Loss: 0.220.. \n",
      "Epoch: 746/10000..  Training Loss: 0.216.. \n",
      "Epoch: 747/10000..  Training Loss: 0.213.. \n",
      "Epoch: 748/10000..  Training Loss: 0.212.. \n",
      "Epoch: 749/10000..  Training Loss: 0.212.. \n",
      "Epoch: 750/10000..  Training Loss: 0.213.. \n",
      "Epoch: 751/10000..  Training Loss: 0.215.. \n",
      "Epoch: 752/10000..  Training Loss: 0.218.. \n",
      "Epoch: 753/10000..  Training Loss: 0.217.. \n",
      "Epoch: 754/10000..  Training Loss: 0.216.. \n",
      "Epoch: 755/10000..  Training Loss: 0.214.. \n",
      "Epoch: 756/10000..  Training Loss: 0.212.. \n",
      "Epoch: 757/10000..  Training Loss: 0.211.. \n",
      "Epoch: 758/10000..  Training Loss: 0.212.. \n",
      "Epoch: 759/10000..  Training Loss: 0.213.. \n",
      "Epoch: 760/10000..  Training Loss: 0.214.. \n",
      "Epoch: 761/10000..  Training Loss: 0.215.. \n",
      "Epoch: 762/10000..  Training Loss: 0.216.. \n",
      "Epoch: 763/10000..  Training Loss: 0.216.. \n",
      "Epoch: 764/10000..  Training Loss: 0.214.. \n",
      "Epoch: 765/10000..  Training Loss: 0.213.. \n",
      "Epoch: 766/10000..  Training Loss: 0.212.. \n",
      "Epoch: 767/10000..  Training Loss: 0.211.. \n",
      "Epoch: 768/10000..  Training Loss: 0.211.. \n",
      "Epoch: 769/10000..  Training Loss: 0.213.. \n",
      "Epoch: 770/10000..  Training Loss: 0.214.. \n",
      "Epoch: 771/10000..  Training Loss: 0.214.. \n",
      "Epoch: 772/10000..  Training Loss: 0.215.. \n",
      "Epoch: 773/10000..  Training Loss: 0.214.. \n",
      "Epoch: 774/10000..  Training Loss: 0.213.. \n",
      "Epoch: 775/10000..  Training Loss: 0.212.. \n",
      "Epoch: 776/10000..  Training Loss: 0.211.. \n",
      "Epoch: 777/10000..  Training Loss: 0.210.. \n",
      "Epoch: 778/10000..  Training Loss: 0.211.. \n",
      "Epoch: 779/10000..  Training Loss: 0.211.. \n",
      "Epoch: 780/10000..  Training Loss: 0.212.. \n",
      "Epoch: 781/10000..  Training Loss: 0.214.. \n",
      "Epoch: 782/10000..  Training Loss: 0.215.. \n",
      "Epoch: 783/10000..  Training Loss: 0.216.. \n",
      "Epoch: 784/10000..  Training Loss: 0.215.. \n",
      "Epoch: 785/10000..  Training Loss: 0.214.. \n",
      "Epoch: 786/10000..  Training Loss: 0.212.. \n",
      "Epoch: 787/10000..  Training Loss: 0.211.. \n",
      "Epoch: 788/10000..  Training Loss: 0.210.. \n",
      "Epoch: 789/10000..  Training Loss: 0.210.. \n",
      "Epoch: 790/10000..  Training Loss: 0.211.. \n",
      "Epoch: 791/10000..  Training Loss: 0.211.. \n",
      "Epoch: 792/10000..  Training Loss: 0.213.. \n",
      "Epoch: 793/10000..  Training Loss: 0.215.. \n",
      "Epoch: 794/10000..  Training Loss: 0.217.. \n",
      "Epoch: 795/10000..  Training Loss: 0.216.. \n",
      "Epoch: 796/10000..  Training Loss: 0.215.. \n",
      "Epoch: 797/10000..  Training Loss: 0.213.. \n",
      "Epoch: 798/10000..  Training Loss: 0.210.. \n",
      "Epoch: 799/10000..  Training Loss: 0.210.. \n",
      "Epoch: 800/10000..  Training Loss: 0.210.. \n",
      "Epoch: 801/10000..  Training Loss: 0.211.. \n",
      "Epoch: 802/10000..  Training Loss: 0.212.. \n",
      "Epoch: 803/10000..  Training Loss: 0.214.. \n",
      "Epoch: 804/10000..  Training Loss: 0.215.. \n",
      "Epoch: 805/10000..  Training Loss: 0.216.. \n",
      "Epoch: 806/10000..  Training Loss: 0.214.. \n",
      "Epoch: 807/10000..  Training Loss: 0.213.. \n",
      "Epoch: 808/10000..  Training Loss: 0.210.. \n",
      "Epoch: 809/10000..  Training Loss: 0.209.. \n",
      "Epoch: 810/10000..  Training Loss: 0.210.. \n",
      "Epoch: 811/10000..  Training Loss: 0.210.. \n",
      "Epoch: 812/10000..  Training Loss: 0.210.. \n",
      "Epoch: 813/10000..  Training Loss: 0.211.. \n",
      "Epoch: 814/10000..  Training Loss: 0.213.. \n",
      "Epoch: 815/10000..  Training Loss: 0.213.. \n",
      "Epoch: 816/10000..  Training Loss: 0.214.. \n",
      "Epoch: 817/10000..  Training Loss: 0.213.. \n",
      "Epoch: 818/10000..  Training Loss: 0.211.. \n",
      "Epoch: 819/10000..  Training Loss: 0.210.. \n",
      "Epoch: 820/10000..  Training Loss: 0.209.. \n",
      "Epoch: 821/10000..  Training Loss: 0.209.. \n",
      "Epoch: 822/10000..  Training Loss: 0.208.. \n",
      "Epoch: 823/10000..  Training Loss: 0.210.. \n",
      "Epoch: 824/10000..  Training Loss: 0.211.. \n",
      "Epoch: 825/10000..  Training Loss: 0.212.. \n",
      "Epoch: 826/10000..  Training Loss: 0.212.. \n",
      "Epoch: 827/10000..  Training Loss: 0.214.. \n",
      "Epoch: 828/10000..  Training Loss: 0.213.. \n",
      "Epoch: 829/10000..  Training Loss: 0.212.. \n",
      "Epoch: 830/10000..  Training Loss: 0.211.. \n",
      "Epoch: 831/10000..  Training Loss: 0.210.. \n",
      "Epoch: 832/10000..  Training Loss: 0.208.. \n",
      "Epoch: 833/10000..  Training Loss: 0.208.. \n",
      "Epoch: 834/10000..  Training Loss: 0.209.. \n",
      "Epoch: 835/10000..  Training Loss: 0.209.. \n",
      "Epoch: 836/10000..  Training Loss: 0.210.. \n",
      "Epoch: 837/10000..  Training Loss: 0.212.. \n",
      "Epoch: 838/10000..  Training Loss: 0.214.. \n",
      "Epoch: 839/10000..  Training Loss: 0.213.. \n",
      "Epoch: 840/10000..  Training Loss: 0.213.. \n",
      "Epoch: 841/10000..  Training Loss: 0.210.. \n",
      "Epoch: 842/10000..  Training Loss: 0.208.. \n",
      "Epoch: 843/10000..  Training Loss: 0.208.. \n",
      "Epoch: 844/10000..  Training Loss: 0.209.. \n",
      "Epoch: 845/10000..  Training Loss: 0.209.. \n",
      "Epoch: 846/10000..  Training Loss: 0.210.. \n",
      "Epoch: 847/10000..  Training Loss: 0.213.. \n",
      "Epoch: 848/10000..  Training Loss: 0.213.. \n",
      "Epoch: 849/10000..  Training Loss: 0.213.. \n",
      "Epoch: 850/10000..  Training Loss: 0.212.. \n",
      "Epoch: 851/10000..  Training Loss: 0.211.. \n",
      "Epoch: 852/10000..  Training Loss: 0.208.. \n",
      "Epoch: 853/10000..  Training Loss: 0.209.. \n",
      "Epoch: 854/10000..  Training Loss: 0.210.. \n",
      "Epoch: 855/10000..  Training Loss: 0.210.. \n",
      "Epoch: 856/10000..  Training Loss: 0.213.. \n",
      "Epoch: 857/10000..  Training Loss: 0.214.. \n",
      "Epoch: 858/10000..  Training Loss: 0.213.. \n",
      "Epoch: 859/10000..  Training Loss: 0.211.. \n",
      "Epoch: 860/10000..  Training Loss: 0.211.. \n",
      "Epoch: 861/10000..  Training Loss: 0.208.. \n",
      "Epoch: 862/10000..  Training Loss: 0.208.. \n",
      "Epoch: 863/10000..  Training Loss: 0.211.. \n",
      "Epoch: 864/10000..  Training Loss: 0.210.. \n",
      "Epoch: 865/10000..  Training Loss: 0.212.. \n",
      "Epoch: 866/10000..  Training Loss: 0.212.. \n",
      "Epoch: 867/10000..  Training Loss: 0.209.. \n",
      "Epoch: 868/10000..  Training Loss: 0.208.. \n",
      "Epoch: 869/10000..  Training Loss: 0.209.. \n",
      "Epoch: 870/10000..  Training Loss: 0.207.. \n",
      "Epoch: 871/10000..  Training Loss: 0.207.. \n",
      "Epoch: 872/10000..  Training Loss: 0.208.. \n",
      "Epoch: 873/10000..  Training Loss: 0.208.. \n",
      "Epoch: 874/10000..  Training Loss: 0.208.. \n",
      "Epoch: 875/10000..  Training Loss: 0.209.. \n",
      "Epoch: 876/10000..  Training Loss: 0.209.. \n",
      "Epoch: 877/10000..  Training Loss: 0.208.. \n",
      "Epoch: 878/10000..  Training Loss: 0.209.. \n",
      "Epoch: 879/10000..  Training Loss: 0.208.. \n",
      "Epoch: 880/10000..  Training Loss: 0.207.. \n",
      "Epoch: 881/10000..  Training Loss: 0.207.. \n",
      "Epoch: 882/10000..  Training Loss: 0.206.. \n",
      "Epoch: 883/10000..  Training Loss: 0.206.. \n",
      "Epoch: 884/10000..  Training Loss: 0.206.. \n",
      "Epoch: 885/10000..  Training Loss: 0.206.. \n",
      "Epoch: 886/10000..  Training Loss: 0.206.. \n",
      "Epoch: 887/10000..  Training Loss: 0.206.. \n",
      "Epoch: 888/10000..  Training Loss: 0.207.. \n",
      "Epoch: 889/10000..  Training Loss: 0.208.. \n",
      "Epoch: 890/10000..  Training Loss: 0.208.. \n",
      "Epoch: 891/10000..  Training Loss: 0.210.. \n",
      "Epoch: 892/10000..  Training Loss: 0.211.. \n",
      "Epoch: 893/10000..  Training Loss: 0.212.. \n",
      "Epoch: 894/10000..  Training Loss: 0.211.. \n",
      "Epoch: 895/10000..  Training Loss: 0.210.. \n",
      "Epoch: 896/10000..  Training Loss: 0.208.. \n",
      "Epoch: 897/10000..  Training Loss: 0.206.. \n",
      "Epoch: 898/10000..  Training Loss: 0.206.. \n",
      "Epoch: 899/10000..  Training Loss: 0.205.. \n",
      "Epoch: 900/10000..  Training Loss: 0.207.. \n",
      "Epoch: 901/10000..  Training Loss: 0.208.. \n",
      "Epoch: 902/10000..  Training Loss: 0.211.. \n",
      "Epoch: 903/10000..  Training Loss: 0.212.. \n",
      "Epoch: 904/10000..  Training Loss: 0.214.. \n",
      "Epoch: 905/10000..  Training Loss: 0.212.. \n",
      "Epoch: 906/10000..  Training Loss: 0.209.. \n",
      "Epoch: 907/10000..  Training Loss: 0.206.. \n",
      "Epoch: 908/10000..  Training Loss: 0.205.. \n",
      "Epoch: 909/10000..  Training Loss: 0.206.. \n",
      "Epoch: 910/10000..  Training Loss: 0.209.. \n",
      "Epoch: 911/10000..  Training Loss: 0.212.. \n",
      "Epoch: 912/10000..  Training Loss: 0.212.. \n",
      "Epoch: 913/10000..  Training Loss: 0.212.. \n",
      "Epoch: 914/10000..  Training Loss: 0.209.. \n",
      "Epoch: 915/10000..  Training Loss: 0.206.. \n",
      "Epoch: 916/10000..  Training Loss: 0.205.. \n",
      "Epoch: 917/10000..  Training Loss: 0.205.. \n",
      "Epoch: 918/10000..  Training Loss: 0.207.. \n",
      "Epoch: 919/10000..  Training Loss: 0.209.. \n",
      "Epoch: 920/10000..  Training Loss: 0.212.. \n",
      "Epoch: 921/10000..  Training Loss: 0.210.. \n",
      "Epoch: 922/10000..  Training Loss: 0.208.. \n",
      "Epoch: 923/10000..  Training Loss: 0.206.. \n",
      "Epoch: 924/10000..  Training Loss: 0.205.. \n",
      "Epoch: 925/10000..  Training Loss: 0.204.. \n",
      "Epoch: 926/10000..  Training Loss: 0.206.. \n",
      "Epoch: 927/10000..  Training Loss: 0.208.. \n",
      "Epoch: 928/10000..  Training Loss: 0.209.. \n",
      "Epoch: 929/10000..  Training Loss: 0.210.. \n",
      "Epoch: 930/10000..  Training Loss: 0.209.. \n",
      "Epoch: 931/10000..  Training Loss: 0.206.. \n",
      "Epoch: 932/10000..  Training Loss: 0.205.. \n",
      "Epoch: 933/10000..  Training Loss: 0.204.. \n",
      "Epoch: 934/10000..  Training Loss: 0.204.. \n",
      "Epoch: 935/10000..  Training Loss: 0.205.. \n",
      "Epoch: 936/10000..  Training Loss: 0.207.. \n",
      "Epoch: 937/10000..  Training Loss: 0.208.. \n",
      "Epoch: 938/10000..  Training Loss: 0.208.. \n",
      "Epoch: 939/10000..  Training Loss: 0.207.. \n",
      "Epoch: 940/10000..  Training Loss: 0.206.. \n",
      "Epoch: 941/10000..  Training Loss: 0.204.. \n",
      "Epoch: 942/10000..  Training Loss: 0.204.. \n",
      "Epoch: 943/10000..  Training Loss: 0.204.. \n",
      "Epoch: 944/10000..  Training Loss: 0.204.. \n",
      "Epoch: 945/10000..  Training Loss: 0.205.. \n",
      "Epoch: 946/10000..  Training Loss: 0.206.. \n",
      "Epoch: 947/10000..  Training Loss: 0.207.. \n",
      "Epoch: 948/10000..  Training Loss: 0.207.. \n",
      "Epoch: 949/10000..  Training Loss: 0.207.. \n",
      "Epoch: 950/10000..  Training Loss: 0.206.. \n",
      "Epoch: 951/10000..  Training Loss: 0.205.. \n",
      "Epoch: 952/10000..  Training Loss: 0.204.. \n",
      "Epoch: 953/10000..  Training Loss: 0.203.. \n",
      "Epoch: 954/10000..  Training Loss: 0.203.. \n",
      "Epoch: 955/10000..  Training Loss: 0.203.. \n",
      "Epoch: 956/10000..  Training Loss: 0.204.. \n",
      "Epoch: 957/10000..  Training Loss: 0.205.. \n",
      "Epoch: 958/10000..  Training Loss: 0.206.. \n",
      "Epoch: 959/10000..  Training Loss: 0.207.. \n",
      "Epoch: 960/10000..  Training Loss: 0.209.. \n",
      "Epoch: 961/10000..  Training Loss: 0.208.. \n",
      "Epoch: 962/10000..  Training Loss: 0.207.. \n",
      "Epoch: 963/10000..  Training Loss: 0.204.. \n",
      "Epoch: 964/10000..  Training Loss: 0.203.. \n",
      "Epoch: 965/10000..  Training Loss: 0.203.. \n",
      "Epoch: 966/10000..  Training Loss: 0.203.. \n",
      "Epoch: 967/10000..  Training Loss: 0.204.. \n",
      "Epoch: 968/10000..  Training Loss: 0.205.. \n",
      "Epoch: 969/10000..  Training Loss: 0.207.. \n",
      "Epoch: 970/10000..  Training Loss: 0.207.. \n",
      "Epoch: 971/10000..  Training Loss: 0.209.. \n",
      "Epoch: 972/10000..  Training Loss: 0.207.. \n",
      "Epoch: 973/10000..  Training Loss: 0.206.. \n",
      "Epoch: 974/10000..  Training Loss: 0.204.. \n",
      "Epoch: 975/10000..  Training Loss: 0.203.. \n",
      "Epoch: 976/10000..  Training Loss: 0.202.. \n",
      "Epoch: 977/10000..  Training Loss: 0.202.. \n",
      "Epoch: 978/10000..  Training Loss: 0.203.. \n",
      "Epoch: 979/10000..  Training Loss: 0.204.. \n",
      "Epoch: 980/10000..  Training Loss: 0.206.. \n",
      "Epoch: 981/10000..  Training Loss: 0.207.. \n",
      "Epoch: 982/10000..  Training Loss: 0.209.. \n",
      "Epoch: 983/10000..  Training Loss: 0.207.. \n",
      "Epoch: 984/10000..  Training Loss: 0.206.. \n",
      "Epoch: 985/10000..  Training Loss: 0.203.. \n",
      "Epoch: 986/10000..  Training Loss: 0.202.. \n",
      "Epoch: 987/10000..  Training Loss: 0.202.. \n",
      "Epoch: 988/10000..  Training Loss: 0.202.. \n",
      "Epoch: 989/10000..  Training Loss: 0.203.. \n",
      "Epoch: 990/10000..  Training Loss: 0.205.. \n",
      "Epoch: 991/10000..  Training Loss: 0.207.. \n",
      "Epoch: 992/10000..  Training Loss: 0.207.. \n",
      "Epoch: 993/10000..  Training Loss: 0.208.. \n",
      "Epoch: 994/10000..  Training Loss: 0.207.. \n",
      "Epoch: 995/10000..  Training Loss: 0.205.. \n",
      "Epoch: 996/10000..  Training Loss: 0.203.. \n",
      "Epoch: 997/10000..  Training Loss: 0.202.. \n",
      "Epoch: 998/10000..  Training Loss: 0.202.. \n",
      "Epoch: 999/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1000/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1001/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1002/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1003/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1004/10000..  Training Loss: 0.209.. \n",
      "Epoch: 1005/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1006/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1007/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1008/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1009/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1010/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1011/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1012/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1013/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1014/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1015/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1016/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1017/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1018/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1019/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1020/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1021/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1022/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1023/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1024/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1025/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1026/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1027/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1028/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1029/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1030/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1031/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1032/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1033/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1034/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1035/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1036/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1037/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1038/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1039/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1040/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1041/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1042/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1043/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1044/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1045/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1046/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1047/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1048/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1049/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1050/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1051/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1052/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1053/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1054/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1055/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1056/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1057/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1058/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1059/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1060/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1061/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1062/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1063/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1064/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1065/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1066/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1067/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1068/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1069/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1070/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1071/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1072/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1073/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1074/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1075/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1076/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1077/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1078/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1079/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1080/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1081/10000..  Training Loss: 0.207.. \n",
      "Epoch: 1082/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1083/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1084/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1085/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1086/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1087/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1088/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1089/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1090/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1091/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1092/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1093/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1094/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1095/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1096/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1097/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1098/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1099/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1100/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1101/10000..  Training Loss: 0.206.. \n",
      "Epoch: 1102/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1103/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1104/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1105/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1106/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1107/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1108/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1109/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1110/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1111/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1112/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1113/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1114/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1115/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1116/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1117/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1118/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1119/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1120/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1121/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1122/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1123/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1124/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1125/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1126/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1127/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1128/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1129/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1130/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1131/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1132/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1133/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1134/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1135/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1136/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1137/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1138/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1139/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1140/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1141/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1142/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1143/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1144/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1145/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1146/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1147/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1148/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1149/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1150/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1151/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1152/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1153/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1154/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1155/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1156/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1157/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1158/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1159/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1160/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1161/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1162/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1163/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1164/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1165/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1166/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1167/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1168/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1169/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1170/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1171/10000..  Training Loss: 0.203.. \n",
      "Epoch: 1172/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1173/10000..  Training Loss: 0.208.. \n",
      "Epoch: 1174/10000..  Training Loss: 0.204.. \n",
      "Epoch: 1175/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1176/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1177/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1178/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1179/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1180/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1181/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1182/10000..  Training Loss: 0.205.. \n",
      "Epoch: 1183/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1184/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1185/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1186/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1187/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1188/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1189/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1190/10000..  Training Loss: 0.202.. \n",
      "Epoch: 1191/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1192/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1193/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1194/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1195/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1196/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1197/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1198/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1199/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1200/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1201/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1202/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1203/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1204/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1205/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1206/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1207/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1208/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1209/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1210/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1211/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1212/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1213/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1214/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1215/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1216/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1217/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1218/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1219/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1220/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1221/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1222/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1223/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1224/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1225/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1226/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1227/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1228/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1229/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1230/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1231/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1232/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1233/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1234/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1235/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1236/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1237/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1238/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1239/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1240/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1241/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1242/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1243/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1244/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1245/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1246/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1247/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1248/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1249/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1250/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1251/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1252/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1253/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1254/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1255/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1256/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1257/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1258/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1259/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1260/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1261/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1262/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1263/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1264/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1265/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1266/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1267/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1268/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1269/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1270/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1271/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1272/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1273/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1274/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1275/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1276/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1277/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1278/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1279/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1280/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1281/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1282/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1283/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1284/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1285/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1286/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1287/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1288/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1289/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1290/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1291/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1292/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1293/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1294/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1295/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1296/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1297/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1298/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1299/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1300/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1301/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1302/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1303/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1304/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1305/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1306/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1307/10000..  Training Loss: 0.200.. \n",
      "Epoch: 1308/10000..  Training Loss: 0.199.. \n",
      "Epoch: 1309/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1310/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1311/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1312/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1313/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1314/10000..  Training Loss: 0.201.. \n",
      "Epoch: 1315/10000..  Training Loss: 0.198.. \n",
      "Epoch: 1316/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1317/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1318/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1319/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1320/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1321/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1322/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1323/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1324/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1325/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1326/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1327/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1328/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1329/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1330/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1331/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1332/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1333/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1334/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1335/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1336/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1337/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1338/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1339/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1340/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1341/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1342/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1343/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1344/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1345/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1346/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1347/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1348/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1349/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1350/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1351/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1352/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1353/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1354/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1355/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1356/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1357/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1358/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1359/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1360/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1361/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1362/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1363/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1364/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1365/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1366/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1367/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1368/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1369/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1370/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1371/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1372/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1373/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1374/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1375/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1376/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1377/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1378/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1379/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1380/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1381/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1382/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1383/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1384/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1385/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1386/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1387/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1388/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1389/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1390/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1391/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1392/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1393/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1394/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1395/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1396/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1397/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1398/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1399/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1400/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1401/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1402/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1403/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1404/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1405/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1406/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1407/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1408/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1409/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1410/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1411/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1412/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1413/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1414/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1415/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1416/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1417/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1418/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1419/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1420/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1421/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1422/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1423/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1424/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1425/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1426/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1427/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1428/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1429/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1430/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1431/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1432/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1433/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1434/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1435/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1436/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1437/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1438/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1439/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1440/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1441/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1442/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1443/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1444/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1445/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1446/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1447/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1448/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1449/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1450/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1451/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1452/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1453/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1454/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1455/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1456/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1457/10000..  Training Loss: 0.197.. \n",
      "Epoch: 1458/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1459/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1460/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1461/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1462/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1463/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1464/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1465/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1466/10000..  Training Loss: 0.196.. \n",
      "Epoch: 1467/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1468/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1469/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1470/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1471/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1472/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1473/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1474/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1475/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1476/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1477/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1478/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1479/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1480/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1481/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1482/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1483/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1484/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1485/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1486/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1487/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1488/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1489/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1490/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1491/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1492/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1493/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1494/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1495/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1496/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1497/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1498/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1499/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1500/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1501/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1502/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1503/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1504/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1505/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1506/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1507/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1508/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1509/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1510/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1511/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1512/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1513/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1514/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1515/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1516/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1517/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1518/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1519/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1520/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1521/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1522/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1523/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1524/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1525/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1526/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1527/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1528/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1529/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1530/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1531/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1532/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1533/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1534/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1535/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1536/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1537/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1538/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1539/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1540/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1541/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1542/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1543/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1544/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1545/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1546/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1547/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1548/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1549/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1550/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1551/10000..  Training Loss: 0.195.. \n",
      "Epoch: 1552/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1553/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1554/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1555/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1556/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1557/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1558/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1559/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1560/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1561/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1562/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1563/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1564/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1565/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1566/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1567/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1568/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1569/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1570/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1571/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1572/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1573/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1574/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1575/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1576/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1577/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1578/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1579/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1580/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1581/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1582/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1583/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1584/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1585/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1586/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1587/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1588/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1589/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1590/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1591/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1592/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1593/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1594/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1595/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1596/10000..  Training Loss: 0.193.. \n",
      "Epoch: 1597/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1598/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1599/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1600/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1601/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1602/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1603/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1604/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1605/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1606/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1607/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1608/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1609/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1610/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1611/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1612/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1613/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1614/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1615/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1616/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1617/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1618/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1619/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1620/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1621/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1622/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1623/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1624/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1625/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1626/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1627/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1628/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1629/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1630/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1631/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1632/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1633/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1634/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1635/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1636/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1637/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1638/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1639/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1640/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1641/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1642/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1643/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1644/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1645/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1646/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1647/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1648/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1649/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1650/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1651/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1652/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1653/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1654/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1655/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1656/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1657/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1658/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1659/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1660/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1661/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1662/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1663/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1664/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1665/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1666/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1667/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1668/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1669/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1670/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1671/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1672/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1673/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1674/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1675/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1676/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1677/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1678/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1679/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1680/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1681/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1682/10000..  Training Loss: 0.194.. \n",
      "Epoch: 1683/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1684/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1685/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1686/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1687/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1688/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1689/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1690/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1691/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1692/10000..  Training Loss: 0.192.. \n",
      "Epoch: 1693/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1694/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1695/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1696/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1697/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1698/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1699/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1700/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1701/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1702/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1703/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1704/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1705/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1706/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1707/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1708/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1709/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1710/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1711/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1712/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1713/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1714/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1715/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1716/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1717/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1718/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1719/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1720/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1721/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1722/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1723/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1724/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1725/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1726/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1727/10000..  Training Loss: 0.191.. \n",
      "Epoch: 1728/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1729/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1730/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1731/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1732/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1733/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1734/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1735/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1736/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1737/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1738/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1739/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1740/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1741/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1742/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1743/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1744/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1745/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1746/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1747/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1748/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1749/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1750/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1751/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1752/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1753/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1754/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1755/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1756/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1757/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1758/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1759/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1760/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1761/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1762/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1763/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1764/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1765/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1766/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1767/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1768/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1769/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1770/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1771/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1772/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1773/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1774/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1775/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1776/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1777/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1778/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1779/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1780/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1781/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1782/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1783/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1784/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1785/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1786/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1787/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1788/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1789/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1790/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1791/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1792/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1793/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1794/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1795/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1796/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1797/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1798/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1799/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1800/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1801/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1802/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1803/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1804/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1805/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1806/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1807/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1808/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1809/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1810/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1811/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1812/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1813/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1814/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1815/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1816/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1817/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1818/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1819/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1820/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1821/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1822/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1823/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1824/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1825/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1826/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1827/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1828/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1829/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1830/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1831/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1832/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1833/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1834/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1835/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1836/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1837/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1838/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1839/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1840/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1841/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1842/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1843/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1844/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1845/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1846/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1847/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1848/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1849/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1850/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1851/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1852/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1853/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1854/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1855/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1856/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1857/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1858/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1859/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1860/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1861/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1862/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1863/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1864/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1865/10000..  Training Loss: 0.190.. \n",
      "Epoch: 1866/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1867/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1868/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1869/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1870/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1871/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1872/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1873/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1874/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1875/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1876/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1877/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1878/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1879/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1880/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1881/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1882/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1883/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1884/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1885/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1886/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1887/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1888/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1889/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1890/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1891/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1892/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1893/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1894/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1895/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1896/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1897/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1898/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1899/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1900/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1901/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1902/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1903/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1904/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1905/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1906/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1907/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1908/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1909/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1910/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1911/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1912/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1913/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1914/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1915/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1916/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1917/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1918/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1919/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1920/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1921/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1922/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1923/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1924/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1925/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1926/10000..  Training Loss: 0.186.. \n",
      "Epoch: 1927/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1928/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1929/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1930/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1931/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1932/10000..  Training Loss: 0.188.. \n",
      "Epoch: 1933/10000..  Training Loss: 0.189.. \n",
      "Epoch: 1934/10000..  Training Loss: 0.187.. \n",
      "Epoch: 1935/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1936/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1937/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1938/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1939/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1940/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1941/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1942/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1943/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1944/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1945/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1946/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1947/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1948/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1949/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1950/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1951/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1952/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1953/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1954/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1955/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1956/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1957/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1958/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1959/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1960/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1961/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1962/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1963/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1964/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1965/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1966/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1967/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1968/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1969/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1970/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1971/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1972/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1973/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1974/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1975/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1976/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1977/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1978/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1979/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1980/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1981/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1982/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1983/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1984/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1985/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1986/10000..  Training Loss: 0.181.. \n",
      "Epoch: 1987/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1988/10000..  Training Loss: 0.182.. \n",
      "Epoch: 1989/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1990/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1991/10000..  Training Loss: 0.185.. \n",
      "Epoch: 1992/10000..  Training Loss: 0.184.. \n",
      "Epoch: 1993/10000..  Training Loss: 0.183.. \n",
      "Epoch: 1994/10000..  Training Loss: 0.180.. \n",
      "Epoch: 1995/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1996/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1997/10000..  Training Loss: 0.178.. \n",
      "Epoch: 1998/10000..  Training Loss: 0.179.. \n",
      "Epoch: 1999/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2000/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2001/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2002/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2003/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2004/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2005/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2006/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2007/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2008/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2009/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2010/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2011/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2012/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2013/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2014/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2015/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2016/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2017/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2018/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2019/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2020/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2021/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2022/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2023/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2024/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2025/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2026/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2027/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2028/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2029/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2030/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2031/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2032/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2033/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2034/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2035/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2036/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2037/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2038/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2039/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2040/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2041/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2042/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2043/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2044/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2045/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2046/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2047/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2048/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2049/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2050/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2051/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2052/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2053/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2054/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2055/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2056/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2057/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2058/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2059/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2060/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2061/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2062/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2063/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2064/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2065/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2066/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2067/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2068/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2069/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2070/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2071/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2072/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2073/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2074/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2075/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2076/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2077/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2078/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2079/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2080/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2081/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2082/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2083/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2084/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2085/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2086/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2087/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2088/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2089/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2090/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2091/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2092/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2093/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2094/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2095/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2096/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2097/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2098/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2099/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2100/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2101/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2102/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2103/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2104/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2105/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2106/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2107/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2108/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2109/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2110/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2111/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2112/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2113/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2114/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2115/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2116/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2117/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2118/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2119/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2120/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2121/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2122/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2123/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2124/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2125/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2126/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2127/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2128/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2129/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2130/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2131/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2132/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2133/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2134/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2135/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2136/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2137/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2138/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2139/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2140/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2141/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2142/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2143/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2144/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2145/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2146/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2147/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2148/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2149/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2150/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2151/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2152/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2153/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2154/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2155/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2156/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2157/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2158/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2159/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2160/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2161/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2162/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2163/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2164/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2165/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2166/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2167/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2168/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2169/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2170/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2171/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2172/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2173/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2174/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2175/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2176/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2177/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2178/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2179/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2180/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2181/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2182/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2183/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2184/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2185/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2186/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2187/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2188/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2189/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2190/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2191/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2192/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2193/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2194/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2195/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2196/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2197/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2198/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2199/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2200/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2201/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2202/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2203/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2204/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2205/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2206/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2207/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2208/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2209/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2210/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2211/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2212/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2213/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2214/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2215/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2216/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2217/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2218/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2219/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2220/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2221/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2222/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2223/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2224/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2225/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2226/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2227/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2228/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2229/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2230/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2231/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2232/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2233/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2234/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2235/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2236/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2237/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2238/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2239/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2240/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2241/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2242/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2243/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2244/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2245/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2246/10000..  Training Loss: 0.185.. \n",
      "Epoch: 2247/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2248/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2249/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2250/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2251/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2252/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2253/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2254/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2255/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2256/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2257/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2258/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2259/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2260/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2261/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2262/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2263/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2264/10000..  Training Loss: 0.184.. \n",
      "Epoch: 2265/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2266/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2267/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2268/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2269/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2270/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2271/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2272/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2273/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2274/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2275/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2276/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2277/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2278/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2279/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2280/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2281/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2282/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2283/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2284/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2285/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2286/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2287/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2288/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2289/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2290/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2291/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2292/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2293/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2294/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2295/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2296/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2297/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2298/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2299/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2300/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2301/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2302/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2303/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2304/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2305/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2306/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2307/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2308/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2309/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2310/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2311/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2312/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2313/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2314/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2315/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2316/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2317/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2318/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2319/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2320/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2321/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2322/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2323/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2324/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2325/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2326/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2327/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2328/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2329/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2330/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2331/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2332/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2333/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2334/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2335/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2336/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2337/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2338/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2339/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2340/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2341/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2342/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2343/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2344/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2345/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2346/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2347/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2348/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2349/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2350/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2351/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2352/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2353/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2354/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2355/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2356/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2357/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2358/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2359/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2360/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2361/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2362/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2363/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2364/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2365/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2366/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2367/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2368/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2369/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2370/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2371/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2372/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2373/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2374/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2375/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2376/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2377/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2378/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2379/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2380/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2381/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2382/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2383/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2384/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2385/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2386/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2387/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2388/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2389/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2390/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2391/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2392/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2393/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2394/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2395/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2396/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2397/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2398/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2399/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2400/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2401/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2402/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2403/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2404/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2405/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2406/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2407/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2408/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2409/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2410/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2411/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2412/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2413/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2414/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2415/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2416/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2417/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2418/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2419/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2420/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2421/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2422/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2423/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2424/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2425/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2426/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2427/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2428/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2429/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2430/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2431/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2432/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2433/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2434/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2435/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2436/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2437/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2438/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2439/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2440/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2441/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2442/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2443/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2444/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2445/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2446/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2447/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2448/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2449/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2450/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2451/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2452/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2453/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2454/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2455/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2456/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2457/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2458/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2459/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2460/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2461/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2462/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2463/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2464/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2465/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2466/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2467/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2468/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2469/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2470/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2471/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2472/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2473/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2474/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2475/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2476/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2477/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2478/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2479/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2480/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2481/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2482/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2483/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2484/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2485/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2486/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2487/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2488/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2489/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2490/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2491/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2492/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2493/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2494/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2495/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2496/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2497/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2498/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2499/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2500/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2501/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2502/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2503/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2504/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2505/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2506/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2507/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2508/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2509/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2510/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2511/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2512/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2513/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2514/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2515/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2516/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2517/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2518/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2519/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2520/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2521/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2522/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2523/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2524/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2525/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2526/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2527/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2528/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2529/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2530/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2531/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2532/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2533/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2534/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2535/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2536/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2537/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2538/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2539/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2540/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2541/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2542/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2543/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2544/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2545/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2546/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2547/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2548/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2549/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2550/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2551/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2552/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2553/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2554/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2555/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2556/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2557/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2558/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2559/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2560/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2561/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2562/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2563/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2564/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2565/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2566/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2567/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2568/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2569/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2570/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2571/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2572/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2573/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2574/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2575/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2576/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2577/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2578/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2579/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2580/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2581/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2582/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2583/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2584/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2585/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2586/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2587/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2588/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2589/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2590/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2591/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2592/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2593/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2594/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2595/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2596/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2597/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2598/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2599/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2600/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2601/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2602/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2603/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2604/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2605/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2606/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2607/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2608/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2609/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2610/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2611/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2612/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2613/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2614/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2615/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2616/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2617/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2618/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2619/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2620/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2621/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2622/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2623/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2624/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2625/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2626/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2627/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2628/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2629/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2630/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2631/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2632/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2633/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2634/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2635/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2636/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2637/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2638/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2639/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2640/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2641/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2642/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2643/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2644/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2645/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2646/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2647/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2648/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2649/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2650/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2651/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2652/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2653/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2654/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2655/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2656/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2657/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2658/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2659/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2660/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2661/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2662/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2663/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2664/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2665/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2666/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2667/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2668/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2669/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2670/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2671/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2672/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2673/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2674/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2675/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2676/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2677/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2678/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2679/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2680/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2681/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2682/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2683/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2684/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2685/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2686/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2687/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2688/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2689/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2690/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2691/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2692/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2693/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2694/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2695/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2696/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2697/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2698/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2699/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2700/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2701/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2702/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2703/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2704/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2705/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2706/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2707/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2708/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2709/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2710/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2711/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2712/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2713/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2714/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2715/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2716/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2717/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2718/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2719/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2720/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2721/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2722/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2723/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2724/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2725/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2726/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2727/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2728/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2729/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2730/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2731/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2732/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2733/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2734/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2735/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2736/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2737/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2738/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2739/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2740/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2741/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2742/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2743/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2744/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2745/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2746/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2747/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2748/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2749/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2750/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2751/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2752/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2753/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2754/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2755/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2756/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2757/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2758/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2759/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2760/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2761/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2762/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2763/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2764/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2765/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2766/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2767/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2768/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2769/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2770/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2771/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2772/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2773/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2774/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2775/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2776/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2777/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2778/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2779/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2780/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2781/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2782/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2783/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2784/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2785/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2786/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2787/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2788/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2789/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2790/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2791/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2792/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2793/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2794/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2795/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2796/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2797/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2798/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2799/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2800/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2801/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2802/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2803/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2804/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2805/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2806/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2807/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2808/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2809/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2810/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2811/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2812/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2813/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2814/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2815/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2816/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2817/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2818/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2819/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2820/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2821/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2822/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2823/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2824/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2825/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2826/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2827/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2828/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2829/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2830/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2831/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2832/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2833/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2834/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2835/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2836/10000..  Training Loss: 0.179.. \n",
      "Epoch: 2837/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2838/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2839/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2840/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2841/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2842/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2843/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2844/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2845/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2846/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2847/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2848/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2849/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2850/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2851/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2852/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2853/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2854/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2855/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2856/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2857/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2858/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2859/10000..  Training Loss: 0.183.. \n",
      "Epoch: 2860/10000..  Training Loss: 0.181.. \n",
      "Epoch: 2861/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2862/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2863/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2864/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2865/10000..  Training Loss: 0.170.. \n",
      "Epoch: 2866/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2867/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2868/10000..  Training Loss: 0.182.. \n",
      "Epoch: 2869/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2870/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2871/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2872/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2873/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2874/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2875/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2876/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2877/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2878/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2879/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2880/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2881/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2882/10000..  Training Loss: 0.175.. \n",
      "Epoch: 2883/10000..  Training Loss: 0.176.. \n",
      "Epoch: 2884/10000..  Training Loss: 0.178.. \n",
      "Epoch: 2885/10000..  Training Loss: 0.174.. \n",
      "Epoch: 2886/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2887/10000..  Training Loss: 0.169.. \n",
      "Epoch: 2888/10000..  Training Loss: 0.173.. \n",
      "Epoch: 2889/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2890/10000..  Training Loss: 0.180.. \n",
      "Epoch: 2891/10000..  Training Loss: 0.186.. \n",
      "Epoch: 2892/10000..  Training Loss: 0.177.. \n",
      "Epoch: 2893/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2894/10000..  Training Loss: 0.171.. \n",
      "Epoch: 2895/10000..  Training Loss: 0.172.. \n",
      "Epoch: 2896/10000..  Training Loss: 0.202.. \n",
      "Epoch: 2897/10000..  Training Loss: 0.665.. \n",
      "Epoch: 2898/10000..  Training Loss: 0.979.. \n",
      "Epoch: 2899/10000..  Training Loss: 1.497.. \n",
      "Epoch: 2900/10000..  Training Loss: 1.257.. \n",
      "Epoch: 2901/10000..  Training Loss: 1.522.. \n",
      "Epoch: 2902/10000..  Training Loss: 1.046.. \n",
      "Epoch: 2903/10000..  Training Loss: 0.639.. \n",
      "Epoch: 2904/10000..  Training Loss: 0.552.. \n",
      "Epoch: 2905/10000..  Training Loss: 0.702.. \n",
      "Epoch: 2906/10000..  Training Loss: 0.417.. \n",
      "Epoch: 2907/10000..  Training Loss: 0.379.. \n",
      "Epoch: 2908/10000..  Training Loss: 0.478.. \n",
      "Epoch: 2909/10000..  Training Loss: 0.462.. \n",
      "Epoch: 2910/10000..  Training Loss: 0.424.. \n",
      "Epoch: 2911/10000..  Training Loss: 0.349.. \n",
      "Epoch: 2912/10000..  Training Loss: 0.374.. \n",
      "Epoch: 2913/10000..  Training Loss: 0.308.. \n",
      "Epoch: 2914/10000..  Training Loss: 0.269.. \n",
      "Epoch: 2915/10000..  Training Loss: 0.276.. \n",
      "Epoch: 2916/10000..  Training Loss: 0.299.. \n",
      "Epoch: 2917/10000..  Training Loss: 0.281.. \n",
      "Epoch: 2918/10000..  Training Loss: 0.274.. \n",
      "Epoch: 2919/10000..  Training Loss: 0.240.. \n",
      "Epoch: 2920/10000..  Training Loss: 0.232.. \n",
      "Epoch: 2921/10000..  Training Loss: 0.264.. \n",
      "Epoch: 2922/10000..  Training Loss: 0.227.. \n",
      "Epoch: 2923/10000..  Training Loss: 0.246.. \n",
      "Epoch: 2924/10000..  Training Loss: 0.288.. \n",
      "Epoch: 2925/10000..  Training Loss: 0.238.. \n",
      "Epoch: 2926/10000..  Training Loss: 0.249.. \n",
      "Epoch: 2927/10000..  Training Loss: 0.239.. \n",
      "Epoch: 2928/10000..  Training Loss: 0.234.. \n",
      "Epoch: 2929/10000..  Training Loss: 0.233.. \n",
      "Epoch: 2930/10000..  Training Loss: 0.226.. \n",
      "Epoch: 2931/10000..  Training Loss: 0.215.. \n",
      "Epoch: 2932/10000..  Training Loss: 0.215.. \n",
      "Epoch: 2933/10000..  Training Loss: 0.218.. \n",
      "Epoch: 2934/10000..  Training Loss: 0.217.. \n",
      "Epoch: 2935/10000..  Training Loss: 0.216.. \n",
      "Epoch: 2936/10000..  Training Loss: 0.212.. \n",
      "Epoch: 2937/10000..  Training Loss: 0.203.. \n",
      "Epoch: 2938/10000..  Training Loss: 0.202.. \n",
      "Epoch: 2939/10000..  Training Loss: 0.206.. \n",
      "Epoch: 2940/10000..  Training Loss: 0.207.. \n",
      "Epoch: 2941/10000..  Training Loss: 0.202.. \n",
      "Epoch: 2942/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2943/10000..  Training Loss: 0.198.. \n",
      "Epoch: 2944/10000..  Training Loss: 0.200.. \n",
      "Epoch: 2945/10000..  Training Loss: 0.199.. \n",
      "Epoch: 2946/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2947/10000..  Training Loss: 0.195.. \n",
      "Epoch: 2948/10000..  Training Loss: 0.197.. \n",
      "Epoch: 2949/10000..  Training Loss: 0.196.. \n",
      "Epoch: 2950/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2951/10000..  Training Loss: 0.193.. \n",
      "Epoch: 2952/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2953/10000..  Training Loss: 0.194.. \n",
      "Epoch: 2954/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2955/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2956/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2957/10000..  Training Loss: 0.192.. \n",
      "Epoch: 2958/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2959/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2960/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2961/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2962/10000..  Training Loss: 0.191.. \n",
      "Epoch: 2963/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2964/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2965/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2966/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2967/10000..  Training Loss: 0.190.. \n",
      "Epoch: 2968/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2969/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2970/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2971/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2972/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2973/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2974/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2975/10000..  Training Loss: 0.189.. \n",
      "Epoch: 2976/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2977/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2978/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2979/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2980/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2981/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2982/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2983/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2984/10000..  Training Loss: 0.188.. \n",
      "Epoch: 2985/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2986/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2987/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2988/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2989/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2990/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2991/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2992/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2993/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2994/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2995/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2996/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2997/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2998/10000..  Training Loss: 0.187.. \n",
      "Epoch: 2999/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3000/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3001/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3002/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3003/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3004/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3005/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3006/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3007/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3008/10000..  Training Loss: 0.187.. \n",
      "Epoch: 3009/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3010/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3011/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3012/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3013/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3014/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3015/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3016/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3017/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3018/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3019/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3020/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3021/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3022/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3023/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3024/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3025/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3026/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3027/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3028/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3029/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3030/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3031/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3032/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3033/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3034/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3035/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3036/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3037/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3038/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3039/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3040/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3041/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3042/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3043/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3044/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3045/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3046/10000..  Training Loss: 0.186.. \n",
      "Epoch: 3047/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3048/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3049/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3050/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3051/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3052/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3053/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3054/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3055/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3056/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3057/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3058/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3059/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3060/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3061/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3062/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3063/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3064/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3065/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3066/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3067/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3068/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3069/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3070/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3071/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3072/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3073/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3074/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3075/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3076/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3077/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3078/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3079/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3080/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3081/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3082/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3083/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3084/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3085/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3086/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3087/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3088/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3089/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3090/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3091/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3092/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3093/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3094/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3095/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3096/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3097/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3098/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3099/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3100/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3101/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3102/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3103/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3104/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3105/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3106/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3107/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3108/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3109/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3110/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3111/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3112/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3113/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3114/10000..  Training Loss: 0.185.. \n",
      "Epoch: 3115/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3116/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3117/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3118/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3119/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3120/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3121/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3122/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3123/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3124/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3125/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3126/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3127/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3128/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3129/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3130/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3131/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3132/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3133/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3134/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3135/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3136/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3137/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3138/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3139/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3140/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3141/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3142/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3143/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3144/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3145/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3146/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3147/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3148/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3149/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3150/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3151/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3152/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3153/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3154/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3155/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3156/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3157/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3158/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3159/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3160/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3161/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3162/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3163/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3164/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3165/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3166/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3167/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3168/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3169/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3170/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3171/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3172/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3173/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3174/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3175/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3176/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3177/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3178/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3179/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3180/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3181/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3182/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3183/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3184/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3185/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3186/10000..  Training Loss: 0.184.. \n",
      "Epoch: 3187/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3188/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3189/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3190/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3191/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3192/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3193/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3194/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3195/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3196/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3197/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3198/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3199/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3200/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3201/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3202/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3203/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3204/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3205/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3206/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3207/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3208/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3209/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3210/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3211/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3212/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3213/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3214/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3215/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3216/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3217/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3218/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3219/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3220/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3221/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3222/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3223/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3224/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3225/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3226/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3227/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3228/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3229/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3230/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3231/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3232/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3233/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3234/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3235/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3236/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3237/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3238/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3239/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3240/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3241/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3242/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3243/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3244/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3245/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3246/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3247/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3248/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3249/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3250/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3251/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3252/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3253/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3254/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3255/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3256/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3257/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3258/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3259/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3260/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3261/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3262/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3263/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3264/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3265/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3266/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3267/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3268/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3269/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3270/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3271/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3272/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3273/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3274/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3275/10000..  Training Loss: 0.183.. \n",
      "Epoch: 3276/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3277/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3278/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3279/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3280/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3281/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3282/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3283/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3284/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3285/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3286/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3287/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3288/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3289/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3290/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3291/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3292/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3293/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3294/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3295/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3296/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3297/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3298/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3299/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3300/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3301/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3302/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3303/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3304/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3305/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3306/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3307/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3308/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3309/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3310/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3311/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3312/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3313/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3314/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3315/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3316/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3317/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3318/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3319/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3320/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3321/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3322/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3323/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3324/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3325/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3326/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3327/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3328/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3329/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3330/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3331/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3332/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3333/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3334/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3335/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3336/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3337/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3338/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3339/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3340/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3341/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3342/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3343/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3344/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3345/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3346/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3347/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3348/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3349/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3350/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3351/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3352/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3353/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3354/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3355/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3356/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3357/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3358/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3359/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3360/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3361/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3362/10000..  Training Loss: 0.182.. \n",
      "Epoch: 3363/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3364/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3365/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3366/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3367/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3368/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3369/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3370/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3371/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3372/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3373/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3374/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3375/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3376/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3377/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3378/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3379/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3380/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3381/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3382/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3383/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3384/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3385/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3386/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3387/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3388/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3389/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3390/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3391/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3392/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3393/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3394/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3395/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3396/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3397/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3398/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3399/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3400/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3401/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3402/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3403/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3404/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3405/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3406/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3407/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3408/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3409/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3410/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3411/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3412/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3413/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3414/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3415/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3416/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3417/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3418/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3419/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3420/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3421/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3422/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3423/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3424/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3425/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3426/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3427/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3428/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3429/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3430/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3431/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3432/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3433/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3434/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3435/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3436/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3437/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3438/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3439/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3440/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3441/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3442/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3443/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3444/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3445/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3446/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3447/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3448/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3449/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3450/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3451/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3452/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3453/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3454/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3455/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3456/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3457/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3458/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3459/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3460/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3461/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3462/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3463/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3464/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3465/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3466/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3467/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3468/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3469/10000..  Training Loss: 0.181.. \n",
      "Epoch: 3470/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3471/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3472/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3473/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3474/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3475/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3476/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3477/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3478/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3479/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3480/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3481/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3482/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3483/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3484/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3485/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3486/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3487/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3488/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3489/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3490/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3491/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3492/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3493/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3494/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3495/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3496/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3497/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3498/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3499/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3500/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3501/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3502/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3503/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3504/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3505/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3506/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3507/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3508/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3509/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3510/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3511/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3512/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3513/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3514/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3515/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3516/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3517/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3518/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3519/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3520/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3521/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3522/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3523/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3524/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3525/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3526/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3527/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3528/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3529/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3530/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3531/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3532/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3533/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3534/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3535/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3536/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3537/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3538/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3539/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3540/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3541/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3542/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3543/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3544/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3545/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3546/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3547/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3548/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3549/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3550/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3551/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3552/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3553/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3554/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3555/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3556/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3557/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3558/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3559/10000..  Training Loss: 0.180.. \n",
      "Epoch: 3560/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3561/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3562/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3563/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3564/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3565/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3566/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3567/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3568/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3569/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3570/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3571/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3572/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3573/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3574/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3575/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3576/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3577/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3578/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3579/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3580/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3581/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3582/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3583/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3584/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3585/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3586/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3587/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3588/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3589/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3590/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3591/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3592/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3593/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3594/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3595/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3596/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3597/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3598/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3599/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3600/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3601/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3602/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3603/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3604/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3605/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3606/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3607/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3608/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3609/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3610/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3611/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3612/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3613/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3614/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3615/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3616/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3617/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3618/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3619/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3620/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3621/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3622/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3623/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3624/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3625/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3626/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3627/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3628/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3629/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3630/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3631/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3632/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3633/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3634/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3635/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3636/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3637/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3638/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3639/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3640/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3641/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3642/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3643/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3644/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3645/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3646/10000..  Training Loss: 0.179.. \n",
      "Epoch: 3647/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3648/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3649/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3650/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3651/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3652/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3653/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3654/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3655/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3656/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3657/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3658/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3659/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3660/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3661/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3662/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3663/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3664/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3665/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3666/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3667/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3668/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3669/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3670/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3671/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3672/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3673/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3674/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3675/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3676/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3677/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3678/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3679/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3680/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3681/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3682/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3683/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3684/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3685/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3686/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3687/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3688/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3689/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3690/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3691/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3692/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3693/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3694/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3695/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3696/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3697/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3698/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3699/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3700/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3701/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3702/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3703/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3704/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3705/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3706/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3707/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3708/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3709/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3710/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3711/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3712/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3713/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3714/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3715/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3716/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3717/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3718/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3719/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3720/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3721/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3722/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3723/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3724/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3725/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3726/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3727/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3728/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3729/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3730/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3731/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3732/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3733/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3734/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3735/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3736/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3737/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3738/10000..  Training Loss: 0.178.. \n",
      "Epoch: 3739/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3740/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3741/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3742/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3743/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3744/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3745/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3746/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3747/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3748/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3749/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3750/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3751/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3752/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3753/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3754/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3755/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3756/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3757/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3758/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3759/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3760/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3761/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3762/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3763/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3764/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3765/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3766/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3767/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3768/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3769/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3770/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3771/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3772/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3773/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3774/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3775/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3776/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3777/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3778/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3779/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3780/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3781/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3782/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3783/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3784/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3785/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3786/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3787/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3788/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3789/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3790/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3791/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3792/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3793/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3794/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3795/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3796/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3797/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3798/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3799/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3800/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3801/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3802/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3803/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3804/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3805/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3806/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3807/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3808/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3809/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3810/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3811/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3812/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3813/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3814/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3815/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3816/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3817/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3818/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3819/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3820/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3821/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3822/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3823/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3824/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3825/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3826/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3827/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3828/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3829/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3830/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3831/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3832/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3833/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3834/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3835/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3836/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3837/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3838/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3839/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3840/10000..  Training Loss: 0.177.. \n",
      "Epoch: 3841/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3842/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3843/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3844/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3845/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3846/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3847/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3848/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3849/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3850/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3851/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3852/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3853/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3854/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3855/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3856/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3857/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3858/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3859/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3860/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3861/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3862/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3863/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3864/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3865/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3866/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3867/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3868/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3869/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3870/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3871/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3872/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3873/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3874/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3875/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3876/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3877/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3878/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3879/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3880/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3881/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3882/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3883/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3884/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3885/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3886/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3887/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3888/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3889/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3890/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3891/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3892/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3893/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3894/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3895/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3896/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3897/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3898/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3899/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3900/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3901/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3902/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3903/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3904/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3905/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3906/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3907/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3908/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3909/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3910/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3911/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3912/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3913/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3914/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3915/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3916/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3917/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3918/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3919/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3920/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3921/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3922/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3923/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3924/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3925/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3926/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3927/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3928/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3929/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3930/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3931/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3932/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3933/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3934/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3935/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3936/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3937/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3938/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3939/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3940/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3941/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3942/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3943/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3944/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3945/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3946/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3947/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3948/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3949/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3950/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3951/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3952/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3953/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3954/10000..  Training Loss: 0.176.. \n",
      "Epoch: 3955/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3956/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3957/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3958/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3959/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3960/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3961/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3962/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3963/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3964/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3965/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3966/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3967/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3968/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3969/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3970/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3971/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3972/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3973/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3974/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3975/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3976/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3977/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3978/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3979/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3980/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3981/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3982/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3983/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3984/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3985/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3986/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3987/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3988/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3989/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3990/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3991/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3992/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3993/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3994/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3995/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3996/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3997/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3998/10000..  Training Loss: 0.175.. \n",
      "Epoch: 3999/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4000/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4001/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4002/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4003/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4004/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4005/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4006/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4007/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4008/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4009/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4010/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4011/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4012/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4013/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4014/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4015/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4016/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4017/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4018/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4019/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4020/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4021/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4022/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4023/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4024/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4025/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4026/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4027/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4028/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4029/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4030/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4031/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4032/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4033/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4034/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4035/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4036/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4037/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4038/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4039/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4040/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4041/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4042/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4043/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4044/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4045/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4046/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4047/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4048/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4049/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4050/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4051/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4052/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4053/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4054/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4055/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4056/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4057/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4058/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4059/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4060/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4061/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4062/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4063/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4064/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4065/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4066/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4067/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4068/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4069/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4070/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4071/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4072/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4073/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4074/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4075/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4076/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4077/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4078/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4079/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4080/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4081/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4082/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4083/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4084/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4085/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4086/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4087/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4088/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4089/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4090/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4091/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4092/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4093/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4094/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4095/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4096/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4097/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4098/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4099/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4100/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4101/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4102/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4103/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4104/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4105/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4106/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4107/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4108/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4109/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4110/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4111/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4112/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4113/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4114/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4115/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4116/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4117/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4118/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4119/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4120/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4121/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4122/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4123/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4124/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4125/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4126/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4127/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4128/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4129/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4130/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4131/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4132/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4133/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4134/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4135/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4136/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4137/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4138/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4139/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4140/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4141/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4142/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4143/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4144/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4145/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4146/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4147/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4148/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4149/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4150/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4151/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4152/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4153/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4154/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4155/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4156/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4157/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4158/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4159/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4160/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4161/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4162/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4163/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4164/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4165/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4166/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4167/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4168/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4169/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4170/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4171/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4172/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4173/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4174/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4175/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4176/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4177/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4178/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4179/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4180/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4181/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4182/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4183/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4184/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4185/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4186/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4187/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4188/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4189/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4190/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4191/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4192/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4193/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4194/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4195/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4196/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4197/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4198/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4199/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4200/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4201/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4202/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4203/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4204/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4205/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4206/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4207/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4208/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4209/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4210/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4211/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4212/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4213/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4214/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4215/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4216/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4217/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4218/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4219/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4220/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4221/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4222/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4223/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4224/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4225/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4226/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4227/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4228/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4229/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4230/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4231/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4232/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4233/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4234/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4235/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4236/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4237/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4238/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4239/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4240/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4241/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4242/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4243/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4244/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4245/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4246/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4247/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4248/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4249/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4250/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4251/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4252/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4253/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4254/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4255/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4256/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4257/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4258/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4259/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4260/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4261/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4262/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4263/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4264/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4265/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4266/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4267/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4268/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4269/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4270/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4271/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4272/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4273/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4274/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4275/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4276/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4277/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4278/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4279/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4280/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4281/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4282/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4283/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4284/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4285/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4286/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4287/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4288/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4289/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4290/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4291/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4292/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4293/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4294/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4295/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4296/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4297/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4298/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4299/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4300/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4301/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4302/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4303/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4304/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4305/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4306/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4307/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4308/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4309/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4310/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4311/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4312/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4313/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4314/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4315/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4316/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4317/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4318/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4319/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4320/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4321/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4322/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4323/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4324/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4325/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4326/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4327/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4328/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4329/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4330/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4331/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4332/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4333/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4334/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4335/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4336/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4337/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4338/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4339/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4340/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4341/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4342/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4343/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4344/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4345/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4346/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4347/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4348/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4349/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4350/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4351/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4352/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4353/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4354/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4355/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4356/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4357/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4358/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4359/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4360/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4361/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4362/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4363/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4364/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4365/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4366/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4367/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4368/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4369/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4370/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4371/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4372/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4373/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4374/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4375/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4376/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4377/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4378/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4379/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4380/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4381/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4382/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4383/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4384/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4385/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4386/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4387/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4388/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4389/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4390/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4391/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4392/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4393/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4394/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4395/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4396/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4397/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4398/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4399/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4400/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4401/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4402/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4403/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4404/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4405/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4406/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4407/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4408/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4409/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4410/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4411/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4412/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4413/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4414/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4415/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4416/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4417/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4418/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4419/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4420/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4421/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4422/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4423/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4424/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4425/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4426/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4427/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4428/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4429/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4430/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4431/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4432/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4433/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4434/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4435/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4436/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4437/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4438/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4439/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4440/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4441/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4442/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4443/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4444/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4445/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4446/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4447/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4448/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4449/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4450/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4451/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4452/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4453/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4454/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4455/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4456/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4457/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4458/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4459/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4460/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4461/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4462/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4463/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4464/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4465/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4466/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4467/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4468/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4469/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4470/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4471/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4472/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4473/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4474/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4475/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4476/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4477/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4478/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4479/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4480/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4481/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4482/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4483/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4484/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4485/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4486/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4487/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4488/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4489/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4490/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4491/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4492/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4493/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4494/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4495/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4496/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4497/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4498/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4499/10000..  Training Loss: 0.176.. \n",
      "Epoch: 4500/10000..  Training Loss: 0.177.. \n",
      "Epoch: 4501/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4502/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4503/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4504/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4505/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4506/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4507/10000..  Training Loss: 0.175.. \n",
      "Epoch: 4508/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4509/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4510/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4511/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4512/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4513/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4514/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4515/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4516/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4517/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4518/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4519/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4520/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4521/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4522/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4523/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4524/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4525/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4526/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4527/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4528/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4529/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4530/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4531/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4532/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4533/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4534/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4535/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4536/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4537/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4538/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4539/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4540/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4541/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4542/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4543/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4544/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4545/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4546/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4547/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4548/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4549/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4550/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4551/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4552/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4553/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4554/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4555/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4556/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4557/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4558/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4559/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4560/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4561/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4562/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4563/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4564/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4565/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4566/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4567/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4568/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4569/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4570/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4571/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4572/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4573/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4574/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4575/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4576/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4577/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4578/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4579/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4580/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4581/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4582/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4583/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4584/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4585/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4586/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4587/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4588/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4589/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4590/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4591/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4592/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4593/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4594/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4595/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4596/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4597/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4598/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4599/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4600/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4601/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4602/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4603/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4604/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4605/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4606/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4607/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4608/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4609/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4610/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4611/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4612/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4613/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4614/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4615/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4616/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4617/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4618/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4619/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4620/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4621/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4622/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4623/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4624/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4625/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4626/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4627/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4628/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4629/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4630/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4631/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4632/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4633/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4634/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4635/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4636/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4637/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4638/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4639/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4640/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4641/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4642/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4643/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4644/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4645/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4646/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4647/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4648/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4649/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4650/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4651/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4652/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4653/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4654/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4655/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4656/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4657/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4658/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4659/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4660/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4661/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4662/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4663/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4664/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4665/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4666/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4667/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4668/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4669/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4670/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4671/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4672/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4673/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4674/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4675/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4676/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4677/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4678/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4679/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4680/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4681/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4682/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4683/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4684/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4685/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4686/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4687/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4688/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4689/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4690/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4691/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4692/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4693/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4694/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4695/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4696/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4697/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4698/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4699/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4700/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4701/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4702/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4703/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4704/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4705/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4706/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4707/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4708/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4709/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4710/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4711/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4712/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4713/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4714/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4715/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4716/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4717/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4718/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4719/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4720/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4721/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4722/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4723/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4724/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4725/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4726/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4727/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4728/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4729/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4730/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4731/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4732/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4733/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4734/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4735/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4736/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4737/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4738/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4739/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4740/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4741/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4742/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4743/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4744/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4745/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4746/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4747/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4748/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4749/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4750/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4751/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4752/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4753/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4754/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4755/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4756/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4757/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4758/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4759/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4760/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4761/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4762/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4763/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4764/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4765/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4766/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4767/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4768/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4769/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4770/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4771/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4772/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4773/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4774/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4775/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4776/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4777/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4778/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4779/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4780/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4781/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4782/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4783/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4784/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4785/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4786/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4787/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4788/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4789/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4790/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4791/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4792/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4793/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4794/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4795/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4796/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4797/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4798/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4799/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4800/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4801/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4802/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4803/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4804/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4805/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4806/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4807/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4808/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4809/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4810/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4811/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4812/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4813/10000..  Training Loss: 0.174.. \n",
      "Epoch: 4814/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4815/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4816/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4817/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4818/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4819/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4820/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4821/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4822/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4823/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4824/10000..  Training Loss: 0.173.. \n",
      "Epoch: 4825/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4826/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4827/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4828/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4829/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4830/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4831/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4832/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4833/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4834/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4835/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4836/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4837/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4838/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4839/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4840/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4841/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4842/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4843/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4844/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4845/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4846/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4847/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4848/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4849/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4850/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4851/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4852/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4853/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4854/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4855/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4856/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4857/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4858/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4859/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4860/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4861/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4862/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4863/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4864/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4865/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4866/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4867/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4868/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4869/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4870/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4871/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4872/10000..  Training Loss: 0.172.. \n",
      "Epoch: 4873/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4874/10000..  Training Loss: 0.171.. \n",
      "Epoch: 4875/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4876/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4877/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4878/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4879/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4880/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4881/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4882/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4883/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4884/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4885/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4886/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4887/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4888/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4889/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4890/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4891/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4892/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4893/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4894/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4895/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4896/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4897/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4898/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4899/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4900/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4901/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4902/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4903/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4904/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4905/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4906/10000..  Training Loss: 0.166.. \n",
      "Epoch: 4907/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4908/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4909/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4910/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4911/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4912/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4913/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4914/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4915/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4916/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4917/10000..  Training Loss: 0.170.. \n",
      "Epoch: 4918/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4919/10000..  Training Loss: 0.169.. \n",
      "Epoch: 4920/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4921/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4922/10000..  Training Loss: 0.168.. \n",
      "Epoch: 4923/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4924/10000..  Training Loss: 0.167.. \n",
      "Epoch: 4925/10000..  Training Loss: 0.167.. \n"
     ]
    }
   ],
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "kfolds = 10\n",
    "kfold = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n",
    "\n",
    "epochs_per_training = 10000\n",
    "learning_rate = 0.001\n",
    "\n",
    "all_train_losses = []\n",
    "all_test_losses = []\n",
    "all_accuracies = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_df, train_y)):    \n",
    "    # Hyperparameters\n",
    "    epochs = epochs_per_training\n",
    "    \n",
    "    batch_size = 1024\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Initialize the model, optimizer and the dataset\n",
    "    model = TitanicClassifier().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    X_train, X_test = train_df[train_ids], train_df[test_ids]\n",
    "\n",
    "    y_train = train_y[train_ids]\n",
    "    y_test = train_y[test_ids]\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train.values).long())\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test = torch.utils.data.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test.values).long())\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_correct = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_batch_loss = 0\n",
    "        for i in train_loader:\n",
    "            inputs, labels = i\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_batch_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_batch_loss/len(train_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]))\n",
    "        \n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        total_samples = 0  # Add this to keep track of total samples in test set\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i in test_loader:\n",
    "                inputs, labels = i\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model.forward(inputs)\n",
    "                batch_loss = criterion(outputs, labels)\n",
    "                test_loss += batch_loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct = (predicted == labels).sum().item()\n",
    "                accuracy += correct\n",
    "                total_samples += len(labels)\n",
    "\n",
    "        all_train_losses.append(np.mean(train_losses))\n",
    "        all_test_losses.append(test_loss/len(test_loader))\n",
    "        all_accuracies.append(accuracy / total_samples * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500246ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print K-Fold results\n",
    "print('K-Fold Cross Validation Results')\n",
    "print('--------------------------------')\n",
    "print('Average Training Loss: {:.3f}.. '.format(np.mean(all_train_losses)))\n",
    "print('Average Test Loss: {:.3f}.. '.format(np.mean(all_test_losses)))\n",
    "print('Average Test Accuracy: {:.3f}%'.format(np.mean(all_accuracies)))\n",
    "\n",
    "# Plot the training and test losses\n",
    "plt.plot(all_train_losses, label='Training loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f659ac2-df3f-4200-a239-868176632d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_test_losses, label='Validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5feaf4-a9c7-431c-80bf-b70c0808eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracies\n",
    "plt.plot(all_accuracies, label='Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6663e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "model = TitanicClassifier().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = epochs_per_training\n",
    "batch_size = 1024\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(train_df), batch_size):\n",
    "        inputs = torch.from_numpy(train_df[i:i+batch_size].astype(np.float32)).to(device)\n",
    "         # Convert train_y to a NumPy array and then to a PyTorch tensor\n",
    "        labels = torch.from_numpy(train_y[i:i+batch_size].values.astype(np.int64)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Training Epoch {e+1}/{epochs}, Loss: {running_loss/len(train_df)}\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    inputs = torch.from_numpy(test_df.astype(np.float32)).to(device)\n",
    "    log_ps = model(inputs)\n",
    "    ps = torch.exp(log_ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    \n",
    "    # Move top_class tensor to CPU and then convert to NumPy array\n",
    "    predictions = top_class.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a608f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to 0 or 1\n",
    "predictions = np.round(predictions).astype(int)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "\n",
    "# Populate the passenger ID columns\n",
    "submission_df['PassengerId'] = test_df_ids\n",
    "submission_df['Survived'] = predictions\n",
    "\n",
    "# Output CSV file\n",
    "print(\"Saving submission...\")\n",
    "submission_df.to_csv('data/submission.csv', index=False)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09579be-8664-4437-9408-053965cd384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived = submission_df[submission_df['Survived'] == 1]\n",
    "died = submission_df[submission_df['Survived'] == 0]\n",
    "print('Survived: %i (%.1f%%)' % (len(survived), float(len(survived))/len(submission_df)*100.0))\n",
    "print('Died: %i (%.1f%%)' % (len(died), float(len(died))/len(submission_df)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb673279-cf57-4398-b162-7b5745d3cffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
